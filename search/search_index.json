{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#orthogonium-improved-implementations-of-orthogonal-layers","title":"\u2728 Orthogonium: Improved implementations of orthogonal layers","text":"<p>This library aims to centralize, standardize and improve methods to  build orthogonal layers, with a focus on convolutional layers . We noticed that a layer's implementation play a significant role in the final performance : a more efficient implementation  allows larger networks and more training steps within the same compute  budget. So our implementation differs from original papers in order to  be faster, to consume less memory or be more flexible.</p>"},{"location":"#what-is-included-in-this-library","title":"\ud83d\udcc3 What is included in this library ?","text":"Layer name Description Orthogonal ? Usage Status AOC (Adaptive-BCOP) The most scalable method to build orthogonal convolution. Allows control of kernel size, stride, groups dilation and convtranspose Orthogonal A flexible method for complex architectures. Preserve orthogonality and works on large scale images. done Adaptive-SC-Fac Same as previous layer but based on SC-Fac instead of BCOP, which claims a complete parametrization of separable convolutions Orthogonal Same as above pending Adaptive-SOC SOC modified to be: i) faster and memory efficient ii) handle stride, groups, dilation &amp; convtranspose Orthogonal Good for depthwise convolutions and cases where control over the kernel size is not required in progress SLL The original SLL layer, which is already quite efficient. 1-Lipschitz Well suited for residual blocks, it also contains ReLU activations. done SLL-AOC SLL-AOC is to the downsampling block what SLL is to the residual block (see ResNet paper) 1-Lipschitz Allows to construct a \"strided\" residual block than can change the number of channels. It adds a convolution in the residual path. done Sandwish-AOC Sandwish convolutions that uses AOC to replace the FFT. Allowing it to scale to large images. 1-Lipschitz pending Adaptive-ECO ECO modified to i) handle stride, groups &amp; convtranspose Orthogonal (low priority)"},{"location":"#directory-structure","title":"directory structure","text":"<pre><code>orthogonium\n\u251c\u2500\u2500 layers\n\u2502   \u251c\u2500\u2500 conv\n\u2502   \u2502   \u251c\u2500\u2500 AOC\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ortho_conv.py # contains AdaptiveOrthoConv2d layer\n\u2502   \u2502   \u251c\u2500\u2500 AdaptiveSOC\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ortho_conv.py # contains AdaptiveSOCConv2d layer (untested)\n\u2502   \u2502   \u251c\u2500\u2500 SLL\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 sll_layer.py # contains SDPBasedLipschitzConv, SDPBasedLipschitzDense, SLLxAOCLipschitzResBlock\n\u2502   \u251c\u2500\u2500 legacy\n\u2502   \u2502   \u251c\u2500\u2500 original code of BCOP, SOC, Cayley etc.\n\u2502   \u251c\u2500\u2500 linear\n\u2502   \u2502   \u251c\u2500\u2500 ortho_linear.py # contains OrthoLinear layer (can be used with BB, QR and Exp parametrization)\n\u2502   \u251c\u2500\u2500 normalization.py # contains Batch centering and Layer centering\n\u2502   \u251c\u2500\u2500 custom_activations.py # contains custom activations for 1 lipschitz networks\n\u2502   \u251c\u2500\u2500 channel_shuffle.py # contains channel shuffle layer  \n\u251c\u2500\u2500 model_factory.py # factory function to construct various models for the zoo\n\u251c\u2500\u2500 losses # loss functions, VRA estimation\n</code></pre>"},{"location":"#aoc","title":"AOC:","text":"<p>AOC is a method that allows to build orthogonal convolutions with  an explicit kernel, that support all features like stride, conv transposed, grouped convolutions and dilation (and all compositions of these parameters). This approach is highly scalable, and can be applied to problems like Imagenet-1K.</p>"},{"location":"#adaptive-sc-fac","title":"Adaptive-SC-FAC:","text":"<p>As AOC is built on top of BCOP method, we can construct an equivalent method constructed on top of SC-Fac instead. This will allow to compare performance of the two methods given that they have very similar parametrization. (See our  paper for discussions about the similarities and differences between the two methods).</p>"},{"location":"#adaptive-soc","title":"Adaptive-SOC:","text":"<p>Adaptive-SOC blend the approach of AOC and SOC. It differs from SOC in the way that it is more memory efficient and  sometimes faster. It also allows to handle stride, groups, dilation and transposed convolutions. However, it does not allow to  control the kernel size explicitly as the resulting kernel size is larger than the requested kernel size.  It is due to the computation to the exponential of a kernel that increases the kernel size at each iteration.</p> <p>Its development is still in progress, so extra testing is still require to ensure exact orthogonality.</p>"},{"location":"#sll","title":"SLL:","text":"<p>SLL is a method that allows to construct small residual blocks with ReLU activations. We kept most to the original  implementation, and added <code>SLLxAOCLipschitzResBlock</code> that construct a down-sampling residual block by fusing SLL with  $AOC.</p>"},{"location":"#more-layers-are-coming-soon","title":"more layers are coming soon !","text":""},{"location":"#install-the-library","title":"\ud83c\udfe0 Install the library:","text":"<p>The library will soon be available on pip, in the meanwhile, you can clone the repository and run the following command  to install it locally: <pre><code>pip install -e .\n</code></pre></p>"},{"location":"#use-the-layer","title":"Use the layer:","text":"<pre><code>from orthogonium.layers.conv.AOC import AdaptiveOrthoConv2d\nfrom orthogonium.reparametrizers import DEFAULT_ORTHO_PARAMS\n\n# use OrthoConv2d with the same params as torch.nn.Conv2d\n\nconv = AdaptiveOrthoConv2d(\n  kernel_size=kernel_size,\n  in_channels=256,\n  out_channels=256,\n  stride=2,\n  groups=16,\n  bias=True,\n  padding=(kernel_size // 2, kernel_size // 2),\n  padding_mode=\"circular\",\n  ortho_params=DEFAULT_ORTHO_PARAMS\n)\n</code></pre>"},{"location":"#model-zoo","title":"\ud83d\udc2f Model Zoo","text":"<p>Stay tuned, a model zoo will be available soon !</p>"},{"location":"#disclaimer","title":"\ud83d\udca5Disclaimer","text":"<p>Given the great quality of the original implementations, orthogonium do not focus on reproducing exactly the results of the original papers, but rather on providing a more efficient implementation. Some degradations in the final provable  accuracy may be observed when reproducing the results of the original papers, we consider this acceptable is the gain  in terms of scalability is worth it. This library aims to provide more scalable and versatile implementations for people who seek to use orthogonal layers  in a larger scale setting.</p>"},{"location":"#ressources","title":"\ud83d\udd2d Ressources","text":""},{"location":"#1-lipschitz-cnns-and-orthogonal-cnns","title":"1 Lipschitz CNNs and orthogonal CNNs","text":"<ul> <li>1-Lipschitz Layers Compared: github and paper</li> <li>BCOP: github and paper</li> <li>SC-Fac: paper</li> <li>ECO: paper</li> <li>Cayley: github and paper</li> <li>LOT: github and paper</li> <li>ProjUNN-T: github and paper</li> <li>SLL: github and paper</li> <li>Sandwish: github and paper</li> <li>AOL: github and paper</li> <li>SOC: github and paper 1, paper 2</li> </ul>"},{"location":"#lipschitz-constant-evaluation","title":"Lipschitz constant evaluation","text":"<ul> <li>Spectral Norm of Convolutional Layers with Circular and Zero Paddings </li> <li>Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration</li> <li>github of the two papers</li> </ul>"},{"location":"#contributing","title":"\ud83c\udf7b Contributing","text":"<p>This library is still in a very early stage, so expect some bugs and missing features. Also, before the version 1.0.0, the API may change and no backward compatibility will be ensured, this will allow a rapid integration of new features. In order to prioritize the development, we will focus on the most used layers and models. If you have a specific need, please open an issue, and we will try to address it as soon as possible.</p> <p>Also, if you have a model that you would like to share, please open a PR with the model and the training script. We will be happy to include it in the zoo.</p> <p>If you want to contribute, please open a PR with the new feature or bug fix. We will review it as soon as possible.</p>"},{"location":"#ongoing-developments","title":"Ongoing developments","text":"<p>Layers: - SOC:   - remove channels padding to handle ci != co efficiently   - enable groups   - enable support for native stride, transposition and dilation - AOL:   - torch implementation of AOL - Sandwish:   - import code   - plug AOC into Sandwish conv</p> <p>ZOO: - models from the paper</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Thanks for taking the time to contribute!</p> <p>From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain.</p>"},{"location":"CONTRIBUTING/#setup-with-make","title":"Setup with make","text":"<ul> <li>Clone the repo <code>git clone git@github.com:thib-s/orthogonium.git</code>.</li> <li>Go to your freshly downloaded repo <code>cd orthogonium</code></li> <li>Create a virtual environment and install the necessary dependencies for development:</li> </ul> <p><code>make prepare-dev &amp;&amp; source orthogonium_dev_env/bin/activate</code>.</p> <p>Welcome to the team !</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>To run test <code>make test</code> This command activate your virtual environment and launch the <code>tox</code> command.</p> <p><code>tox</code> on the otherhand will do the following: - run pytest on the tests folder - run pylint on the deel-datasets main files</p> <p>Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons.</p> <p>Please, make sure you run all the tests at least once before opening a pull request.</p> <p>A word toward Pylint for those that don't know it:</p> <p>Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.</p> <p>Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least).</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":"<p>After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly (see Governance policy).</p> <p>Something that will increase the chance that your pull request is accepted:</p> <ul> <li>Write tests and ensure that the existing ones pass.</li> <li>If <code>make test</code> is succesful, you have fair chances to pass the CI workflows (linting and test)</li> <li>Follow the existing coding style and run <code>make check_all</code> to check all files format.</li> <li>Write a good commit message (we follow a lowercase convention).</li> <li>For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.</li> </ul>"},{"location":"api/activations/","title":"activations","text":""},{"location":"api/activations/#orthogonium.layers.custom_activations.Abs","title":"<code>Abs</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>class Abs(nn.Module):\n    def __init__(self):\n        \"\"\"\n        Initializes an instance of the Abs class.\n\n        This method is automatically called when a new object of the Abs class\n        is instantiated. It calls the initializer of its superclass to ensure\n        proper initialization of inherited class functionality, setting up\n        the required base structures or attributes.\n        \"\"\"\n        super(Abs, self).__init__()\n\n    def forward(self, z):\n        return torch.abs(z)\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.Abs.__init__","title":"<code>__init__()</code>","text":"<p>Initializes an instance of the Abs class.</p> <p>This method is automatically called when a new object of the Abs class is instantiated. It calls the initializer of its superclass to ensure proper initialization of inherited class functionality, setting up the required base structures or attributes.</p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes an instance of the Abs class.\n\n    This method is automatically called when a new object of the Abs class\n    is instantiated. It calls the initializer of its superclass to ensure\n    proper initialization of inherited class functionality, setting up\n    the required base structures or attributes.\n    \"\"\"\n    super(Abs, self).__init__()\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.HouseHolder","title":"<code>HouseHolder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>class HouseHolder(nn.Module):\n    def __init__(self, channels, axis=1):\n        \"\"\"\n        A activation that applies a parameterized transformation via Householder\n        reflection technique. It is initialized with the number of input channels, which must\n        be even, and an axis that determines the dimension along which operations are applied.\n        This is a corrected version of the original implementation from Singla et al. (2019),\n        which features a 1/sqrt(2) scaling factor to be 1-Lipschitz.\n\n        Attributes:\n            theta (torch.nn.Parameter): Learnable parameter that determines the transformation\n                applied via Householder reflection.\n            axis (int): Dimension along which the operation is performed.\n\n        Args:\n            channels (int): Total number of input channels. Must be an even number.\n            axis (int): Dimension along which the transformation is applied. Default is 1.\n        \"\"\"\n        super(HouseHolder, self).__init__()\n        assert (channels % 2) == 0\n        eff_channels = channels // 2\n\n        self.theta = nn.Parameter(\n            0.5 * np.pi * torch.ones(1, eff_channels, 1, 1), requires_grad=True\n        )\n        self.axis = axis\n\n    def forward(self, z):\n        theta = self.theta\n        x, y = z.split(z.shape[self.axis] // 2, self.axis)\n\n        selector = (x * torch.sin(0.5 * theta)) - (y * torch.cos(0.5 * theta))\n\n        a_2 = x * torch.cos(theta) + y * torch.sin(theta)\n        b_2 = x * torch.sin(theta) - y * torch.cos(theta)\n\n        a = x * (selector &lt;= 0) + a_2 * (selector &gt; 0)\n        b = y * (selector &lt;= 0) + b_2 * (selector &gt; 0)\n        return torch.cat([a, b], dim=self.axis) / SQRT_2\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.HouseHolder.__init__","title":"<code>__init__(channels, axis=1)</code>","text":"<p>A activation that applies a parameterized transformation via Householder reflection technique. It is initialized with the number of input channels, which must be even, and an axis that determines the dimension along which operations are applied. This is a corrected version of the original implementation from Singla et al. (2019), which features a 1/sqrt(2) scaling factor to be 1-Lipschitz.</p> <p>Attributes:</p> Name Type Description <code>theta</code> <code>Parameter</code> <p>Learnable parameter that determines the transformation applied via Householder reflection.</p> <code>axis</code> <code>int</code> <p>Dimension along which the operation is performed.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>Total number of input channels. Must be an even number.</p> required <code>axis</code> <code>int</code> <p>Dimension along which the transformation is applied. Default is 1.</p> <code>1</code> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>def __init__(self, channels, axis=1):\n    \"\"\"\n    A activation that applies a parameterized transformation via Householder\n    reflection technique. It is initialized with the number of input channels, which must\n    be even, and an axis that determines the dimension along which operations are applied.\n    This is a corrected version of the original implementation from Singla et al. (2019),\n    which features a 1/sqrt(2) scaling factor to be 1-Lipschitz.\n\n    Attributes:\n        theta (torch.nn.Parameter): Learnable parameter that determines the transformation\n            applied via Householder reflection.\n        axis (int): Dimension along which the operation is performed.\n\n    Args:\n        channels (int): Total number of input channels. Must be an even number.\n        axis (int): Dimension along which the transformation is applied. Default is 1.\n    \"\"\"\n    super(HouseHolder, self).__init__()\n    assert (channels % 2) == 0\n    eff_channels = channels // 2\n\n    self.theta = nn.Parameter(\n        0.5 * np.pi * torch.ones(1, eff_channels, 1, 1), requires_grad=True\n    )\n    self.axis = axis\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.HouseHolder_Order_2","title":"<code>HouseHolder_Order_2</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>class HouseHolder_Order_2(nn.Module):\n    def __init__(self, channels, axis=1):\n        \"\"\"\n        Represents a layer or module that performs operations using Householder\n        transformations of order 2, parameterized by angles corresponding to\n        each group of channels. This is a corrected version of the original\n        implementation from Singla et al. (2019), which features a 1/sqrt(2)\n        scaling factor to be 1-Lipschitz.\n\n        Attributes:\n            num_groups (int): The number of groups, which is half the number\n            of channels provided as input.\n\n            axis (int): The axis along which the computation is performed.\n\n            theta0 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n            representing the first set of angles (in radians) used in the\n            parameterization.\n\n            theta1 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n            representing the second set of angles (in radians) used in the\n            parameterization.\n\n            theta2 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n            representing the third set of angles (in radians) used in the\n            parameterization.\n\n        Args:\n            channels (int): The total number of input channels. Must be an even\n            number, as it will be split into groups.\n\n            axis (int, optional): Specifies the axis for computations. Defaults\n            to 1.\n        \"\"\"\n        super(HouseHolder_Order_2, self).__init__()\n        assert (channels % 2) == 0\n        self.num_groups = channels // 2\n        self.axis = axis\n\n        self.theta0 = nn.Parameter(\n            (np.pi * torch.rand(self.num_groups)), requires_grad=True\n        )\n        self.theta1 = nn.Parameter(\n            (np.pi * torch.rand(self.num_groups)), requires_grad=True\n        )\n        self.theta2 = nn.Parameter(\n            (np.pi * torch.rand(self.num_groups)), requires_grad=True\n        )\n\n    def forward(self, z):\n        theta0 = torch.clamp(self.theta0.view(1, -1, 1, 1), 0.0, 2 * np.pi)\n\n        x, y = z.split(z.shape[self.axis] // 2, self.axis)\n        z_theta = (torch.atan2(y, x) - (0.5 * theta0)) % (2 * np.pi)\n\n        theta1 = torch.clamp(self.theta1.view(1, -1, 1, 1), 0.0, 2 * np.pi)\n        theta2 = torch.clamp(self.theta2.view(1, -1, 1, 1), 0.0, 2 * np.pi)\n        theta3 = 2 * np.pi - theta1\n        theta4 = 2 * np.pi - theta2\n\n        ang1 = 0.5 * (theta1)\n        ang2 = 0.5 * (theta1 + theta2)\n        ang3 = 0.5 * (theta1 + theta2 + theta3)\n        ang4 = 0.5 * (theta1 + theta2 + theta3 + theta4)\n\n        select1 = torch.logical_and(z_theta &gt;= 0, z_theta &lt; ang1)\n        select2 = torch.logical_and(z_theta &gt;= ang1, z_theta &lt; ang2)\n        select3 = torch.logical_and(z_theta &gt;= ang2, z_theta &lt; ang3)\n        select4 = torch.logical_and(z_theta &gt;= ang3, z_theta &lt; ang4)\n\n        a1 = x\n        b1 = y\n\n        a2 = x * torch.cos(theta0 + theta1) + y * torch.sin(theta0 + theta1)\n        b2 = x * torch.sin(theta0 + theta1) - y * torch.cos(theta0 + theta1)\n\n        a3 = x * torch.cos(theta2) + y * torch.sin(theta2)\n        b3 = -x * torch.sin(theta2) + y * torch.cos(theta2)\n\n        a4 = x * torch.cos(theta0) + y * torch.sin(theta0)\n        b4 = x * torch.sin(theta0) - y * torch.cos(theta0)\n\n        a = (a1 * select1) + (a2 * select2) + (a3 * select3) + (a4 * select4)\n        b = (b1 * select1) + (b2 * select2) + (b3 * select3) + (b4 * select4)\n\n        z = torch.cat([a, b], dim=self.axis) / SQRT_2\n        return z\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.HouseHolder_Order_2.__init__","title":"<code>__init__(channels, axis=1)</code>","text":"<p>Represents a layer or module that performs operations using Householder transformations of order 2, parameterized by angles corresponding to each group of channels. This is a corrected version of the original implementation from Singla et al. (2019), which features a 1/sqrt(2) scaling factor to be 1-Lipschitz.</p> <p>Attributes:</p> Name Type Description <code>num_groups</code> <code>int</code> <p>The number of groups, which is half the number</p> <code>axis</code> <code>int</code> <p>The axis along which the computation is performed.</p> <code>theta0</code> <code>Parameter</code> <p>A tensor parameter of shape <code>(num_groups,)</code></p> <code>theta1</code> <code>Parameter</code> <p>A tensor parameter of shape <code>(num_groups,)</code></p> <code>theta2</code> <code>Parameter</code> <p>A tensor parameter of shape <code>(num_groups,)</code></p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>The total number of input channels. Must be an even</p> required <code>axis</code> <code>int</code> <p>Specifies the axis for computations. Defaults</p> <code>1</code> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>def __init__(self, channels, axis=1):\n    \"\"\"\n    Represents a layer or module that performs operations using Householder\n    transformations of order 2, parameterized by angles corresponding to\n    each group of channels. This is a corrected version of the original\n    implementation from Singla et al. (2019), which features a 1/sqrt(2)\n    scaling factor to be 1-Lipschitz.\n\n    Attributes:\n        num_groups (int): The number of groups, which is half the number\n        of channels provided as input.\n\n        axis (int): The axis along which the computation is performed.\n\n        theta0 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n        representing the first set of angles (in radians) used in the\n        parameterization.\n\n        theta1 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n        representing the second set of angles (in radians) used in the\n        parameterization.\n\n        theta2 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n        representing the third set of angles (in radians) used in the\n        parameterization.\n\n    Args:\n        channels (int): The total number of input channels. Must be an even\n        number, as it will be split into groups.\n\n        axis (int, optional): Specifies the axis for computations. Defaults\n        to 1.\n    \"\"\"\n    super(HouseHolder_Order_2, self).__init__()\n    assert (channels % 2) == 0\n    self.num_groups = channels // 2\n    self.axis = axis\n\n    self.theta0 = nn.Parameter(\n        (np.pi * torch.rand(self.num_groups)), requires_grad=True\n    )\n    self.theta1 = nn.Parameter(\n        (np.pi * torch.rand(self.num_groups)), requires_grad=True\n    )\n    self.theta2 = nn.Parameter(\n        (np.pi * torch.rand(self.num_groups)), requires_grad=True\n    )\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.MaxMin","title":"<code>MaxMin</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>class MaxMin(nn.Module):\n    def __init__(self, axis=1):\n        \"\"\"\n        This class implements the MaxMin activation function. Which is a\n        pairwise activation function that returns the maximum and minimum (ordered)\n        of each pair of elements in the input tensor.\n\n        Parameters\n            axis : int, default=1 the axis along which to apply the activation function.\n\n        \"\"\"\n        self.axis = axis\n        super(MaxMin, self).__init__()\n\n    def forward(self, z):\n        a, b = z.split(z.shape[self.axis] // 2, self.axis)\n        c, d = torch.max(a, b), torch.min(a, b)\n        return torch.cat([c, d], dim=self.axis)\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.MaxMin.__init__","title":"<code>__init__(axis=1)</code>","text":"<p>This class implements the MaxMin activation function. Which is a pairwise activation function that returns the maximum and minimum (ordered) of each pair of elements in the input tensor.</p> <p>Parameters     axis : int, default=1 the axis along which to apply the activation function.</p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>def __init__(self, axis=1):\n    \"\"\"\n    This class implements the MaxMin activation function. Which is a\n    pairwise activation function that returns the maximum and minimum (ordered)\n    of each pair of elements in the input tensor.\n\n    Parameters\n        axis : int, default=1 the axis along which to apply the activation function.\n\n    \"\"\"\n    self.axis = axis\n    super(MaxMin, self).__init__()\n</code></pre>"},{"location":"api/aoc/","title":"Aoc","text":"<p>The most scalable method to build orthogonal convolution. Allows control of kernel size,  stride, groups dilation and transposed convolutions.</p> <p>The classes <code>AdaptiveOrthoConv2d</code> and <code>AdaptiveOrthoConv2d</code> are not classes,  but factory function to selecte bewteen 3 different parametrizations, as depicted in the following figure:</p> <p></p>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d","title":"<code>AdaptiveOrthoConv2d(in_channels, out_channels, kernel_size, stride=1, padding='same', dilation=1, groups=1, bias=True, padding_mode='circular', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel size and stride.</p>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features","title":"Key Features:","text":"<pre><code>- Enforces orthogonality, preserving gradient norms.\n- Supports native striding, dilation, grouped convolutions, and flexible padding.\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RKOConv2d`.\n- When stride == 1, the layer is a `FastBlockConv2d`.\n- Otherwise, the layer is a `BcopRkoConv2d`.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>str or _size_2_t</code> <p>Padding mode or size. Default is \"same\".</p> <code>'same'</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input to output channels. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"circular\".</p> <code>'circular'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>Conv2d</code> <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p> <p>Raises:</p> Type Description <code>`ValueError`</code> <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\ortho_conv.py</code> <pre><code>def AdaptiveOrthoConv2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: Union[str, _size_2_t] = \"same\",\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"circular\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.Conv2d:\n    \"\"\"\n    Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel size and stride.\n\n    Key Features:\n    -------------\n        - Enforces orthogonality, preserving gradient norms.\n        - Supports native striding, dilation, grouped convolutions, and flexible padding.\n\n    Behavior:\n    -------------\n        - When kernel_size == stride, the layer is an `RKOConv2d`.\n        - When stride == 1, the layer is a `FastBlockConv2d`.\n        - Otherwise, the layer is a `BcopRkoConv2d`.\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the convolution. Default is 1.\n        padding (str or _size_2_t, optional): Padding mode or size. Default is \"same\".\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        padding_mode (str, optional): Padding mode. Default is \"circular\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).\n\n    Raises:\n        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n    \"\"\"\n\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RKOConv2d\n    elif (stride == 1) or (in_channels &gt;= out_channels):\n        convclass = FastBlockConv2d\n    else:\n        convclass = BcopRkoConv2d\n    return convclass(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=groups,\n        bias=bias,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d","title":"<code>AdaptiveOrthoConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal convolutional transpose layer, adapting based on kernel size and stride.</p>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features","title":"Key Features:","text":"<pre><code>- Ensures orthogonality in transpose convolutions for stable gradient propagation.\n- Supports dilation, grouped operations, and efficient kernel construction.\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RkoConvTranspose2d`.\n- When stride == 1, the layer is a `FastBlockConvTranspose2D`.\n- Otherwise, the layer is a `BcopRkoConvTranspose2d`.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the transpose convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>_size_2_t</code> <p>Padding size. Default is 0.</p> <code>0</code> <code>output_padding</code> <code>_size_2_t</code> <p>Additional size for output. Default is 0.</p> <code>0</code> <code>groups</code> <code>int</code> <p>Number of groups. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"zeros\".</p> <code>'zeros'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>ConvTranspose2d</code> <p>A configured instance of <code>nn.ConvTranspose2d</code> (one of <code>RkoConvTranspose2d</code>, <code>FastBlockConvTranspose2D</code>, or <code>BcopRkoConvTranspose2d</code>).</p> <p>Raises: - <code>ValueError</code>: If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\ortho_conv.py</code> <pre><code>def AdaptiveOrthoConvTranspose2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.ConvTranspose2d:\n    \"\"\"\n    Factory function to create an orthogonal convolutional transpose layer, adapting based on kernel size and stride.\n\n    Key Features:\n    -------------\n        - Ensures orthogonality in transpose convolutions for stable gradient propagation.\n        - Supports dilation, grouped operations, and efficient kernel construction.\n\n    Behavior:\n    ---------\n        - When kernel_size == stride, the layer is an `RkoConvTranspose2d`.\n        - When stride == 1, the layer is a `FastBlockConvTranspose2D`.\n        - Otherwise, the layer is a `BcopRkoConvTranspose2d`.\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the transpose convolution. Default is 1.\n        padding (_size_2_t, optional): Padding size. Default is 0.\n        output_padding (_size_2_t, optional): Additional size for output. Default is 0.\n        groups (int, optional): Number of groups. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        padding_mode (str, optional): Padding mode. Default is \"zeros\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.ConvTranspose2d` (one of `RkoConvTranspose2d`, `FastBlockConvTranspose2D`, or `BcopRkoConvTranspose2d`).\n\n    **Raises:**\n    - `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n    \"\"\"\n\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RkoConvTranspose2d\n    elif stride == 1:\n        convclass = FastBlockConvTranspose2D\n    else:\n        convclass = BcopRkoConvTranspose2d\n    return convclass(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        output_padding=output_padding,\n        groups=groups,\n        bias=bias,\n        dilation=dilation,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer","title":"<code>BCOPTrivializer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>class BCOPTrivializer(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        groups,\n    ):\n        \"\"\"This module is used to generate orthogonal kernels for the BCOP layer. It takes\n        as input a matrix PQ of shape (groups, 2*kernel_size, c, c//2) and returns a kernel\n        of shape (c, c, kernel_size, kernel_size) that is orthogonal.\n\n        Args:\n            in_channels (int): number of input channels\n            out_channels (int): number of output channels\n            kernel_size (int): size of the kernel\n            groups (int): number of groups\n        \"\"\"\n        super(BCOPTrivializer, self).__init__()\n        self.kernel_size = kernel_size\n        self.groups = groups\n        self.out_channels = out_channels\n        self.in_channels = in_channels\n        self.min_channels = min(in_channels, out_channels)\n        self.max_channels = max(in_channels, out_channels)\n        self.transpose = out_channels &lt; in_channels\n        self.num_kernels = 2 * kernel_size\n\n    def forward(self, PQ):\n        ident = (\n            torch.eye(self.max_channels // self.groups, device=PQ.device)\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n        # PQ contains 2*(kernel_size - 1) + 2 matrices of shape (c, c//2)\n        # the 2 first matrices will be composed to build a (c, c) matrix\n        # this (c, c) matrix will be used to build the first 1x1 conv\n        # the remaining matrices will be used to build the 2x2 convs as\n        # described in BCOP paper\n        ####\n        # first we compute PQ@PQ.t (used by both 1x1 and 2x2 convs)\n        # we can rewrite PQ@PQ.t as an einsum\n        PQ = torch.einsum(\"gijl,gikl-&gt;gijk\", PQ, PQ)\n        # PQ = PQ @ PQ.transpose(-1, -2)\n        # we build the 1x1 conv using the two first matrices\n        # we construct the (c, c) matrix by computing (I - 2*PQ[0]) @ (I - 2*PQ[1])\n        # this is an extension of Householder reflection to the matrix case where\n        # instead of reflecting a vector and compose c matrices, we reflect 2\n        # (c, c//2) matrices. This results in an orthogonal matrix, but the fact that\n        # any orthogonal matrix can be decomposed this way is yet to be proven.\n        c11 = ident - 2 * PQ[:, 0]\n        c11 = c11 @ (ident - 2 * PQ[:, 1])\n        # reshape the matrix to build a 1x1 conv\n        c11 = c11.view(\n            self.max_channels,\n            self.max_channels // self.groups,\n            1,\n            1,\n        )\n        # if the number of channels is different, we need to remove the extra channels\n        # this results in a row/column othogonal matrix. It is still more efficient than\n        # doing a separate orthogonalization (as shapes differs).\n        if self.in_channels != self.out_channels:\n            c11 = c11[:, : self.min_channels // self.groups, :, :]\n\n        # build all 2x2 convs in parallel\n        # half of the matrices will be used to create a 2x1 conv while the other half\n        # will be used to create a 1x2 conv. The 2x1 and 1x2 convs will be composed\n        # to build a 2x2 conv. c12 and c21 are notation abuse, since the tensors represent\n        # 1x1 convs (it is the vertical/horizontal stacking of c12/c21 with (I-c12) and (I-c21)\n        # that will result in a 1x2/2x1 conv)\n        c12 = PQ[:, 2 : 2 + (self.kernel_size - 1), :, :]\n        c21 = PQ[:, 2 + (self.kernel_size - 1) :, :, :]\n        c22 = block_orth(\n            c12, c21\n        )  # this is an efficient and parallel way to compute the 2x2 convs\n        # i used to belive that transposing half of the matrices would alleviate the expressiveness\n        # issue, but it is not notable.\n        # c22[1::2] = -c22[1::2].flip(-1, -2)\n\n        # we now need to compose the 2x2 convs to build the k*k kernel\n        # by using the associativity of the block conv operator we can\n        # run the steps of the BCOP algorithm in parallel: we groups the\n        # 2x2 convs in pairs and apply the block conv operator to each pair\n        # until we have a single conv. If k-1 is a power of two this algorithm\n        # run in log(k-1) steps. (naive associative scan algorithm)\n        while c22.shape[0] % 2 == 0:\n            mid = c22.shape[0] // 2\n            c22 = fast_batched_matrix_conv(c22[:mid], c22[mid:], self.groups)\n        # we finally compose the 1x1 conv with the kxk conv\n        res = c11\n        for i in range(c22.shape[0]):  # c22.shape[0] == 1 if k-1 is a power of two\n            res = fast_matrix_conv(res, c22[i], self.groups)\n        # since it is less expensive to compute the transposed kernel when co &lt; ci\n        # we transpose the kernel if needed\n        if self.transpose:\n            res = transpose_kernel(res, self.groups, flip=False)\n        # it seems more efficient to make the kernel contiguous since it will be used\n        # in a convolution\n        return res.contiguous()\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, groups)</code>","text":"<p>This module is used to generate orthogonal kernels for the BCOP layer. It takes as input a matrix PQ of shape (groups, 2*kernel_size, c, c//2) and returns a kernel of shape (c, c, kernel_size, kernel_size) that is orthogonal.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>number of input channels</p> required <code>out_channels</code> <code>int</code> <p>number of output channels</p> required <code>kernel_size</code> <code>int</code> <p>size of the kernel</p> required <code>groups</code> <code>int</code> <p>number of groups</p> required Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels,\n    out_channels,\n    kernel_size,\n    groups,\n):\n    \"\"\"This module is used to generate orthogonal kernels for the BCOP layer. It takes\n    as input a matrix PQ of shape (groups, 2*kernel_size, c, c//2) and returns a kernel\n    of shape (c, c, kernel_size, kernel_size) that is orthogonal.\n\n    Args:\n        in_channels (int): number of input channels\n        out_channels (int): number of output channels\n        kernel_size (int): size of the kernel\n        groups (int): number of groups\n    \"\"\"\n    super(BCOPTrivializer, self).__init__()\n    self.kernel_size = kernel_size\n    self.groups = groups\n    self.out_channels = out_channels\n    self.in_channels = in_channels\n    self.min_channels = min(in_channels, out_channels)\n    self.max_channels = max(in_channels, out_channels)\n    self.transpose = out_channels &lt; in_channels\n    self.num_kernels = 2 * kernel_size\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d","title":"<code>FastBlockConv2d</code>","text":"<p>               Bases: <code>Conv2d</code></p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>class FastBlockConv2d(nn.Conv2d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: Union[str, _size_2_t] = \"same\",\n        dilation: _size_2_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = \"circular\",\n        ortho_params: OrthoParams = OrthoParams(),\n    ):\n        \"\"\"\n        Fast implementation of the Block Circulant Orthogonal Parametrization (BCOP) for convolutional layers.\n        This approach changes the original BCOP algorithm to make it more scalable and efficient. This implementation\n        rewrite efficiently the block convolution operator as a single convolution operation. Also the iterative algorithm\n        is parallelized in the associative scan fashion.\n\n        This layer is a drop-in replacement for the nn.Conv2d layer. It is orthogonal and Lipschitz continuous while maintaining\n        the same interface as the Con2d. Also this method has an explicit kernel, whihc allows to compute the singular values of\n        the convolutional layer.\n\n        Striding is not supported when out_channels &gt; in_channels. Real striding is supported in BcopRkoConv2d. The use of\n        OrthogonalConv2d is recommended.\n        \"\"\"\n        super(FastBlockConv2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode,\n        )\n\n        # raise runtime error if kernel size &gt;= stride\n        if self.kernel_size[0] &lt; self.stride[0] or self.kernel_size[1] &lt; self.stride[1]:\n            raise ValueError(\n                \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n            )\n        if (\n            (self.stride[0] &gt; 1 or self.stride[1] &gt; 1) and (out_channels &gt; in_channels)\n        ) or (\n            self.stride[0] &gt; self.kernel_size[0] or self.stride[1] &gt; self.kernel_size[1]\n        ):\n            raise ValueError(\n                \"stride &gt; 1 is not supported when out_channels &gt; in_channels, \"\n                \"use TODO layer instead\"\n            )\n        if (\n            (self.out_channels &gt;= self.in_channels)\n            and (((self.dilation[0] % self.stride[0]) == 0) and (self.stride[0] &gt; 1))\n            and (((self.dilation[1] % self.stride[1]) == 0) and (self.stride[1] &gt; 1))\n        ):\n            raise ValueError(\n                \"dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.\"\n            )\n        del self.weight\n        attach_bcop_weight(\n            self,\n            \"weight\",\n            (\n                out_channels,\n                in_channels // groups,\n                self.kernel_size[0],\n                self.kernel_size[1],\n            ),\n            groups,\n            ortho_params=ortho_params,\n        )\n\n    def singular_values(self):\n        \"\"\"Compute the singular values of the convolutional layer using the FFT+SVD method.\n\n        Returns:\n            Tuple[float, float, float]: min singular value, max singular value and\n            normalized stable rank (1 means orthogonal matrix)\n        \"\"\"\n        # use the fft+svd method to compute the singular values\n        # assuming circular padding, if \"zero\" padding is used the value\n        # will be overestimated (ie. the singular values will be larger than\n        # the real ones)\n        if self.padding_mode != \"circular\":\n            print(\n                f\"padding {self.padding} not supported, return min and max\"\n                f\"singular values as if it was 'circular' padding \"\n                f\"(overestimate the values).\"\n            )\n        sv_min, sv_max, stable_rank = conv_singular_values_numpy(\n            self.weight.detach()\n            .cpu()\n            .view(\n                self.groups,\n                self.out_channels // self.groups,\n                self.in_channels // self.groups,\n                self.kernel_size[0],\n                self.kernel_size[1],\n            )\n            .numpy(),\n            self._input_shape,\n        )\n        return sv_min, sv_max, stable_rank\n\n    def forward(self, X):\n        self._input_shape = X.shape[2:]\n        return super(FastBlockConv2d, self).forward(X)\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, padding='same', dilation=1, groups=1, bias=True, padding_mode='circular', ortho_params=OrthoParams())</code>","text":"<p>Fast implementation of the Block Circulant Orthogonal Parametrization (BCOP) for convolutional layers. This approach changes the original BCOP algorithm to make it more scalable and efficient. This implementation rewrite efficiently the block convolution operator as a single convolution operation. Also the iterative algorithm is parallelized in the associative scan fashion.</p> <p>This layer is a drop-in replacement for the nn.Conv2d layer. It is orthogonal and Lipschitz continuous while maintaining the same interface as the Con2d. Also this method has an explicit kernel, whihc allows to compute the singular values of the convolutional layer.</p> <p>Striding is not supported when out_channels &gt; in_channels. Real striding is supported in BcopRkoConv2d. The use of OrthogonalConv2d is recommended.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: Union[str, _size_2_t] = \"same\",\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"circular\",\n    ortho_params: OrthoParams = OrthoParams(),\n):\n    \"\"\"\n    Fast implementation of the Block Circulant Orthogonal Parametrization (BCOP) for convolutional layers.\n    This approach changes the original BCOP algorithm to make it more scalable and efficient. This implementation\n    rewrite efficiently the block convolution operator as a single convolution operation. Also the iterative algorithm\n    is parallelized in the associative scan fashion.\n\n    This layer is a drop-in replacement for the nn.Conv2d layer. It is orthogonal and Lipschitz continuous while maintaining\n    the same interface as the Con2d. Also this method has an explicit kernel, whihc allows to compute the singular values of\n    the convolutional layer.\n\n    Striding is not supported when out_channels &gt; in_channels. Real striding is supported in BcopRkoConv2d. The use of\n    OrthogonalConv2d is recommended.\n    \"\"\"\n    super(FastBlockConv2d, self).__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        groups,\n        bias,\n        padding_mode,\n    )\n\n    # raise runtime error if kernel size &gt;= stride\n    if self.kernel_size[0] &lt; self.stride[0] or self.kernel_size[1] &lt; self.stride[1]:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if (\n        (self.stride[0] &gt; 1 or self.stride[1] &gt; 1) and (out_channels &gt; in_channels)\n    ) or (\n        self.stride[0] &gt; self.kernel_size[0] or self.stride[1] &gt; self.kernel_size[1]\n    ):\n        raise ValueError(\n            \"stride &gt; 1 is not supported when out_channels &gt; in_channels, \"\n            \"use TODO layer instead\"\n        )\n    if (\n        (self.out_channels &gt;= self.in_channels)\n        and (((self.dilation[0] % self.stride[0]) == 0) and (self.stride[0] &gt; 1))\n        and (((self.dilation[1] % self.stride[1]) == 0) and (self.stride[1] &gt; 1))\n    ):\n        raise ValueError(\n            \"dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.\"\n        )\n    del self.weight\n    attach_bcop_weight(\n        self,\n        \"weight\",\n        (\n            out_channels,\n            in_channels // groups,\n            self.kernel_size[0],\n            self.kernel_size[1],\n        ),\n        groups,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d.singular_values","title":"<code>singular_values()</code>","text":"<p>Compute the singular values of the convolutional layer using the FFT+SVD method.</p> <p>Returns:</p> Type Description <p>Tuple[float, float, float]: min singular value, max singular value and</p> <p>normalized stable rank (1 means orthogonal matrix)</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def singular_values(self):\n    \"\"\"Compute the singular values of the convolutional layer using the FFT+SVD method.\n\n    Returns:\n        Tuple[float, float, float]: min singular value, max singular value and\n        normalized stable rank (1 means orthogonal matrix)\n    \"\"\"\n    # use the fft+svd method to compute the singular values\n    # assuming circular padding, if \"zero\" padding is used the value\n    # will be overestimated (ie. the singular values will be larger than\n    # the real ones)\n    if self.padding_mode != \"circular\":\n        print(\n            f\"padding {self.padding} not supported, return min and max\"\n            f\"singular values as if it was 'circular' padding \"\n            f\"(overestimate the values).\"\n        )\n    sv_min, sv_max, stable_rank = conv_singular_values_numpy(\n        self.weight.detach()\n        .cpu()\n        .view(\n            self.groups,\n            self.out_channels // self.groups,\n            self.in_channels // self.groups,\n            self.kernel_size[0],\n            self.kernel_size[1],\n        )\n        .numpy(),\n        self._input_shape,\n    )\n    return sv_min, sv_max, stable_rank\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D","title":"<code>FastBlockConvTranspose2D</code>","text":"<p>               Bases: <code>ConvTranspose2d</code></p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>class FastBlockConvTranspose2D(nn.ConvTranspose2d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        output_padding: _size_2_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_2_t = 1,\n        padding_mode: str = \"zeros\",\n        ortho_params: OrthoParams = OrthoParams(),\n    ):\n        \"\"\"\n        Extention of the BCOP algorithm to transposed convolutions. This implementation\n        uses the same algorithm as the FlashBCOP layer, but the layer acts as a transposed\n        convolutional layer.\n        \"\"\"\n        super(FastBlockConvTranspose2D, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding if padding_mode == \"zeros\" else 0,\n            output_padding,\n            groups,\n            bias,\n            dilation,\n            \"zeros\",\n        )\n        self.real_padding_mode = padding_mode\n        if padding == \"same\":\n            padding = self._calculate_same_padding()\n        self.real_padding = self._standardize_padding(padding)\n\n        if (\n            (self.out_channels &lt;= self.in_channels)\n            and (((self.dilation[0] % self.stride[0]) == 0) and (self.stride[0] &gt; 1))\n            and (((self.dilation[1] % self.stride[1]) == 0) and (self.stride[1] &gt; 1))\n        ):\n            raise ValueError(\n                \"dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.\"\n            )\n        if self.kernel_size[0] &lt; self.stride[0] or self.kernel_size[1] &lt; self.stride[1]:\n            raise ValueError(\n                \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n            )\n        if (\n            ((max(in_channels, out_channels) // groups) &lt; 2)\n            and (self.kernel_size[0] != self.stride[0])\n            and (self.kernel_size[1] != self.stride[1])\n        ):\n            raise ValueError(\"inner conv must have at least 2 channels\")\n        del self.weight\n        attach_bcop_weight(\n            self,\n            \"weight\",\n            (\n                in_channels,\n                out_channels // self.groups,\n                self.kernel_size[0],\n                self.kernel_size[1],\n            ),\n            groups,\n            ortho_params=ortho_params,\n        )\n\n    def _calculate_same_padding(self) -&gt; tuple:\n        \"\"\"Calculate padding for 'same' mode.\"\"\"\n        return (\n            int(\n                np.ceil(\n                    (self.dilation[0] * (self.kernel_size[0] - 1) + 1 - self.stride[0])\n                    / 2\n                )\n            ),\n            int(\n                np.floor(\n                    (self.dilation[0] * (self.kernel_size[0] - 1) + 1 - self.stride[0])\n                    / 2\n                )\n            ),\n            int(\n                np.ceil(\n                    (self.dilation[1] * (self.kernel_size[1] - 1) + 1 - self.stride[1])\n                    / 2\n                )\n            ),\n            int(\n                np.floor(\n                    (self.dilation[1] * (self.kernel_size[1] - 1) + 1 - self.stride[1])\n                    / 2\n                )\n            ),\n        )\n\n    def _standardize_padding(self, padding: _size_2_t) -&gt; tuple:\n        \"\"\"Ensure padding is always a tuple.\"\"\"\n        if isinstance(padding, int):\n            padding = (padding, padding)\n        if isinstance(padding, tuple):\n            if len(padding) == 2:\n                padding = (padding[0], padding[0], padding[1], padding[1])\n            return padding\n        raise ValueError(f\"padding must be int or tuple, got {type(padding)} instead\")\n\n    def singular_values(self):\n        if self.padding_mode != \"circular\":\n            print(\n                f\"padding {self.padding} not supported, return min and max\"\n                f\"singular values as if it was 'circular' padding \"\n                f\"(overestimate the values).\"\n            )\n        sv_min, sv_max, stable_rank = conv_singular_values_numpy(\n            self.weight.detach()\n            .cpu()\n            .reshape(\n                self.groups,\n                self.in_channels // self.groups,\n                self.out_channels // self.groups,\n                self.kernel_size[0],\n                self.kernel_size[1],\n            )\n            .numpy(),\n            self._input_shape,\n        )\n        return sv_min, sv_max, stable_rank\n\n    def forward(self, X):\n        self._input_shape = X.shape[2:]\n        if self.real_padding_mode != \"zeros\":\n            X = nn.functional.pad(X, self.real_padding, self.real_padding_mode)\n            y = nn.functional.conv_transpose2d(\n                X,\n                self.weight,\n                self.bias,\n                self.stride,\n                (\n                    (\n                        -self.stride[0]\n                        + self.dilation[0] * (self.kernel_size[0] - 1)\n                        + 1\n                    ),\n                    (\n                        -self.stride[1]\n                        + self.dilation[1] * (self.kernel_size[1] - 1)\n                        + 1\n                    ),\n                ),\n                self.output_padding,\n                self.groups,\n                dilation=self.dilation,\n            )\n            return y\n        else:\n            return super(FastBlockConvTranspose2D, self).forward(X)\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', ortho_params=OrthoParams())</code>","text":"<p>Extention of the BCOP algorithm to transposed convolutions. This implementation uses the same algorithm as the FlashBCOP layer, but the layer acts as a transposed convolutional layer.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    ortho_params: OrthoParams = OrthoParams(),\n):\n    \"\"\"\n    Extention of the BCOP algorithm to transposed convolutions. This implementation\n    uses the same algorithm as the FlashBCOP layer, but the layer acts as a transposed\n    convolutional layer.\n    \"\"\"\n    super(FastBlockConvTranspose2D, self).__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding if padding_mode == \"zeros\" else 0,\n        output_padding,\n        groups,\n        bias,\n        dilation,\n        \"zeros\",\n    )\n    self.real_padding_mode = padding_mode\n    if padding == \"same\":\n        padding = self._calculate_same_padding()\n    self.real_padding = self._standardize_padding(padding)\n\n    if (\n        (self.out_channels &lt;= self.in_channels)\n        and (((self.dilation[0] % self.stride[0]) == 0) and (self.stride[0] &gt; 1))\n        and (((self.dilation[1] % self.stride[1]) == 0) and (self.stride[1] &gt; 1))\n    ):\n        raise ValueError(\n            \"dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if self.kernel_size[0] &lt; self.stride[0] or self.kernel_size[1] &lt; self.stride[1]:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if (\n        ((max(in_channels, out_channels) // groups) &lt; 2)\n        and (self.kernel_size[0] != self.stride[0])\n        and (self.kernel_size[1] != self.stride[1])\n    ):\n        raise ValueError(\"inner conv must have at least 2 channels\")\n    del self.weight\n    attach_bcop_weight(\n        self,\n        \"weight\",\n        (\n            in_channels,\n            out_channels // self.groups,\n            self.kernel_size[0],\n            self.kernel_size[1],\n        ),\n        groups,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.attach_bcop_weight","title":"<code>attach_bcop_weight(layer, weight_name, kernel_shape, groups, ortho_params=OrthoParams())</code>","text":"<p>Attach a weight to a layer and parametrize it with the BCOPTrivializer module. The attached weight will be the kernel of an orthogonal convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Module</code> <p>layer to which the weight will be attached</p> required <code>weight_name</code> <code>str</code> <p>name of the weight</p> required <code>kernel_shape</code> <code>tuple</code> <p>shape of the kernel (out_channels, in_channels/groups, kernel_size, kernel_size)</p> required <code>groups</code> <code>int</code> <p>number of groups</p> required <code>bjorck_params</code> <code>BjorckParams</code> <p>parameters of the Bjorck orthogonalization. Defaults to BjorckParams().</p> required <p>Returns:</p> Type Description <p>torch.Tensor: a handle to the attached weight</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def attach_bcop_weight(\n    layer, weight_name, kernel_shape, groups, ortho_params: OrthoParams = OrthoParams()\n):\n    \"\"\"\n    Attach a weight to a layer and parametrize it with the BCOPTrivializer module.\n    The attached weight will be the kernel of an orthogonal convolutional layer.\n\n    Args:\n        layer (torch.nn.Module): layer to which the weight will be attached\n        weight_name (str): name of the weight\n        kernel_shape (tuple): shape of the kernel (out_channels, in_channels/groups, kernel_size, kernel_size)\n        groups (int): number of groups\n        bjorck_params (BjorckParams, optional): parameters of the Bjorck orthogonalization. Defaults to BjorckParams().\n\n    Returns:\n        torch.Tensor: a handle to the attached weight\n    \"\"\"\n    out_channels, in_channels, kernel_size, k2 = kernel_shape\n    in_channels *= groups  # compute the real number of input channels\n    assert kernel_size == k2, \"only square kernels are supported for the moment\"\n    max_channels = max(in_channels, out_channels)\n    num_kernels = (\n        2 * kernel_size\n    )  # the number of projectors needed to create the kernel\n    # register projectors matrices\n    layer.register_parameter(\n        weight_name,\n        torch.nn.Parameter(\n            torch.Tensor(\n                groups,\n                num_kernels,\n                (max_channels // groups),\n                (max_channels // (groups * 2)),\n            ),\n            requires_grad=True,\n        ),\n    )\n    weight = getattr(layer, weight_name)\n    torch.nn.init.orthogonal_(weight)\n    if weight.shape[-1] == 1:\n        # if max_channels//groups == 1, we can use L2 normalization\n        # instead of Bjorck orthogonalization which is significantly faster\n        parametrize.register_parametrization(\n            layer,\n            weight_name,\n            L2Normalize(dtype=weight.dtype, dim=(-2)),\n        )\n    else:\n        # register power iteration and Bjorck orthogonalization\n        parametrize.register_parametrization(\n            layer,\n            weight_name,\n            ortho_params.spectral_normalizer(weight_shape=weight.shape),\n        )\n        parametrize.register_parametrization(\n            layer,\n            weight_name,\n            ortho_params.orthogonalizer(\n                weight_shape=weight.shape,\n            ),\n        )\n    # now we have orthogonal projectors, we can build the orthogonal kernel\n    parametrize.register_parametrization(\n        layer,\n        weight_name,\n        BCOPTrivializer(\n            in_channels,\n            out_channels,\n            kernel_size,\n            groups,\n        ),\n        unsafe=True,\n    )\n    return weight\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.block_orth","title":"<code>block_orth(p1, p2)</code>","text":"<p>Construct a 2x2 orthogonal matrix from two orthogonal orthogonal projectors. Each projector can be seen as a 1x1 convolution, hence the stacking spatial stacking of [pi, I-pi] can be seen as a 2x1 or 1x2 orthogonal convolution. By using the block convolution operator, we can compute the 2x2 orthogonal conv. In this specific case, we can write the whole operation as a single einsum.</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Tensor</code> <p>orthogonal projector of shape (g, x, c, c) where g is the number of groups, x is a batch dimension (allowing to compute the operation in parallel) and c is the number of channels.</p> required <code>p2</code> <code>Tensor</code> <p>orthogonal projector of shape (g, x, c, c) also.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: orthogonal 2x2 conv of shape (x, g*c, c, 2, 2)</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def block_orth(p1, p2):\n    \"\"\"Construct a 2x2 orthogonal matrix from two orthogonal orthogonal projectors.\n    Each projector can be seen as a 1x1 convolution, hence the stacking spatial stacking\n    of [pi, I-pi] can be seen as a 2x1 or 1x2 orthogonal convolution. By using the block\n    convolution operator, we can compute the 2x2 orthogonal conv. In this specific case,\n    we can write the whole operation as a single einsum.\n\n    Args:\n        p1 (torch.Tensor): orthogonal projector of shape (g, x, c, c) where g is the number\n            of groups, x is a batch dimension (allowing to compute the operation in parallel)\n            and c is the number of channels.\n        p2 (torch.Tensor): orthogonal projector of shape (g, x, c, c) also.\n\n    Returns:\n        torch.Tensor: orthogonal 2x2 conv of shape (x, g*c, c, 2, 2)\n    \"\"\"\n    assert p1.shape == p2.shape\n    g, x, n, n2 = p1.shape\n    eye = torch.eye(n, device=p1.device, dtype=p1.dtype)\n    # sorry for using x as a batch dimension, but this einsum was hard to write (thank you unit tests!)\n    res = torch.einsum(\n        \"bgxij,cgxjk-&gt;xgikbc\", torch.stack([p1, eye - p1]), torch.stack([p2, eye - p2])\n    )\n    # we reshape the result to get a 2x2 conv kernel\n    res = res.reshape(x, g * n, n, 2, 2)\n    return res\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.conv_singular_values_numpy","title":"<code>conv_singular_values_numpy(kernel, input_shape)</code>","text":"<p>Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers. In International Conference on Learning Representations, 2019.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def conv_singular_values_numpy(kernel, input_shape):\n    \"\"\"\n    Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers.\n    In International Conference on Learning Representations, 2019.\n    \"\"\"\n    kernel = np.transpose(kernel, [0, 3, 4, 1, 2])  # g, k1, k2, ci, co\n    transforms = np.fft.fft2(kernel, input_shape, axes=[1, 2])  # g, k1, k2, ci, co\n    try:\n        svs = np.linalg.svd(\n            transforms, compute_uv=False, full_matrices=False\n        )  # g, k1, k2, min(ci, co)\n        stable_rank = (np.mean(svs) ** 2) / svs.max()\n        return svs.min(), svs.max(), stable_rank\n    except np.linalg.LinAlgError:\n        print(\"numerical error in svd, returning only largest singular value\")\n        return None, np.linalg.norm(transforms, axis=(1, 2), ord=2), None\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_batched_matrix_conv","title":"<code>fast_batched_matrix_conv(m1, m2, groups=1)</code>","text":"<p>Compute the convolution of two matrices using the block convolution operator. This is exactly the same as fast_matrix_conv but with an additional batch dimension. This is useful when we want to compute the convolution of multiple matrices in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>m1</code> <code>Tensor</code> <p>matrix of shape (b, c2, c1/g, k1, k2)</p> required <code>m2</code> <code>Tensor</code> <p>matrix of shape (b, c3, c2/g, l1, l2)</p> required <code>groups</code> <code>int</code> <p>number of groups. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <p>torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape</p> <p>(b, c3, c1, k+l-1, k+l-1)</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def fast_batched_matrix_conv(m1, m2, groups=1):\n    \"\"\"Compute the convolution of two matrices using the block convolution operator.\n    This is exactly the same as fast_matrix_conv but with an additional batch dimension.\n    This is useful when we want to compute the convolution of multiple matrices in parallel.\n\n    Args:\n        m1 (torch.Tensor): matrix of shape (b, c2, c1/g, k1, k2)\n        m2 (torch.Tensor): matrix of shape (b, c3, c2/g, l1, l2)\n        groups (int, optional): number of groups. Defaults to 1.\n\n    Returns:\n        torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape\n        (b, c3, c1, k+l-1, k+l-1)\n    \"\"\"\n    b, m, n, k1, k2 = m1.shape\n    b2, nb, mb, l1, l2 = m2.shape\n    assert m == mb * groups\n    assert b == b2\n    m1 = m1.view(b * m, n, k1, k2)\n    m2 = m2.view(b * nb, mb, l1, l2)\n    # Rearrange m1 for conv\n    m1 = m1.transpose(0, 1)  # n*m*k1*k2\n    # Rearrange m2 for conv\n    m2 = m2.flip(-2, -1)\n    r2 = torch.nn.functional.conv2d(m1, m2, groups=groups * b, padding=(l1 - 1, l2 - 1))\n    # Rearrange result\n    r2 = r2.view(n, b, nb, k1 + l1 - 1, k2 + l2 - 1)\n    r2 = r2.permute(1, 2, 0, 3, 4)\n    return r2\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_matrix_conv","title":"<code>fast_matrix_conv(m1, m2, groups=1)</code>","text":"<p>Compute the convolution of two matrices using the block convolution operator. The original algorithm can be written as a single convolution operation, which unlock the massive parallelism of the convolution operator. This implementation is also more memory efficient than the original algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>m1</code> <code>Tensor</code> <p>matrix of shape (c2, c1/g, k1, k2)</p> required <code>m2</code> <code>Tensor</code> <p>matrix of shape (c3, c2/g, l1, l2)</p> required <code>groups</code> <code>int</code> <p>number of groups. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <p>torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape</p> <p>(c3, c1, k+l-1, k+l-1)</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def fast_matrix_conv(m1, m2, groups=1):\n    \"\"\"Compute the convolution of two matrices using the block convolution operator.\n    The original algorithm can be written as a single convolution operation, which\n    unlock the massive parallelism of the convolution operator. This implementation\n    is also more memory efficient than the original algorithm.\n\n    Args:\n        m1 (torch.Tensor): matrix of shape (c2, c1/g, k1, k2)\n        m2 (torch.Tensor): matrix of shape (c3, c2/g, l1, l2)\n        groups (int, optional): number of groups. Defaults to 1.\n\n    Returns:\n        torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape\n        (c3, c1, k+l-1, k+l-1)\n\n    \"\"\"\n    # m1 is m*n*k1*k2\n    # m2 is nb*m*l1*l2\n    m, n, k1, k2 = m1.shape\n    nb, mb, l1, l2 = m2.shape\n    assert m == mb * groups\n\n    # Rearrange m1 for conv\n    m1 = m1.transpose(0, 1)  # n*m*k1*k2\n\n    # Rearrange m2 for conv\n    m2 = m2.flip(-2, -1)\n\n    # Run conv, output shape nb*n*(k+l-1)*(k+l-1)\n    r2 = torch.nn.functional.conv2d(m1, m2, groups=groups, padding=(l1 - 1, l2 - 1))\n\n    # Rearrange result\n    return r2.transpose(0, 1)  # n*nb*(k+l-1)*(k+l-1)\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.transpose_kernel","title":"<code>transpose_kernel(p, groups, flip=True)</code>","text":"<p>Compute the transpose of a kernel. This is done by transposing the kernel and flipping it along the last two dimensions. This operation is equivalent to the transpose of the convolution operator (when the stride is 1)</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Tensor</code> <p>kernel of shape (cig, cog, k1, k2)</p> required <code>groups</code> <code>int</code> <p>number of groups</p> required <code>flip</code> <code>bool</code> <p>if True, the kernel will be flipped. Defaults to True. False can be used when the is no need to flip the kernel.</p> <code>True</code> <p>Returns:</p> Type Description <p>torch.Tensor: transposed kernel of shape (cog, cig, k1, k2)</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def transpose_kernel(p, groups, flip=True):\n    \"\"\"Compute the transpose of a kernel. This is done by transposing the kernel and\n    flipping it along the last two dimensions. This operation is equivalent to the\n    transpose of the convolution operator (when the stride is 1)\n\n    Args:\n        p (torch.Tensor): kernel of shape (cig, cog, k1, k2)\n        groups (int): number of groups\n        flip (bool, optional): if True, the kernel will be flipped. Defaults to True.\n            False can be used when the is no need to flip the kernel.\n\n    Returns:\n        torch.Tensor: transposed kernel of shape (cog, cig, k1, k2)\n    \"\"\"\n    cig, cog, k1, k2 = p.shape\n    cig = cig // groups\n    # we do not perform flip since it does not affect orthogonality\n    p = p.view(groups, cig, cog, k1, k2)\n    p = p.transpose(1, 2)\n    if flip:\n        p = p.flip(-1, -2)\n    # merge groups to get the final kernel\n    p = p.reshape(cog * groups, cig, k1, k2)\n    return p\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.rko_conv.RkoConvTranspose2d","title":"<code>RkoConvTranspose2d</code>","text":"<p>               Bases: <code>ConvTranspose2d</code></p> Source code in <code>orthogonium\\layers\\conv\\AOC\\rko_conv.py</code> <pre><code>class RkoConvTranspose2d(nn.ConvTranspose2d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        output_padding: _size_2_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_2_t = 1,\n        padding_mode: str = \"zeros\",\n        ortho_params: OrthoParams = OrthoParams(),\n    ):\n        super(RkoConvTranspose2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding if padding_mode == \"zeros\" else 0,\n            output_padding,\n            groups,\n            bias,\n            dilation,\n            \"zeros\",\n        )\n        self.real_padding_mode = padding_mode\n        if padding == \"same\":\n            padding = self._calculate_same_padding()\n        self.real_padding = self._standardize_padding(padding)\n        if self.kernel_size[0] &lt; self.stride[0] or self.kernel_size[1] &lt; self.stride[1]:\n            raise ValueError(\n                \"kernel size must be smaller than stride. The set of orthogonal convolutions is empty in this setting.\"\n            )\n        if (\n            (self.out_channels &lt;= self.in_channels)\n            and (((self.dilation[0] % self.stride[0]) == 0) and (self.stride[0] &gt; 1))\n            and (((self.dilation[1] % self.stride[1]) == 0) and (self.stride[1] &gt; 1))\n        ):\n            raise ValueError(\n                \"dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.\"\n            )\n        if (\n            self.stride[0] != self.kernel_size[0]\n            or self.stride[1] != self.kernel_size[1]\n        ):\n            self.scale = 1 / math.sqrt(\n                math.ceil(self.kernel_size[0] / self.stride[0])\n                * math.ceil(self.kernel_size[1] / self.stride[1])\n            )\n        else:\n            self.scale = 1\n        del self.weight\n        attach_rko_weight(\n            self,\n            \"weight\",\n            (in_channels, out_channels // groups, self.stride[0], self.stride[1]),\n            groups,\n            scale=self.scale,\n            ortho_params=ortho_params,\n        )\n\n    def _calculate_same_padding(self) -&gt; tuple:\n        \"\"\"Calculate padding for 'same' mode.\"\"\"\n        return (\n            int(\n                np.ceil(\n                    (self.dilation[0] * (self.kernel_size[0] - 1) + 1 - self.stride[0])\n                    / 2\n                )\n            ),\n            int(\n                np.floor(\n                    (self.dilation[0] * (self.kernel_size[0] - 1) + 1 - self.stride[0])\n                    / 2\n                )\n            ),\n            int(\n                np.ceil(\n                    (self.dilation[1] * (self.kernel_size[1] - 1) + 1 - self.stride[1])\n                    / 2\n                )\n            ),\n            int(\n                np.floor(\n                    (self.dilation[1] * (self.kernel_size[1] - 1) + 1 - self.stride[1])\n                    / 2\n                )\n            ),\n        )\n\n    def _standardize_padding(self, padding: _size_2_t) -&gt; tuple:\n        \"\"\"Ensure padding is always a tuple.\"\"\"\n        if isinstance(padding, int):\n            padding = (padding, padding)\n        if isinstance(padding, tuple):\n            if len(padding) == 2:\n                padding = (padding[0], padding[0], padding[1], padding[1])\n            return padding\n        raise ValueError(f\"padding must be int or tuple, got {type(padding)} instead\")\n\n    def singular_values(self):\n        if (self.stride[0] == self.kernel_size[0]) and (\n            self.stride[1] == self.kernel_size[1]\n        ):\n            svs = np.linalg.svd(\n                self.weight.reshape(\n                    self.groups,\n                    self.in_channels // self.groups,\n                    (self.out_channels // self.groups)\n                    * (self.stride[0] * self.stride[1]),\n                )\n                .detach()\n                .cpu()\n                .numpy(),\n                compute_uv=False,\n            )\n            sv_min = svs.min()\n            sv_max = svs.max()\n            stable_rank = (np.mean(svs) ** 2) / (svs.max() ** 2)\n            return sv_min, sv_max, stable_rank\n        elif self.stride[0] &gt; 1 or self.stride[1] &gt; 1:\n            raise RuntimeError(\n                \"Not able to compute singular values for this configuration\"\n            )\n        # Implements interface required by LipschitzModuleL2\n        sv_min, sv_max, stable_rank = conv_singular_values_numpy(\n            self.weight.detach()\n            .cpu()\n            .view(\n                self.groups,\n                self.out_channels // self.groups,\n                self.in_channels // self.groups,\n                self.kernel_size[0],\n                self.kernel_size[1],\n            )\n            .numpy(),\n            self._input_shape,\n        )\n        return sv_min, sv_max, stable_rank\n\n    def forward(self, X):\n        self._input_shape = X.shape[2:]\n        if self.real_padding_mode != \"zeros\":\n            X = nn.functional.pad(X, self.real_padding, self.real_padding_mode)\n            y = nn.functional.conv_transpose2d(\n                X,\n                self.weight,\n                self.bias,\n                self.stride,\n                (\n                    (\n                        -self.stride[0]\n                        + self.dilation[0] * (self.kernel_size[0] - 1)\n                        + 1\n                    ),\n                    (\n                        -self.stride[1]\n                        + self.dilation[1] * (self.kernel_size[1] - 1)\n                        + 1\n                    ),\n                ),\n                self.output_padding,\n                self.groups,\n                dilation=self.dilation,\n            )\n            return y\n        else:\n            return super(RkoConvTranspose2d, self).forward(X)\n</code></pre>"},{"location":"api/conv/","title":"convolutions","text":""},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d","title":"<code>AdaptiveOrthoConv2d(in_channels, out_channels, kernel_size, stride=1, padding='same', dilation=1, groups=1, bias=True, padding_mode='circular', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel size and stride.</p>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features","title":"Key Features:","text":"<pre><code>- Enforces orthogonality, preserving gradient norms.\n- Supports native striding, dilation, grouped convolutions, and flexible padding.\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RKOConv2d`.\n- When stride == 1, the layer is a `FastBlockConv2d`.\n- Otherwise, the layer is a `BcopRkoConv2d`.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>str or _size_2_t</code> <p>Padding mode or size. Default is \"same\".</p> <code>'same'</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input to output channels. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"circular\".</p> <code>'circular'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>Conv2d</code> <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p> <p>Raises:</p> Type Description <code>`ValueError`</code> <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\ortho_conv.py</code> <pre><code>def AdaptiveOrthoConv2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: Union[str, _size_2_t] = \"same\",\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"circular\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.Conv2d:\n    \"\"\"\n    Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel size and stride.\n\n    Key Features:\n    -------------\n        - Enforces orthogonality, preserving gradient norms.\n        - Supports native striding, dilation, grouped convolutions, and flexible padding.\n\n    Behavior:\n    -------------\n        - When kernel_size == stride, the layer is an `RKOConv2d`.\n        - When stride == 1, the layer is a `FastBlockConv2d`.\n        - Otherwise, the layer is a `BcopRkoConv2d`.\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the convolution. Default is 1.\n        padding (str or _size_2_t, optional): Padding mode or size. Default is \"same\".\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        padding_mode (str, optional): Padding mode. Default is \"circular\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).\n\n    Raises:\n        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n    \"\"\"\n\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RKOConv2d\n    elif (stride == 1) or (in_channels &gt;= out_channels):\n        convclass = FastBlockConv2d\n    else:\n        convclass = BcopRkoConv2d\n    return convclass(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=groups,\n        bias=bias,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d","title":"<code>AdaptiveOrthoConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal convolutional transpose layer, adapting based on kernel size and stride.</p>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features","title":"Key Features:","text":"<pre><code>- Ensures orthogonality in transpose convolutions for stable gradient propagation.\n- Supports dilation, grouped operations, and efficient kernel construction.\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RkoConvTranspose2d`.\n- When stride == 1, the layer is a `FastBlockConvTranspose2D`.\n- Otherwise, the layer is a `BcopRkoConvTranspose2d`.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the transpose convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>_size_2_t</code> <p>Padding size. Default is 0.</p> <code>0</code> <code>output_padding</code> <code>_size_2_t</code> <p>Additional size for output. Default is 0.</p> <code>0</code> <code>groups</code> <code>int</code> <p>Number of groups. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"zeros\".</p> <code>'zeros'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>ConvTranspose2d</code> <p>A configured instance of <code>nn.ConvTranspose2d</code> (one of <code>RkoConvTranspose2d</code>, <code>FastBlockConvTranspose2D</code>, or <code>BcopRkoConvTranspose2d</code>).</p> <p>Raises: - <code>ValueError</code>: If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\ortho_conv.py</code> <pre><code>def AdaptiveOrthoConvTranspose2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.ConvTranspose2d:\n    \"\"\"\n    Factory function to create an orthogonal convolutional transpose layer, adapting based on kernel size and stride.\n\n    Key Features:\n    -------------\n        - Ensures orthogonality in transpose convolutions for stable gradient propagation.\n        - Supports dilation, grouped operations, and efficient kernel construction.\n\n    Behavior:\n    ---------\n        - When kernel_size == stride, the layer is an `RkoConvTranspose2d`.\n        - When stride == 1, the layer is a `FastBlockConvTranspose2D`.\n        - Otherwise, the layer is a `BcopRkoConvTranspose2d`.\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the transpose convolution. Default is 1.\n        padding (_size_2_t, optional): Padding size. Default is 0.\n        output_padding (_size_2_t, optional): Additional size for output. Default is 0.\n        groups (int, optional): Number of groups. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        padding_mode (str, optional): Padding mode. Default is \"zeros\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.ConvTranspose2d` (one of `RkoConvTranspose2d`, `FastBlockConvTranspose2D`, or `BcopRkoConvTranspose2d`).\n\n    **Raises:**\n    - `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n    \"\"\"\n\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RkoConvTranspose2d\n    elif stride == 1:\n        convclass = FastBlockConvTranspose2D\n    else:\n        convclass = BcopRkoConvTranspose2d\n    return convclass(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        output_padding=output_padding,\n        groups=groups,\n        bias=bias,\n        dilation=dilation,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer--ssl-derived-1-lipschitz-layers","title":"SSL derived 1-Lipschitz Layers","text":"<p>This module implements several 1-Lipschitz residual blocks, inspired by and extending the SDP-based Lipschitz Layers (SLL) from [1]. Specifically:</p> <ul> <li> <p><code>SDPBasedLipschitzResBlock</code>   The original version of the 1-Lipschitz convolutional residual block. It enforces Lipschitz   constraints by rescaling activation outputs according to an estimate of the operator norm.</p> </li> <li> <p><code>SLLxAOCLipschitzResBlock</code>   An extended version of the SLL approach described in [1], combined with additional orthogonal   convolutions to handle stride, kernel-size, or channel-dimension changes. It fuses multiple   convolutions via the block convolution, thereby preserving the 1-Lipschitz property while enabling   strided downsampling or modifying input/output channels.</p> </li> <li> <p><code>AOCLipschitzResBlock</code>   A variant of the original Lipschitz block where the core convolution is replaced by an   <code>AdaptiveOrthoConv2d</code>. It maintains the 1-Lipschitz property with orthogonal weight   parameterization while providing efficient convolution implementations.</p> </li> </ul>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer--references","title":"References","text":"<p>[1] Alexandre Araujo, Aaron J Havens, Blaise Delattre, Alexandre Allauzen, and Bin Hu. A unified alge- braic perspective on lipschitz neural networks. In The Eleventh International Conference on Learning Representations, 2023 [2] Thibaut Boissin, Franck Mamalet, Thomas Fel, Agustin Martin Picard, Thomas Massena, Mathieu Serrurier, An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures</p>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer--notes-on-the-sll-approach","title":"Notes on the SLL approach","text":"<p>In [1], the SLL layer for convolutions is a 1-Lipschitz residual operation defined approximately as:</p> \\[ y = x - \\mathbf{K}^T \\star (t \\times  \\sigma(\\mathbf{K} \\star x + b)), \\] <p>where \\(\\mathbf{K}\\) represents a toeplitz (convolution) matrix that represent a 1-Lipschitz operator. This is done in practice by computing a normalization vector \\(\\mathbf{t}\\) and rescaling the activation outputs by \\(\\mathbf{t}\\).</p> <p>By default, the SLL formulation does not allow strides or changes in the number of channels. To address these issues, <code>SLLxAOCLipschitzResBlock</code> adds extra orthogonal convolutions before and/or after the main SLL operation. These additional convolutions can be merged via block convolution (Proposition 1 in [2]) to maintain 1-Lipschitz behavior while enabling stride and/or channel changes.</p> <p>When \\(\\mathbf{K}\\), \\(\\mathbf{K}_{pre}\\), and \\(\\mathbf{K}_{post}\\) each correspond to 2\u00d72 convolutions, the resulting block effectively contains two 3\u00d73 convolutions in one branch and a single 4\u00d74 stride-2 convolution in the skip branch\u2014quite similar to typical ResNet blocks.</p>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock","title":"<code>AOCLipschitzResBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>class AOCLipschitzResBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        inner_dim_factor: int,\n        kernel_size: _size_2_t,\n        dilation: _size_2_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = \"circular\",\n        ortho_params: OrthoParams = OrthoParams(),\n    ):\n        \"\"\"\n        A Lipschitz residual block in which the main convolution is replaced by\n        `AdaptiveOrthoConv2d` (AOC). This preserves 1-Lipschitz (or lower) behavior through\n        an orthogonal parameterization, without explicitly computing a scaling factor `t`.\n\n        $$\n        y = x - \\mathbf{K}^T \\\\star (\\sigma(\\\\mathbf{K} \\\\star x + b)),\n        $$\n\n        **Args**:\n          - `in_channels` (int): Number of input channels.\n          - `inner_dim_factor` (int): Multiplier for internal representation size.\n          - `kernel_size` (_size_2_t): Convolution kernel size.\n          - `dilation` (_size_2_t, optional): Default is 1.\n          - `groups` (int, optional): Default is 1.\n          - `bias` (bool, optional): If True, adds a learnable bias. Default is True.\n          - `padding_mode` (str, optional): `'circular'` or `'zeros'`. Default is `'circular'`.\n          - `ortho_params` (OrthoParams, optional): Orthogonal parameterization settings. Default is `OrthoParams()`.\n        \"\"\"\n        super().__init__()\n\n        inner_dim = int(in_channels * inner_dim_factor)\n        self.activation = nn.ReLU()\n\n        if padding_mode not in [\"circular\", \"zeros\"]:\n            raise ValueError(\"padding_mode must be either 'circular' or 'zeros'\")\n        if padding_mode == \"circular\":\n            self.padding = 0  # will be handled by the padding function\n        else:\n            self.padding = kernel_size // 2\n\n        self.in_conv = AdaptiveOrthoConv2d(\n            in_channels,\n            inner_dim,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=\"same\",\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n            padding_mode=padding_mode,\n            ortho_params=ortho_params,\n        )\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n        self.groups = groups\n        self.bias = bias\n        self.padding_mode = padding_mode\n\n    def forward(self, x):\n        kernel = self.in_conv.weight\n        # conv\n        res = x\n        if self.padding_mode == \"circular\":\n            res = F.pad(\n                res,\n                (self.padding,) * 4,\n                mode=\"circular\",\n                value=0,\n            )\n        res = F.conv2d(\n            res,\n            kernel,\n            bias=self.in_conv.bias,\n            padding=0,\n            groups=self.groups,\n        )\n        # activation\n        res = self.activation(res)\n        # conv transpose\n        if self.padding_mode == \"circular\":\n            res = F.pad(\n                res,\n                (self.padding,) * 4,\n                mode=\"circular\",\n                value=0,\n            )\n        res = 2 * F.conv_transpose2d(res, kernel, padding=0, groups=self.groups)\n        # residual\n        out = x - res\n        return out\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock.__init__","title":"<code>__init__(in_channels, inner_dim_factor, kernel_size, dilation=1, groups=1, bias=True, padding_mode='circular', ortho_params=OrthoParams())</code>","text":"<p>A Lipschitz residual block in which the main convolution is replaced by <code>AdaptiveOrthoConv2d</code> (AOC). This preserves 1-Lipschitz (or lower) behavior through an orthogonal parameterization, without explicitly computing a scaling factor <code>t</code>.</p> \\[ y = x - \\mathbf{K}^T \\star (\\sigma(\\mathbf{K} \\star x + b)), \\] <p>Args:   - <code>in_channels</code> (int): Number of input channels.   - <code>inner_dim_factor</code> (int): Multiplier for internal representation size.   - <code>kernel_size</code> (_size_2_t): Convolution kernel size.   - <code>dilation</code> (_size_2_t, optional): Default is 1.   - <code>groups</code> (int, optional): Default is 1.   - <code>bias</code> (bool, optional): If True, adds a learnable bias. Default is True.   - <code>padding_mode</code> (str, optional): <code>'circular'</code> or <code>'zeros'</code>. Default is <code>'circular'</code>.   - <code>ortho_params</code> (OrthoParams, optional): Orthogonal parameterization settings. Default is <code>OrthoParams()</code>.</p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    inner_dim_factor: int,\n    kernel_size: _size_2_t,\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"circular\",\n    ortho_params: OrthoParams = OrthoParams(),\n):\n    \"\"\"\n    A Lipschitz residual block in which the main convolution is replaced by\n    `AdaptiveOrthoConv2d` (AOC). This preserves 1-Lipschitz (or lower) behavior through\n    an orthogonal parameterization, without explicitly computing a scaling factor `t`.\n\n    $$\n    y = x - \\mathbf{K}^T \\\\star (\\sigma(\\\\mathbf{K} \\\\star x + b)),\n    $$\n\n    **Args**:\n      - `in_channels` (int): Number of input channels.\n      - `inner_dim_factor` (int): Multiplier for internal representation size.\n      - `kernel_size` (_size_2_t): Convolution kernel size.\n      - `dilation` (_size_2_t, optional): Default is 1.\n      - `groups` (int, optional): Default is 1.\n      - `bias` (bool, optional): If True, adds a learnable bias. Default is True.\n      - `padding_mode` (str, optional): `'circular'` or `'zeros'`. Default is `'circular'`.\n      - `ortho_params` (OrthoParams, optional): Orthogonal parameterization settings. Default is `OrthoParams()`.\n    \"\"\"\n    super().__init__()\n\n    inner_dim = int(in_channels * inner_dim_factor)\n    self.activation = nn.ReLU()\n\n    if padding_mode not in [\"circular\", \"zeros\"]:\n        raise ValueError(\"padding_mode must be either 'circular' or 'zeros'\")\n    if padding_mode == \"circular\":\n        self.padding = 0  # will be handled by the padding function\n    else:\n        self.padding = kernel_size // 2\n\n    self.in_conv = AdaptiveOrthoConv2d(\n        in_channels,\n        inner_dim,\n        kernel_size=kernel_size,\n        stride=1,\n        padding=\"same\",\n        dilation=dilation,\n        groups=groups,\n        bias=bias,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n    self.kernel_size = kernel_size\n    self.dilation = dilation\n    self.groups = groups\n    self.bias = bias\n    self.padding_mode = padding_mode\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense","title":"<code>SDPBasedLipschitzDense</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>class SDPBasedLipschitzDense(nn.Module):\n    def __init__(self, in_features, out_features, inner_dim, **kwargs):\n        \"\"\"\n        A 1-Lipschitz fully-connected layer (dense version). Similar to the convolutional\n        SLL approach, but operates on vectors:\n\n        $$\n        y = x - K^T \\\\times (t \\\\times \\sigma(K \\\\times x + b)),\n        $$\n\n        **Args**:\n          - `in_features` (int): Input size.\n          - `out_features` (int): Output size (must match `in_features` to remain 1-Lipschitz).\n          - `inner_dim` (int): The internal dimension used for the transform.\n        \"\"\"\n        super().__init__()\n\n        inner_dim = inner_dim if inner_dim != -1 else in_features\n        self.activation = nn.ReLU()\n\n        self.weight = nn.Parameter(torch.empty(inner_dim, in_features))\n        self.bias = nn.Parameter(torch.empty(1, inner_dim))\n        self.q = nn.Parameter(torch.randn(inner_dim))\n\n        nn.init.xavier_normal_(self.weight)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / np.sqrt(fan_in)\n        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n\n    def compute_t(self):\n        q = torch.exp(self.q)\n        q_inv = torch.exp(-self.q)\n        t = torch.abs(\n            torch.einsum(\"i,ik,kj,j -&gt; ij\", q_inv, self.weight, self.weight.T, q)\n        ).sum(1)\n        t = safe_inv(t)\n        return t\n\n    def forward(self, x):\n        t = self.compute_t()\n        res = F.linear(x, self.weight)\n        res = res + self.bias\n        res = t * self.activation(res)\n        res = 2 * F.linear(res, self.weight.T)\n        out = x - res\n        return out\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense.__init__","title":"<code>__init__(in_features, out_features, inner_dim, **kwargs)</code>","text":"<p>A 1-Lipschitz fully-connected layer (dense version). Similar to the convolutional SLL approach, but operates on vectors:</p> \\[ y = x - K^T \\times (t \\times \\sigma(K \\times x + b)), \\] <p>Args:   - <code>in_features</code> (int): Input size.   - <code>out_features</code> (int): Output size (must match <code>in_features</code> to remain 1-Lipschitz).   - <code>inner_dim</code> (int): The internal dimension used for the transform.</p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>def __init__(self, in_features, out_features, inner_dim, **kwargs):\n    \"\"\"\n    A 1-Lipschitz fully-connected layer (dense version). Similar to the convolutional\n    SLL approach, but operates on vectors:\n\n    $$\n    y = x - K^T \\\\times (t \\\\times \\sigma(K \\\\times x + b)),\n    $$\n\n    **Args**:\n      - `in_features` (int): Input size.\n      - `out_features` (int): Output size (must match `in_features` to remain 1-Lipschitz).\n      - `inner_dim` (int): The internal dimension used for the transform.\n    \"\"\"\n    super().__init__()\n\n    inner_dim = inner_dim if inner_dim != -1 else in_features\n    self.activation = nn.ReLU()\n\n    self.weight = nn.Parameter(torch.empty(inner_dim, in_features))\n    self.bias = nn.Parameter(torch.empty(1, inner_dim))\n    self.q = nn.Parameter(torch.randn(inner_dim))\n\n    nn.init.xavier_normal_(self.weight)\n    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n    bound = 1 / np.sqrt(fan_in)\n    nn.init.uniform_(self.bias, -bound, bound)  # bias init\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock","title":"<code>SDPBasedLipschitzResBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>class SDPBasedLipschitzResBlock(nn.Module):\n    def __init__(self, cin, inner_dim_factor, kernel_size=3, **kwargs):\n        \"\"\"\n         Original 1-Lipschitz convolutional residual block, based on the SDP-based Lipschitz\n        layer (SLL) approach [1]. It has a structure akin to:\n\n        out = x - 2 * ConvTranspose( t * ReLU(Conv(x) + bias) )\n\n        where `t` is a channel-wise scaling factor ensuring a Lipschitz constant \u2264 1.\n\n        !!! note\n            By default, `SDPBasedLipschitzResBlock` assumes `cin == cout` and does **not** handle\n            stride changes outside the skip connection (i.e., typically used when stride=1 or 2\n            for downsampling in a standard residual architecture).\n\n        **Args**:\n          - `cin` (int): Number of input channels.\n          - `cout` (int): Number of output channels.\n          - `inner_dim_factor` (float): Multiplier for the intermediate dimensionality.\n          - `kernel_size` (int, optional): Size of the convolution kernel. Default is 3.\n          - `stride` (int, optional): Stride for the skip connection. Default is 2.\n          - `**kwargs`: Additional keyword arguments (unused).\n        \"\"\"\n        super().__init__()\n\n        inner_dim = int(cin * inner_dim_factor)\n        self.activation = nn.ReLU()\n\n        self.padding = kernel_size // 2\n\n        self.kernel = nn.Parameter(\n            torch.randn(inner_dim, cin, kernel_size, kernel_size)\n        )\n        self.bias = nn.Parameter(torch.empty(1, inner_dim, 1, 1))\n        self.q = nn.Parameter(torch.randn(inner_dim))\n\n        nn.init.xavier_normal_(self.kernel)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.kernel)\n        bound = 1 / np.sqrt(fan_in)\n        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n\n    def compute_t(self):\n        ktk = F.conv2d(self.kernel, self.kernel, padding=self.kernel.shape[-1] - 1)\n        ktk = torch.abs(ktk)\n        q = torch.exp(self.q).reshape(-1, 1, 1, 1)\n        q_inv = torch.exp(-self.q).reshape(-1, 1, 1, 1)\n        t = (q_inv * ktk * q).sum((1, 2, 3))\n        t = safe_inv(t)\n        return t\n\n    def forward(self, x):\n        t = self.compute_t()\n        t = t.reshape(1, -1, 1, 1)\n        res = F.conv2d(x, self.kernel, padding=1)\n        res = res + self.bias\n        res = t * self.activation(res)\n        res = 2 * F.conv_transpose2d(res, self.kernel, padding=1)\n        out = x - res\n        return out\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock.__init__","title":"<code>__init__(cin, inner_dim_factor, kernel_size=3, **kwargs)</code>","text":"<p>Original 1-Lipschitz convolutional residual block, based on the SDP-based Lipschitz layer (SLL) approach [1]. It has a structure akin to:</p> <p>out = x - 2 * ConvTranspose( t * ReLU(Conv(x) + bias) )</p> <p>where <code>t</code> is a channel-wise scaling factor ensuring a Lipschitz constant \u2264 1.</p> <p>Note</p> <p>By default, <code>SDPBasedLipschitzResBlock</code> assumes <code>cin == cout</code> and does not handle stride changes outside the skip connection (i.e., typically used when stride=1 or 2 for downsampling in a standard residual architecture).</p> <p>Args:   - <code>cin</code> (int): Number of input channels.   - <code>cout</code> (int): Number of output channels.   - <code>inner_dim_factor</code> (float): Multiplier for the intermediate dimensionality.   - <code>kernel_size</code> (int, optional): Size of the convolution kernel. Default is 3.   - <code>stride</code> (int, optional): Stride for the skip connection. Default is 2.   - <code>**kwargs</code>: Additional keyword arguments (unused).</p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>def __init__(self, cin, inner_dim_factor, kernel_size=3, **kwargs):\n    \"\"\"\n     Original 1-Lipschitz convolutional residual block, based on the SDP-based Lipschitz\n    layer (SLL) approach [1]. It has a structure akin to:\n\n    out = x - 2 * ConvTranspose( t * ReLU(Conv(x) + bias) )\n\n    where `t` is a channel-wise scaling factor ensuring a Lipschitz constant \u2264 1.\n\n    !!! note\n        By default, `SDPBasedLipschitzResBlock` assumes `cin == cout` and does **not** handle\n        stride changes outside the skip connection (i.e., typically used when stride=1 or 2\n        for downsampling in a standard residual architecture).\n\n    **Args**:\n      - `cin` (int): Number of input channels.\n      - `cout` (int): Number of output channels.\n      - `inner_dim_factor` (float): Multiplier for the intermediate dimensionality.\n      - `kernel_size` (int, optional): Size of the convolution kernel. Default is 3.\n      - `stride` (int, optional): Stride for the skip connection. Default is 2.\n      - `**kwargs`: Additional keyword arguments (unused).\n    \"\"\"\n    super().__init__()\n\n    inner_dim = int(cin * inner_dim_factor)\n    self.activation = nn.ReLU()\n\n    self.padding = kernel_size // 2\n\n    self.kernel = nn.Parameter(\n        torch.randn(inner_dim, cin, kernel_size, kernel_size)\n    )\n    self.bias = nn.Parameter(torch.empty(1, inner_dim, 1, 1))\n    self.q = nn.Parameter(torch.randn(inner_dim))\n\n    nn.init.xavier_normal_(self.kernel)\n    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.kernel)\n    bound = 1 / np.sqrt(fan_in)\n    nn.init.uniform_(self.bias, -bound, bound)  # bias init\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock","title":"<code>SLLxAOCLipschitzResBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>class SLLxAOCLipschitzResBlock(nn.Module):\n    def __init__(self, cin, cout, inner_dim_factor, kernel_size=3, stride=2, **kwargs):\n        \"\"\"\n        Extended SLL-based convolutional residual block. Supports arbitrary kernel sizes,\n        strides, and changes in the number of channels by integrating additional\n        orthogonal convolutions *and* fusing them via `\\mathbconv` [1].\n\n        The forward pass follows:\n\n        $$\n        y = (\\mathbf{K}_{post} \\circledast \\mathbf{K}_{pre}) \\\\star x - (\\mathbf{K}_{post} \\circledast \\mathbf{K}^T) \\\\star (t \\\\times  \\sigma(( \\mathbf{K} \\circledast \\mathbf{K}_{pre}) \\\\star x + b)),\n        $$\n\n        where $\\mathbf{K}_{pre}$ and $\\mathbf{K}_{post}$ are obtained with AOC.\n\n\n        &lt;img src=\"../../assets/SLL_3.png\" alt=\"illustration of SLL x AOC\" width=\"600\"&gt;\n\n\n\n        where the kernel `\\kernel{K}` may effectively be expanded by pre/post AOC layers to\n        handle stride and channel changes. This approach is described in \"Improving\n        SDP-based Lipschitz Layers\" of [1].\n\n        **Args**:\n          - `cin` (int): Number of input channels.\n          - `inner_dim_factor` (float): Multiplier for the internal channel dimension.\n          - `kernel_size` (int, optional): Base kernel size for the SLL portion. Default is 3.\n          - `**kwargs`: Additional options (unused).\n        \"\"\"\n        super().__init__()\n        inner_kernel_size = kernel_size - (stride - 1)\n        self.skip_kernel_size = stride + (stride // 2)\n        inner_dim = int(cout * inner_dim_factor)\n        self.activation = nn.ReLU()\n        self.stride = stride\n        self.padding = kernel_size // 2\n        self.kernel = nn.Parameter(\n            torch.randn(inner_dim, cin, inner_kernel_size, inner_kernel_size)\n        )\n        self.bias = nn.Parameter(torch.empty(1, inner_dim, 1, 1))\n        self.q = nn.Parameter(torch.randn(inner_dim))\n\n        nn.init.xavier_normal_(self.kernel)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.kernel)\n        bound = 1 / np.sqrt(fan_in)\n        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n\n        self.pre_conv = AdaptiveOrthoConv2d(\n            cin, cin, kernel_size=stride, stride=1, bias=False, padding=0\n        )\n        self.post_conv = AdaptiveOrthoConv2d(\n            cin, cout, kernel_size=stride, stride=stride, bias=False, padding=0\n        )\n\n    def compute_t(self):\n        ktk = F.conv2d(self.kernel, self.kernel, padding=self.kernel.shape[-1] - 1)\n        ktk = torch.abs(ktk)\n        q = torch.exp(self.q).reshape(-1, 1, 1, 1)\n        q_inv = torch.exp(-self.q).reshape(-1, 1, 1, 1)\n        t = (q_inv * ktk * q).sum((1, 2, 3))\n        t = safe_inv(t)\n        return t\n\n    def forward(self, x):\n        # compute t\n        t = self.compute_t()\n        t = t.reshape(1, -1, 1, 1)\n        # print(self.pre_conv.weight.shape, self.kernel.shape, self.post_conv.weight.shape)\n        kernel_1a = fast_matrix_conv(self.pre_conv.weight, self.kernel, groups=1)\n        kernel_1b = fast_matrix_conv(\n            transpose_kernel(self.kernel, groups=1), self.post_conv.weight, groups=1\n        )\n        kernel_2 = fast_matrix_conv(\n            self.pre_conv.weight, self.post_conv.weight, groups=1\n        )\n        # first branch\n        # fuse pre conv with kernel\n        res = F.conv2d(x, kernel_1a, padding=self.padding)\n        res = res + self.bias\n        res = t * self.activation(res)\n        res = 2 * F.conv2d(res, kernel_1b, padding=self.padding, stride=self.stride)\n        # residual branch\n        x = F.conv2d(\n            x, kernel_2, padding=self.skip_kernel_size // 2, stride=self.stride\n        )\n        # skip connection\n        out = x - res\n        return out\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock.__init__","title":"<code>__init__(cin, cout, inner_dim_factor, kernel_size=3, stride=2, **kwargs)</code>","text":"<p>Extended SLL-based convolutional residual block. Supports arbitrary kernel sizes, strides, and changes in the number of channels by integrating additional orthogonal convolutions and fusing them via <code>\\mathbconv</code> [1].</p> <p>The forward pass follows:</p> \\[ y = (\\mathbf{K}_{post} \\circledast \\mathbf{K}_{pre}) \\star x - (\\mathbf{K}_{post} \\circledast \\mathbf{K}^T) \\star (t \\times  \\sigma(( \\mathbf{K} \\circledast \\mathbf{K}_{pre}) \\star x + b)), \\] <p>where \\(\\mathbf{K}_{pre}\\) and \\(\\mathbf{K}_{post}\\) are obtained with AOC.</p> <p></p> <p>where the kernel <code>\\kernel{K}</code> may effectively be expanded by pre/post AOC layers to handle stride and channel changes. This approach is described in \"Improving SDP-based Lipschitz Layers\" of [1].</p> <p>Args:   - <code>cin</code> (int): Number of input channels.   - <code>inner_dim_factor</code> (float): Multiplier for the internal channel dimension.   - <code>kernel_size</code> (int, optional): Base kernel size for the SLL portion. Default is 3.   - <code>**kwargs</code>: Additional options (unused).</p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>def __init__(self, cin, cout, inner_dim_factor, kernel_size=3, stride=2, **kwargs):\n    \"\"\"\n    Extended SLL-based convolutional residual block. Supports arbitrary kernel sizes,\n    strides, and changes in the number of channels by integrating additional\n    orthogonal convolutions *and* fusing them via `\\mathbconv` [1].\n\n    The forward pass follows:\n\n    $$\n    y = (\\mathbf{K}_{post} \\circledast \\mathbf{K}_{pre}) \\\\star x - (\\mathbf{K}_{post} \\circledast \\mathbf{K}^T) \\\\star (t \\\\times  \\sigma(( \\mathbf{K} \\circledast \\mathbf{K}_{pre}) \\\\star x + b)),\n    $$\n\n    where $\\mathbf{K}_{pre}$ and $\\mathbf{K}_{post}$ are obtained with AOC.\n\n\n    &lt;img src=\"../../assets/SLL_3.png\" alt=\"illustration of SLL x AOC\" width=\"600\"&gt;\n\n\n\n    where the kernel `\\kernel{K}` may effectively be expanded by pre/post AOC layers to\n    handle stride and channel changes. This approach is described in \"Improving\n    SDP-based Lipschitz Layers\" of [1].\n\n    **Args**:\n      - `cin` (int): Number of input channels.\n      - `inner_dim_factor` (float): Multiplier for the internal channel dimension.\n      - `kernel_size` (int, optional): Base kernel size for the SLL portion. Default is 3.\n      - `**kwargs`: Additional options (unused).\n    \"\"\"\n    super().__init__()\n    inner_kernel_size = kernel_size - (stride - 1)\n    self.skip_kernel_size = stride + (stride // 2)\n    inner_dim = int(cout * inner_dim_factor)\n    self.activation = nn.ReLU()\n    self.stride = stride\n    self.padding = kernel_size // 2\n    self.kernel = nn.Parameter(\n        torch.randn(inner_dim, cin, inner_kernel_size, inner_kernel_size)\n    )\n    self.bias = nn.Parameter(torch.empty(1, inner_dim, 1, 1))\n    self.q = nn.Parameter(torch.randn(inner_dim))\n\n    nn.init.xavier_normal_(self.kernel)\n    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.kernel)\n    bound = 1 / np.sqrt(fan_in)\n    nn.init.uniform_(self.bias, -bound, bound)  # bias init\n\n    self.pre_conv = AdaptiveOrthoConv2d(\n        cin, cin, kernel_size=stride, stride=1, bias=False, padding=0\n    )\n    self.post_conv = AdaptiveOrthoConv2d(\n        cin, cout, kernel_size=stride, stride=stride, bias=False, padding=0\n    )\n</code></pre>"},{"location":"api/linear/","title":"linear layers","text":""},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.OrthoLinear","title":"<code>OrthoLinear</code>","text":"<p>               Bases: <code>Linear</code></p> Source code in <code>orthogonium\\layers\\linear\\ortho_linear.py</code> <pre><code>class OrthoLinear(nn.Linear):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        ortho_params: OrthoParams = OrthoParams(),\n    ):\n        \"\"\"\n        Initializes an orthogonal linear layer with customizable orthogonalization parameters.\n\n        Attributes:\n            in_features : int\n                Number of input features.\n            out_features : int\n                Number of output features.\n            bias : bool\n                Whether to include a bias term in the layer. Default is True.\n            ortho_params : OrthoParams\n                Parameters for orthogonalization and spectral normalization. Default is the\n                default instance of OrthoParams.\n\n        Parameters:\n            in_features : int\n                The size of each input sample.\n            out_features : int\n                The size of each output sample.\n            bias : bool\n                Indicates if the layer should include a learnable bias parameter.\n            ortho_params : OrthoParams\n                An object containing orthogonalization and normalization configurations.\n\n        Notes\n        -----\n        The layer is initialized with orthogonal weights using `torch.nn.init.orthogonal_`.\n        Weight parameters are further parametrized for both spectral normalization and\n        orthogonal constraints using the provided `OrthoParams` object.\n        \"\"\"\n        super(OrthoLinear, self).__init__(in_features, out_features, bias=bias)\n        torch.nn.init.orthogonal_(self.weight)\n        parametrize.register_parametrization(\n            self,\n            \"weight\",\n            ortho_params.spectral_normalizer(\n                weight_shape=(self.out_features, self.in_features)\n            ),\n        )\n        parametrize.register_parametrization(\n            self, \"weight\", ortho_params.orthogonalizer(weight_shape=self.weight.shape)\n        )\n\n    def singular_values(self):\n        svs = np.linalg.svd(\n            self.weight.detach().cpu().numpy(), full_matrices=False, compute_uv=False\n        )\n        stable_rank = np.sum((np.mean(svs) ** 2)) / (svs.max() ** 2)\n        return svs.min(), svs.max(), stable_rank\n</code></pre>"},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.OrthoLinear.__init__","title":"<code>__init__(in_features, out_features, bias=True, ortho_params=OrthoParams())</code>","text":"<p>Initializes an orthogonal linear layer with customizable orthogonalization parameters.</p> <p>Attributes:</p> Name Type Description <code>in_features</code> <p>int Number of input features.</p> <code>out_features</code> <p>int Number of output features.</p> <code>bias</code> <p>bool Whether to include a bias term in the layer. Default is True.</p> <code>ortho_params</code> <p>OrthoParams Parameters for orthogonalization and spectral normalization. Default is the default instance of OrthoParams.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <p>int The size of each input sample.</p> required <code>out_features</code> <p>int The size of each output sample.</p> required <code>bias</code> <p>bool Indicates if the layer should include a learnable bias parameter.</p> <code>True</code> <code>ortho_params</code> <p>OrthoParams An object containing orthogonalization and normalization configurations.</p> <code>OrthoParams()</code>"},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.OrthoLinear.__init__--notes","title":"Notes","text":"<p>The layer is initialized with orthogonal weights using <code>torch.nn.init.orthogonal_</code>. Weight parameters are further parametrized for both spectral normalization and orthogonal constraints using the provided <code>OrthoParams</code> object.</p> Source code in <code>orthogonium\\layers\\linear\\ortho_linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    ortho_params: OrthoParams = OrthoParams(),\n):\n    \"\"\"\n    Initializes an orthogonal linear layer with customizable orthogonalization parameters.\n\n    Attributes:\n        in_features : int\n            Number of input features.\n        out_features : int\n            Number of output features.\n        bias : bool\n            Whether to include a bias term in the layer. Default is True.\n        ortho_params : OrthoParams\n            Parameters for orthogonalization and spectral normalization. Default is the\n            default instance of OrthoParams.\n\n    Parameters:\n        in_features : int\n            The size of each input sample.\n        out_features : int\n            The size of each output sample.\n        bias : bool\n            Indicates if the layer should include a learnable bias parameter.\n        ortho_params : OrthoParams\n            An object containing orthogonalization and normalization configurations.\n\n    Notes\n    -----\n    The layer is initialized with orthogonal weights using `torch.nn.init.orthogonal_`.\n    Weight parameters are further parametrized for both spectral normalization and\n    orthogonal constraints using the provided `OrthoParams` object.\n    \"\"\"\n    super(OrthoLinear, self).__init__(in_features, out_features, bias=bias)\n    torch.nn.init.orthogonal_(self.weight)\n    parametrize.register_parametrization(\n        self,\n        \"weight\",\n        ortho_params.spectral_normalizer(\n            weight_shape=(self.out_features, self.in_features)\n        ),\n    )\n    parametrize.register_parametrization(\n        self, \"weight\", ortho_params.orthogonalizer(weight_shape=self.weight.shape)\n    )\n</code></pre>"},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.UnitNormLinear","title":"<code>UnitNormLinear</code>","text":"<p>               Bases: <code>Linear</code></p> Source code in <code>orthogonium\\layers\\linear\\ortho_linear.py</code> <pre><code>class UnitNormLinear(nn.Linear):\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        A custom PyTorch Linear layer that ensures weights are normalized to unit norm along a specified dimension.\n\n        This class extends the torch.nn.Linear module and modifies the weight\n        matrix to maintain orthogonal initialization and unit norm\n        normalization during training. In this specific case, each output can be viewed as the result of a 1-Lipschitz\n        function. This means that the whole function in more than 1-Lipschitz but that each output taken independently\n        is 1-Lipschitz.\n\n        Attributes:\n            weight: The learnable weight tensor with orthogonal initialization\n                and enforced unit norm parametrization.\n\n        Args:\n            *args: Variable length positional arguments passed to the base\n                Linear class.\n            **kwargs: Variable length keyword arguments passed to the base\n                Linear class.\n        \"\"\"\n        super(UnitNormLinear, self).__init__(*args, **kwargs)\n        torch.nn.init.orthogonal_(self.weight)\n        parametrize.register_parametrization(\n            self,\n            \"weight\",\n            L2Normalize(dtype=self.weight.dtype, dim=1),\n        )\n\n    def singular_values(self):\n        svs = np.linalg.svd(\n            self.weight.detach().cpu().numpy(), full_matrices=False, compute_uv=False\n        )\n        stable_rank = np.sum(np.mean(svs) ** 2) / (svs.max() ** 2)\n        return svs.min(), svs.max(), stable_rank\n</code></pre>"},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.UnitNormLinear.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>A custom PyTorch Linear layer that ensures weights are normalized to unit norm along a specified dimension.</p> <p>This class extends the torch.nn.Linear module and modifies the weight matrix to maintain orthogonal initialization and unit norm normalization during training. In this specific case, each output can be viewed as the result of a 1-Lipschitz function. This means that the whole function in more than 1-Lipschitz but that each output taken independently is 1-Lipschitz.</p> <p>Attributes:</p> Name Type Description <code>weight</code> <p>The learnable weight tensor with orthogonal initialization and enforced unit norm parametrization.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length positional arguments passed to the base Linear class.</p> <code>()</code> <code>**kwargs</code> <p>Variable length keyword arguments passed to the base Linear class.</p> <code>{}</code> Source code in <code>orthogonium\\layers\\linear\\ortho_linear.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    A custom PyTorch Linear layer that ensures weights are normalized to unit norm along a specified dimension.\n\n    This class extends the torch.nn.Linear module and modifies the weight\n    matrix to maintain orthogonal initialization and unit norm\n    normalization during training. In this specific case, each output can be viewed as the result of a 1-Lipschitz\n    function. This means that the whole function in more than 1-Lipschitz but that each output taken independently\n    is 1-Lipschitz.\n\n    Attributes:\n        weight: The learnable weight tensor with orthogonal initialization\n            and enforced unit norm parametrization.\n\n    Args:\n        *args: Variable length positional arguments passed to the base\n            Linear class.\n        **kwargs: Variable length keyword arguments passed to the base\n            Linear class.\n    \"\"\"\n    super(UnitNormLinear, self).__init__(*args, **kwargs)\n    torch.nn.init.orthogonal_(self.weight)\n    parametrize.register_parametrization(\n        self,\n        \"weight\",\n        L2Normalize(dtype=self.weight.dtype, dim=1),\n    )\n</code></pre>"},{"location":"api/losses/","title":"losses","text":""},{"location":"api/losses/#orthogonium.losses.CosineLoss","title":"<code>CosineLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\losses.py</code> <pre><code>class CosineLoss(nn.Module):\n    def __init__(self):\n        \"\"\"\n        A class that implements the Cosine Loss for measuring the cosine similarity\n        between predictions and targets. Designed for use in scenarios involving\n        angle-based loss calculations or similarity measurements.\n\n        Attributes:\n            None\n\n        \"\"\"\n        super(CosineLoss, self).__init__()\n\n    def forward(self, yp, yt):\n        return -torch.nn.functional.cosine_similarity(\n            yp, torch.nn.functional.one_hot(yt, yp.shape[1])\n        ).mean()\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.CosineLoss.__init__","title":"<code>__init__()</code>","text":"<p>A class that implements the Cosine Loss for measuring the cosine similarity between predictions and targets. Designed for use in scenarios involving angle-based loss calculations or similarity measurements.</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A class that implements the Cosine Loss for measuring the cosine similarity\n    between predictions and targets. Designed for use in scenarios involving\n    angle-based loss calculations or similarity measurements.\n\n    Attributes:\n        None\n\n    \"\"\"\n    super(CosineLoss, self).__init__()\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.LossXent","title":"<code>LossXent</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\losses.py</code> <pre><code>class LossXent(nn.Module):\n    def __init__(self, n_classes, offset=2.12132, temperature=0.25):\n        \"\"\"\n        A custom loss function class for cross-entropy calculation.\n\n        This class initializes a cross-entropy loss criterion along with additional\n        parameters, such as an offset and a temperature factor, to allow a finer control over\n        the accuracy/robustness tradeoff during training.\n\n        Attributes:\n            criterion (nn.CrossEntropyLoss): The PyTorch cross-entropy loss criterion.\n            n_classes (int): The number of classes present in the dataset.\n            offset (float): An offset value for customizing the loss computation.\n            temperature (float): A temperature factor for scaling logits during loss calculation.\n\n        Parameters:\n            n_classes (int): The number of classes in the dataset.\n            offset (float, optional): The offset value for loss computation. Default is 2.12132.\n            temperature (float, optional): The temperature scaling factor. Default is 0.25.\n        \"\"\"\n        super(LossXent, self).__init__()\n        self.criterion = nn.CrossEntropyLoss()\n        self.n_classes = n_classes\n        self.offset = offset\n        self.temperature = temperature\n\n    def __call__(self, outputs, labels):\n        one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=self.n_classes)\n        offset_outputs = outputs - self.offset * one_hot_labels\n        offset_outputs /= self.temperature\n        loss = self.criterion(offset_outputs, labels) * self.temperature\n        return loss\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.LossXent.__init__","title":"<code>__init__(n_classes, offset=2.12132, temperature=0.25)</code>","text":"<p>A custom loss function class for cross-entropy calculation.</p> <p>This class initializes a cross-entropy loss criterion along with additional parameters, such as an offset and a temperature factor, to allow a finer control over the accuracy/robustness tradeoff during training.</p> <p>Attributes:</p> Name Type Description <code>criterion</code> <code>CrossEntropyLoss</code> <p>The PyTorch cross-entropy loss criterion.</p> <code>n_classes</code> <code>int</code> <p>The number of classes present in the dataset.</p> <code>offset</code> <code>float</code> <p>An offset value for customizing the loss computation.</p> <code>temperature</code> <code>float</code> <p>A temperature factor for scaling logits during loss calculation.</p> <p>Parameters:</p> Name Type Description Default <code>n_classes</code> <code>int</code> <p>The number of classes in the dataset.</p> required <code>offset</code> <code>float</code> <p>The offset value for loss computation. Default is 2.12132.</p> <code>2.12132</code> <code>temperature</code> <code>float</code> <p>The temperature scaling factor. Default is 0.25.</p> <code>0.25</code> Source code in <code>orthogonium\\losses.py</code> <pre><code>def __init__(self, n_classes, offset=2.12132, temperature=0.25):\n    \"\"\"\n    A custom loss function class for cross-entropy calculation.\n\n    This class initializes a cross-entropy loss criterion along with additional\n    parameters, such as an offset and a temperature factor, to allow a finer control over\n    the accuracy/robustness tradeoff during training.\n\n    Attributes:\n        criterion (nn.CrossEntropyLoss): The PyTorch cross-entropy loss criterion.\n        n_classes (int): The number of classes present in the dataset.\n        offset (float): An offset value for customizing the loss computation.\n        temperature (float): A temperature factor for scaling logits during loss calculation.\n\n    Parameters:\n        n_classes (int): The number of classes in the dataset.\n        offset (float, optional): The offset value for loss computation. Default is 2.12132.\n        temperature (float, optional): The temperature scaling factor. Default is 0.25.\n    \"\"\"\n    super(LossXent, self).__init__()\n    self.criterion = nn.CrossEntropyLoss()\n    self.n_classes = n_classes\n    self.offset = offset\n    self.temperature = temperature\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss","title":"<code>SoftHKRMulticlassLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\losses.py</code> <pre><code>class SoftHKRMulticlassLoss(torch.nn.Module):\n    def __init__(\n        self,\n        alpha=10.0,\n        min_margin=1.0,\n        alpha_mean=0.99,\n        temperature=1.0,\n    ):\n        \"\"\"\n        The multiclass version of HKR with softmax. This is done by computing\n        the HKR term over each class and averaging the results.\n\n        Note that `y_true` could be either one-hot encoded, +/-1 values.\n\n\n        Args:\n            alpha (float): regularization factor (0 &lt;= alpha &lt;= 1),\n                0 for KR only, 1 for hinge only\n            min_margin (float): margin to enforce.\n            temperature (float): factor for softmax  temperature\n                (higher value increases the weight of the highest non y_true logits)\n            alpha_mean (float): geometric mean factor\n            one_hot_ytrue (bool): set to True when y_true are one hot encoded (0 or 1),\n                and False when y_true already signed bases (for instance +/-1)\n            reduction: passed to tf.keras.Loss constructor\n            name (str): passed to tf.keras.Loss constructor\n\n        \"\"\"\n        assert (alpha &gt;= 0) and (alpha &lt;= 1), \"alpha must in [0,1]\"\n        self.alpha = torch.tensor(alpha, dtype=torch.float32)\n        self.min_margin_v = min_margin\n        self.alpha_mean = alpha_mean\n\n        self.current_mean = torch.tensor((self.min_margin_v,), dtype=torch.float32)\n        \"\"\"    constraint=lambda x: torch.clamp(x, 0.005, 1000),\n            name=\"current_mean\",\n        )\"\"\"\n\n        self.temperature = temperature * self.min_margin_v\n        if alpha == 1.0:  # alpha = 1.0 =&gt; hinge only\n            self.fct = self.multiclass_hinge_soft\n        else:\n            if alpha == 0.0:  # alpha = 0.0 =&gt; KR only\n                self.fct = self.kr_soft\n            else:\n                self.fct = self.hkr\n\n        super(SoftHKRMulticlassLoss, self).__init__()\n\n    def clamp_current_mean(self, x):\n        return torch.clamp(x, 0.005, 1000)\n\n    def _update_mean(self, y_pred):\n        self.current_mean = self.current_mean.to(y_pred.device)\n        current_global_mean = torch.mean(torch.abs(y_pred)).to(\n            dtype=self.current_mean.dtype\n        )\n        current_global_mean = (\n            self.alpha_mean * self.current_mean\n            + (1 - self.alpha_mean) * current_global_mean\n        )\n        self.current_mean = self.clamp_current_mean(current_global_mean).detach()\n        total_mean = current_global_mean\n        total_mean = torch.clamp(total_mean, self.min_margin_v, 20000)\n        return total_mean\n\n    def computeTemperatureSoftMax(self, y_true, y_pred):\n        total_mean = self._update_mean(y_pred)\n        current_temperature = (\n            torch.clamp(self.temperature / total_mean, 0.005, 250)\n            .to(dtype=y_pred.dtype)\n            .detach()\n        )\n        min_value = torch.tensor(torch.finfo(torch.float32).min, dtype=y_pred.dtype).to(\n            device=y_pred.device\n        )\n        opposite_values = torch.where(\n            y_true &gt; 0, min_value, current_temperature * y_pred\n        )\n        F_soft_KR = torch.softmax(opposite_values, dim=-1)\n        one_value = torch.tensor(1.0, dtype=F_soft_KR.dtype).to(device=y_pred.device)\n        F_soft_KR = torch.where(y_true &gt; 0, one_value, F_soft_KR)\n        return F_soft_KR\n\n    def signed_y_pred(self, y_true, y_pred):\n        \"\"\"Return for each item sign(y_true)*y_pred.\"\"\"\n        sign_y_true = torch.where(y_true &gt; 0, 1, -1)  # switch to +/-1\n        sign_y_true = sign_y_true.to(dtype=y_pred.dtype)\n        return y_pred * sign_y_true\n\n    def multiclass_hinge_preproc(self, signed_y_pred, min_margin):\n        \"\"\"From multiclass_hinge(y_true, y_pred, min_margin)\n        simplified to use precalculated signed_y_pred\"\"\"\n        # compute the elementwise hinge term\n        hinge = torch.nn.functional.relu(min_margin / 2.0 - signed_y_pred)\n        return hinge\n\n    def multiclass_hinge_soft_preproc(self, signed_y_pred, F_soft_KR):\n        hinge = self.multiclass_hinge_preproc(signed_y_pred, self.min_margin_v)\n        b = hinge * F_soft_KR\n        b = torch.sum(b, axis=-1)\n        return b\n\n    def multiclass_hinge_soft(self, y_true, y_pred):\n        F_soft_KR = self.computeTemperatureSoftMax(y_true, y_pred)\n        signed_y_pred = self.signed_y_pred(y_true, y_pred)\n        return self.multiclass_hinge_soft_preproc(signed_y_pred, F_soft_KR)\n\n    def kr_soft_preproc(self, signed_y_pred, F_soft_KR):\n        kr = -signed_y_pred\n        a = kr * F_soft_KR\n        a = torch.sum(a, axis=-1)\n        return a\n\n    def kr_soft(self, y_true, y_pred):\n        F_soft_KR = self.computeTemperatureSoftMax(y_true, y_pred)\n        signed_y_pred = self.signed_y_pred(y_true, y_pred)\n        return self.kr_soft_preproc(signed_y_pred, F_soft_KR)\n\n    def hkr(self, y_true, y_pred):\n        F_soft_KR = self.computeTemperatureSoftMax(y_true, y_pred)\n        signed_y_pred = self.signed_y_pred(y_true, y_pred)\n\n        loss_softkr = self.kr_soft_preproc(signed_y_pred, F_soft_KR)\n\n        loss_softhinge = self.multiclass_hinge_soft_preproc(signed_y_pred, F_soft_KR)\n        return (1 - self.alpha) * loss_softkr + self.alpha * loss_softhinge\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        target = torch.nn.functional.one_hot(target, num_classes=input.shape[1])\n        if not (isinstance(input, torch.Tensor)):  # required for dtype.max\n            input = torch.Tensor(input, dtype=input.dtype)\n        if not (isinstance(target, torch.Tensor)):\n            target = torch.Tensor(target, dtype=input.dtype)\n        loss_batch = self.fct(target, input)\n        return torch.mean(loss_batch)\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss.current_mean","title":"<code>current_mean = torch.tensor((self.min_margin_v), dtype=torch.float32)</code>  <code>instance-attribute</code>","text":"<p>constraint=lambda x: torch.clamp(x, 0.005, 1000),     name=\"current_mean\", )</p>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss.__init__","title":"<code>__init__(alpha=10.0, min_margin=1.0, alpha_mean=0.99, temperature=1.0)</code>","text":"<p>The multiclass version of HKR with softmax. This is done by computing the HKR term over each class and averaging the results.</p> <p>Note that <code>y_true</code> could be either one-hot encoded, +/-1 values.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>regularization factor (0 &lt;= alpha &lt;= 1), 0 for KR only, 1 for hinge only</p> <code>10.0</code> <code>min_margin</code> <code>float</code> <p>margin to enforce.</p> <code>1.0</code> <code>temperature</code> <code>float</code> <p>factor for softmax  temperature (higher value increases the weight of the highest non y_true logits)</p> <code>1.0</code> <code>alpha_mean</code> <code>float</code> <p>geometric mean factor</p> <code>0.99</code> <code>one_hot_ytrue</code> <code>bool</code> <p>set to True when y_true are one hot encoded (0 or 1), and False when y_true already signed bases (for instance +/-1)</p> required <code>reduction</code> <p>passed to tf.keras.Loss constructor</p> required <code>name</code> <code>str</code> <p>passed to tf.keras.Loss constructor</p> required Source code in <code>orthogonium\\losses.py</code> <pre><code>def __init__(\n    self,\n    alpha=10.0,\n    min_margin=1.0,\n    alpha_mean=0.99,\n    temperature=1.0,\n):\n    \"\"\"\n    The multiclass version of HKR with softmax. This is done by computing\n    the HKR term over each class and averaging the results.\n\n    Note that `y_true` could be either one-hot encoded, +/-1 values.\n\n\n    Args:\n        alpha (float): regularization factor (0 &lt;= alpha &lt;= 1),\n            0 for KR only, 1 for hinge only\n        min_margin (float): margin to enforce.\n        temperature (float): factor for softmax  temperature\n            (higher value increases the weight of the highest non y_true logits)\n        alpha_mean (float): geometric mean factor\n        one_hot_ytrue (bool): set to True when y_true are one hot encoded (0 or 1),\n            and False when y_true already signed bases (for instance +/-1)\n        reduction: passed to tf.keras.Loss constructor\n        name (str): passed to tf.keras.Loss constructor\n\n    \"\"\"\n    assert (alpha &gt;= 0) and (alpha &lt;= 1), \"alpha must in [0,1]\"\n    self.alpha = torch.tensor(alpha, dtype=torch.float32)\n    self.min_margin_v = min_margin\n    self.alpha_mean = alpha_mean\n\n    self.current_mean = torch.tensor((self.min_margin_v,), dtype=torch.float32)\n    \"\"\"    constraint=lambda x: torch.clamp(x, 0.005, 1000),\n        name=\"current_mean\",\n    )\"\"\"\n\n    self.temperature = temperature * self.min_margin_v\n    if alpha == 1.0:  # alpha = 1.0 =&gt; hinge only\n        self.fct = self.multiclass_hinge_soft\n    else:\n        if alpha == 0.0:  # alpha = 0.0 =&gt; KR only\n            self.fct = self.kr_soft\n        else:\n            self.fct = self.hkr\n\n    super(SoftHKRMulticlassLoss, self).__init__()\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss.multiclass_hinge_preproc","title":"<code>multiclass_hinge_preproc(signed_y_pred, min_margin)</code>","text":"<p>From multiclass_hinge(y_true, y_pred, min_margin) simplified to use precalculated signed_y_pred</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def multiclass_hinge_preproc(self, signed_y_pred, min_margin):\n    \"\"\"From multiclass_hinge(y_true, y_pred, min_margin)\n    simplified to use precalculated signed_y_pred\"\"\"\n    # compute the elementwise hinge term\n    hinge = torch.nn.functional.relu(min_margin / 2.0 - signed_y_pred)\n    return hinge\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss.signed_y_pred","title":"<code>signed_y_pred(y_true, y_pred)</code>","text":"<p>Return for each item sign(y_true)*y_pred.</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def signed_y_pred(self, y_true, y_pred):\n    \"\"\"Return for each item sign(y_true)*y_pred.\"\"\"\n    sign_y_true = torch.where(y_true &gt; 0, 1, -1)  # switch to +/-1\n    sign_y_true = sign_y_true.to(dtype=y_pred.dtype)\n    return y_pred * sign_y_true\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.VRA","title":"<code>VRA(output, class_indices, last_layer_type='classwise', L=1.0, eps=36 / 255, return_certs=False)</code>","text":"<p>Compute the verified robust accuracy (VRA) of a model's output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <p>torch.Tensor The output of the model.</p> required <code>class_indices</code> <p>torch.Tensor The indices of the correct classes. Should not be one-hot encoded.</p> required <code>last_layer_type</code> <p>str The type of the last layer of the model. Should be either \"classwise\" (L-lip per class) or \"global\" (L-lip globally).</p> <code>'classwise'</code> <code>L</code> <p>float The Lipschitz constant of the model.</p> <code>1.0</code> <code>eps</code> <p>float The perturbation size.</p> <code>36 / 255</code> <code>return_certs</code> <p>bool Whether to return the certificates instead of the VRA.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>vra</code> <p>torch.Tensor The VRA of the model.</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def VRA(\n    output,\n    class_indices,\n    last_layer_type=\"classwise\",\n    L=1.0,\n    eps=36 / 255,\n    return_certs=False,\n):\n    \"\"\"Compute the verified robust accuracy (VRA) of a model's output.\n\n    Args:\n        output : torch.Tensor\n            The output of the model.\n        class_indices : torch.Tensor\n            The indices of the correct classes. Should not be one-hot encoded.\n        last_layer_type : str\n            The type of the last layer of the model. Should be either \"classwise\" (L-lip per class) or \"global\" (L-lip globally).\n        L : float\n            The Lipschitz constant of the model.\n        eps : float\n            The perturbation size.\n        return_certs : bool\n            Whether to return the certificates instead of the VRA.\n\n    Returns:\n        vra : torch.Tensor\n            The VRA of the model.\n    \"\"\"\n    batch_size = output.shape[0]\n    batch_indices = torch.arange(batch_size)\n\n    # get the values of the correct class\n    output_class_indices = output[batch_indices, class_indices]\n    # get the values of the top class that is not the correct class\n    # create a mask indicating the correct class\n    onehot = torch.zeros_like(output).cuda()\n    onehot[torch.arange(output.shape[0]), class_indices] = 1.0\n    # subtracting a large number from the correct class to ensure it is not the max\n    # doing so will allow us to find the top of the output that is not the correct class\n    output_trunc = output - onehot * 1e6\n    output_nextmax = torch.max(output_trunc, dim=1)[0]\n    # now we can compute the certificates\n    output_diff = output_class_indices - output_nextmax\n    if last_layer_type == \"global\":\n        den = math.sqrt(2) * L\n    elif last_layer_type == \"classwise\":\n        den = 2 * L\n    else:\n        raise ValueError(\n            \"[VRA] last_layer_type should be either 'global' or 'classwise'\"\n        )\n    certs = output_diff / den\n    # now we can compute the vra\n    # vra is percentage of certs &gt; eps\n    vra = (certs &gt; eps).float()\n    if return_certs:\n        return certs\n    return vra\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.check_last_linear_layer_type","title":"<code>check_last_linear_layer_type(model)</code>","text":"<p>Determines the type of the last linear layer in a given model.</p> <p>This function inspects the architecture of the model and identifies the last linear layer of specific types (nn.Linear, OrthoLinear, UnitNormLinear). It then returns a string indicating the type of the last linear layer based on its class. This allows to determine the parameter to use for computing the VRA of a model's output.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model containing layers to be inspected.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string indicating the type of the last linear layer.  The possible values are:      - \"global\" if the layer is of type OrthoLinear.      - \"classwise\" if the layer is of type UnitNormLinear.      - \"unknown\" if the layer is of any other type or if no        linear layer is found.</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def check_last_linear_layer_type(model):\n    \"\"\"\n    Determines the type of the last linear layer in a given model.\n\n    This function inspects the architecture of the model and identifies the last\n    linear layer of specific types (nn.Linear, OrthoLinear, UnitNormLinear). It\n    then returns a string indicating the type of the last linear layer based on\n    its class. This allows to determine the parameter to use for computing the\n    VRA of a model's output.\n\n    Args:\n        model: The model containing layers to be inspected.\n\n    Returns:\n        str: A string indicating the type of the last linear layer.\n             The possible values are:\n                 - \"global\" if the layer is of type OrthoLinear.\n                 - \"classwise\" if the layer is of type UnitNormLinear.\n                 - \"unknown\" if the layer is of any other type or if no\n                   linear layer is found.\n    \"\"\"\n    # Find the last linear layer in the model\n    last_linear_layer = None\n    layers = list(model.children())\n    for layer in reversed(layers):\n        if (\n            isinstance(layer, nn.Linear)\n            or isinstance(layer, OrthoLinear)\n            or isinstance(layer, UnitNormLinear)\n        ):\n            last_linear_layer = layer\n            break\n\n    # Check the type of the last linear layer\n    if isinstance(last_linear_layer, OrthoLinear):\n        return \"global\"\n    elif isinstance(last_linear_layer, UnitNormLinear):\n        return \"classwise\"\n    else:\n        return \"unknown\"\n</code></pre>"},{"location":"api/reparametrizers/","title":"reparametrizers","text":""},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedBjorckOrthogonalization","title":"<code>BatchedBjorckOrthogonalization</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedBjorckOrthogonalization(nn.Module):\n    def __init__(self, weight_shape, beta=0.5, niters=12, pass_through=False):\n        \"\"\"\n        Initialize the BatchedBjorckOrthogonalization module.\n\n        This module implements the Bj\u00f6rck orthogonalization method, which iteratively refines\n        a weight matrix towards orthogonality. The method is especially effective when the\n        weight matrix columns are nearly orthonormal. It balances computational efficiency\n        with convergence speed through a user-defined `beta` parameter and iteration count.\n\n        Args:\n            weight_shape (tuple): The shape of the weight matrix to be orthogonalized.\n            beta (float): Coefficient controlling the convergence of the orthogonalization process.\n                Default is 0.5.\n            niters (int): Number of iterations for the orthogonalization algorithm. Default is 12.\n            pass_through (bool): If True, most iterations are performed without gradient computation,\n                which can improve efficiency.\n        \"\"\"\n        self.weight_shape = weight_shape\n        self.beta = beta\n        self.niters = niters\n        self.pass_through = pass_through\n        if weight_shape[-2] &lt; weight_shape[-1]:\n            self.wwtw_op = BatchedBjorckOrthogonalization.wwt_w_op\n        else:\n            self.wwtw_op = BatchedBjorckOrthogonalization.w_wtw_op\n        super(BatchedBjorckOrthogonalization, self).__init__()\n\n    @staticmethod\n    def w_wtw_op(w):\n        return w @ (w.transpose(-1, -2) @ w)\n\n    @staticmethod\n    def wwt_w_op(w):\n        return (w @ w.transpose(-1, -2)) @ w\n\n    def forward(self, w):\n        \"\"\"\n        Apply the Bj\u00f6rck orthogonalization process to the weight matrix.\n\n        The algorithm adjusts the input matrix to approximate the closest orthogonal matrix\n        by iteratively applying transformations based on the Bj\u00f6rck algorithm.\n\n        Args:\n            w (torch.Tensor): The weight matrix to be orthogonalized.\n\n        Returns:\n            torch.Tensor: The orthogonalized weight matrix.\n        \"\"\"\n        if self.pass_through:\n            with torch.no_grad():\n                for _ in range(self.niters):\n                    w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n            # Final iteration without no_grad, using parameters:\n            w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n        else:\n            for _ in range(self.niters):\n                w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n        return w\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedBjorckOrthogonalization.__init__","title":"<code>__init__(weight_shape, beta=0.5, niters=12, pass_through=False)</code>","text":"<p>Initialize the BatchedBjorckOrthogonalization module.</p> <p>This module implements the Bj\u00f6rck orthogonalization method, which iteratively refines a weight matrix towards orthogonality. The method is especially effective when the weight matrix columns are nearly orthonormal. It balances computational efficiency with convergence speed through a user-defined <code>beta</code> parameter and iteration count.</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <code>tuple</code> <p>The shape of the weight matrix to be orthogonalized.</p> required <code>beta</code> <code>float</code> <p>Coefficient controlling the convergence of the orthogonalization process. Default is 0.5.</p> <code>0.5</code> <code>niters</code> <code>int</code> <p>Number of iterations for the orthogonalization algorithm. Default is 12.</p> <code>12</code> <code>pass_through</code> <code>bool</code> <p>If True, most iterations are performed without gradient computation, which can improve efficiency.</p> <code>False</code> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape, beta=0.5, niters=12, pass_through=False):\n    \"\"\"\n    Initialize the BatchedBjorckOrthogonalization module.\n\n    This module implements the Bj\u00f6rck orthogonalization method, which iteratively refines\n    a weight matrix towards orthogonality. The method is especially effective when the\n    weight matrix columns are nearly orthonormal. It balances computational efficiency\n    with convergence speed through a user-defined `beta` parameter and iteration count.\n\n    Args:\n        weight_shape (tuple): The shape of the weight matrix to be orthogonalized.\n        beta (float): Coefficient controlling the convergence of the orthogonalization process.\n            Default is 0.5.\n        niters (int): Number of iterations for the orthogonalization algorithm. Default is 12.\n        pass_through (bool): If True, most iterations are performed without gradient computation,\n            which can improve efficiency.\n    \"\"\"\n    self.weight_shape = weight_shape\n    self.beta = beta\n    self.niters = niters\n    self.pass_through = pass_through\n    if weight_shape[-2] &lt; weight_shape[-1]:\n        self.wwtw_op = BatchedBjorckOrthogonalization.wwt_w_op\n    else:\n        self.wwtw_op = BatchedBjorckOrthogonalization.w_wtw_op\n    super(BatchedBjorckOrthogonalization, self).__init__()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedBjorckOrthogonalization.forward","title":"<code>forward(w)</code>","text":"<p>Apply the Bj\u00f6rck orthogonalization process to the weight matrix.</p> <p>The algorithm adjusts the input matrix to approximate the closest orthogonal matrix by iteratively applying transformations based on the Bj\u00f6rck algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>Tensor</code> <p>The weight matrix to be orthogonalized.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The orthogonalized weight matrix.</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def forward(self, w):\n    \"\"\"\n    Apply the Bj\u00f6rck orthogonalization process to the weight matrix.\n\n    The algorithm adjusts the input matrix to approximate the closest orthogonal matrix\n    by iteratively applying transformations based on the Bj\u00f6rck algorithm.\n\n    Args:\n        w (torch.Tensor): The weight matrix to be orthogonalized.\n\n    Returns:\n        torch.Tensor: The orthogonalized weight matrix.\n    \"\"\"\n    if self.pass_through:\n        with torch.no_grad():\n            for _ in range(self.niters):\n                w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n        # Final iteration without no_grad, using parameters:\n        w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n    else:\n        for _ in range(self.niters):\n            w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n    return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedCholeskyOrthogonalization","title":"<code>BatchedCholeskyOrthogonalization</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedCholeskyOrthogonalization(nn.Module):\n    def __init__(self, weight_shape, stable=False):\n        \"\"\"\n        Initialize the BatchedCholeskyOrthogonalization module.\n\n        This module orthogonalizes a weight matrix using the Cholesky decomposition method.\n        It first computes the positive definite matrix \\( V V^T \\), then performs a Cholesky\n        decomposition to obtain a lower triangular matrix. Solving the resulting triangular\n        system yields an orthogonal matrix. This method is efficient and numerically stable,\n        making it suitable for a wide range of applications.\n\n        Args:\n            weight_shape (tuple): The shape of the weight matrix.\n            stable (bool): Whether to use the stable version of the Cholesky-based orthogonalization\n                function, which adds a small positive diagonal element to ensure numerical stability.\n                Default is False.\n        \"\"\"\n        self.weight_shape = weight_shape\n        super(BatchedCholeskyOrthogonalization, self).__init__()\n        if stable:\n            self.orth = BatchedCholeskyOrthogonalization.CholeskyOrthfn_stable.apply\n        else:\n            self.orth = BatchedCholeskyOrthogonalization.CholeskyOrthfn.apply\n\n    # @staticmethod\n    # def orth(X):\n    #     S = X @ X.mT\n    #     eps = S.diagonal(dim1=1, dim2=2).mean(1).mul(1e-3).detach()\n    #     eye = torch.eye(S.size(-1), dtype=S.dtype, device=S.device)\n    #     S = S + eps.view(-1, 1, 1) * eye.unsqueeze(0)\n    #     L = torch.linalg.cholesky(S)\n    #     W = torch.linalg.solve_triangular(L, X, upper=False)\n    #     return W\n\n    class CholeskyOrthfn(torch.autograd.Function):\n        @staticmethod\n        # def forward(ctx, X):\n        #     S = X @ X.mT\n        #     eps = S.diagonal(dim1=1, dim2=2).mean(1).mul(1e-3)\n        #     eye = torch.eye(S.size(-1), dtype=S.dtype, device=S.device)\n        #     S = S + eps.view(-1, 1, 1) * eye.unsqueeze(0)\n        #     L = torch.linalg.cholesky(S)\n        #     W = torch.linalg.solve_triangular(L, X, upper=False)\n        #     ctx.save_for_backward(W, L)\n        #     return W\n        def forward(ctx, X):\n            S = X @ X.mT\n            eps = 1e-5  # A common stable choice\n            S = S + eps * torch.eye(\n                S.size(-1), dtype=S.dtype, device=S.device\n            ).unsqueeze(0)\n            L = torch.linalg.cholesky(S)\n            W = torch.linalg.solve_triangular(L, X, upper=False)\n            ctx.save_for_backward(W, L)\n            return W\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            W, L = ctx.saved_tensors\n            LmT = L.mT.contiguous()\n            gB = torch.linalg.solve_triangular(LmT, grad_output, upper=True)\n            gA = (-gB @ W.mT).tril()\n            gS = (LmT @ gA).tril()\n            gS = gS + gS.tril(-1).mT\n            gS = torch.linalg.solve_triangular(LmT, gS, upper=True)\n            gX = gS @ W + gB\n            return gX\n\n    class CholeskyOrthfn_stable(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, X):\n            S = X @ X.mT\n            eps = 1e-5  # A common stable choice\n            S = S + eps * torch.eye(\n                S.size(-1), dtype=S.dtype, device=S.device\n            ).unsqueeze(0)\n            L = torch.linalg.cholesky(S)\n            W = torch.linalg.solve_triangular(L, X, upper=False)\n            ctx.save_for_backward(X, W, L)\n            return W\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            X, W, L = ctx.saved_tensors\n            gB = torch.linalg.solve_triangular(L.mT, grad_output, upper=True)\n            gA = (-gB @ W.mT).tril()\n            gS = (L.mT @ gA).tril()\n            gS = gS + gS.tril(-1).mT\n            gS = torch.linalg.solve_triangular(L.mT, gS, upper=True)\n            gS = torch.linalg.solve_triangular(L, gS, upper=False, left=False)\n            gX = gS @ X + gB\n            return gX\n\n    def forward(self, w):\n        \"\"\"\n        Apply Cholesky-based orthogonalization to the weight matrix.\n\n        This method constructs a symmetric positive definite matrix from the input weight\n        matrix, performs Cholesky decomposition, and solves the triangular system to produce\n        an orthogonal matrix. It mimics the results of the Gram-Schmidt process but with\n        improved numerical stability.\n\n        Args:\n            w (torch.Tensor): The weight matrix to be orthogonalized.\n\n        Returns:\n            torch.Tensor: The orthogonalized weight matrix.\n        \"\"\"\n        return self.orth(w).view(*self.weight_shape)\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedCholeskyOrthogonalization.__init__","title":"<code>__init__(weight_shape, stable=False)</code>","text":"<p>Initialize the BatchedCholeskyOrthogonalization module.</p> <p>This module orthogonalizes a weight matrix using the Cholesky decomposition method. It first computes the positive definite matrix \\( V V^T \\), then performs a Cholesky decomposition to obtain a lower triangular matrix. Solving the resulting triangular system yields an orthogonal matrix. This method is efficient and numerically stable, making it suitable for a wide range of applications.</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <code>tuple</code> <p>The shape of the weight matrix.</p> required <code>stable</code> <code>bool</code> <p>Whether to use the stable version of the Cholesky-based orthogonalization function, which adds a small positive diagonal element to ensure numerical stability. Default is False.</p> <code>False</code> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape, stable=False):\n    \"\"\"\n    Initialize the BatchedCholeskyOrthogonalization module.\n\n    This module orthogonalizes a weight matrix using the Cholesky decomposition method.\n    It first computes the positive definite matrix \\( V V^T \\), then performs a Cholesky\n    decomposition to obtain a lower triangular matrix. Solving the resulting triangular\n    system yields an orthogonal matrix. This method is efficient and numerically stable,\n    making it suitable for a wide range of applications.\n\n    Args:\n        weight_shape (tuple): The shape of the weight matrix.\n        stable (bool): Whether to use the stable version of the Cholesky-based orthogonalization\n            function, which adds a small positive diagonal element to ensure numerical stability.\n            Default is False.\n    \"\"\"\n    self.weight_shape = weight_shape\n    super(BatchedCholeskyOrthogonalization, self).__init__()\n    if stable:\n        self.orth = BatchedCholeskyOrthogonalization.CholeskyOrthfn_stable.apply\n    else:\n        self.orth = BatchedCholeskyOrthogonalization.CholeskyOrthfn.apply\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedCholeskyOrthogonalization.forward","title":"<code>forward(w)</code>","text":"<p>Apply Cholesky-based orthogonalization to the weight matrix.</p> <p>This method constructs a symmetric positive definite matrix from the input weight matrix, performs Cholesky decomposition, and solves the triangular system to produce an orthogonal matrix. It mimics the results of the Gram-Schmidt process but with improved numerical stability.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>Tensor</code> <p>The weight matrix to be orthogonalized.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The orthogonalized weight matrix.</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def forward(self, w):\n    \"\"\"\n    Apply Cholesky-based orthogonalization to the weight matrix.\n\n    This method constructs a symmetric positive definite matrix from the input weight\n    matrix, performs Cholesky decomposition, and solves the triangular system to produce\n    an orthogonal matrix. It mimics the results of the Gram-Schmidt process but with\n    improved numerical stability.\n\n    Args:\n        w (torch.Tensor): The weight matrix to be orthogonalized.\n\n    Returns:\n        torch.Tensor: The orthogonalized weight matrix.\n    \"\"\"\n    return self.orth(w).view(*self.weight_shape)\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedExponentialOrthogonalization","title":"<code>BatchedExponentialOrthogonalization</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedExponentialOrthogonalization(nn.Module):\n    def __init__(self, weight_shape, niters=7):\n        \"\"\"\n        Initialize the BatchedExponentialOrthogonalization module.\n\n        This module orthogonalizes a weight matrix using the exponential map of a skew-symmetric\n        matrix. By converting the matrix into a skew-symmetric form and applying the matrix\n        exponential, it produces an orthogonal matrix. This approach is particularly useful\n        in contexts where smooth transitions between matrices are required.\n\n        Non-square matrices are padded to the largest dimension to ensure that the matrix can\n        be converted to a skew-symmetric matrix. The resulting matrix is cropped to the original\n        dimension.\n\n        Args:\n            weight_shape (tuple): The shape of the weight matrix.\n            niters (int): Number of iterations for the series expansion approximation of the\n                matrix exponential. Default is 7.\n        \"\"\"\n        self.weight_shape = weight_shape\n        self.max_dim = max(weight_shape[-2:])\n        self.niters = niters\n        super(BatchedExponentialOrthogonalization, self).__init__()\n\n    def forward(self, w):\n        # fill w with zero to have a square matrix over the last two dimensions\n        # if ((self.max_dim - w.shape[-1]) != 0) and ((self.max_dim - w.shape[-2]) != 0):\n        w = torch.nn.functional.pad(\n            w, (0, self.max_dim - w.shape[-1], 0, self.max_dim - w.shape[-2])\n        )\n        # makes w skew symmetric\n        w = (w - w.transpose(-1, -2)) / 2\n        acc = w\n        res = torch.eye(acc.shape[-2], acc.shape[-1], device=w.device) + acc\n        for i in range(2, self.niters):\n            acc = torch.einsum(\"...ij,...jk-&gt;...ik\", acc, w) / i\n            res = res + acc\n        # if transpose:\n        #     res = res.transpose(-1, -2)\n        res = res[..., : self.weight_shape[-2], : self.weight_shape[-1]]\n        return res.contiguous()\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedExponentialOrthogonalization.__init__","title":"<code>__init__(weight_shape, niters=7)</code>","text":"<p>Initialize the BatchedExponentialOrthogonalization module.</p> <p>This module orthogonalizes a weight matrix using the exponential map of a skew-symmetric matrix. By converting the matrix into a skew-symmetric form and applying the matrix exponential, it produces an orthogonal matrix. This approach is particularly useful in contexts where smooth transitions between matrices are required.</p> <p>Non-square matrices are padded to the largest dimension to ensure that the matrix can be converted to a skew-symmetric matrix. The resulting matrix is cropped to the original dimension.</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <code>tuple</code> <p>The shape of the weight matrix.</p> required <code>niters</code> <code>int</code> <p>Number of iterations for the series expansion approximation of the matrix exponential. Default is 7.</p> <code>7</code> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape, niters=7):\n    \"\"\"\n    Initialize the BatchedExponentialOrthogonalization module.\n\n    This module orthogonalizes a weight matrix using the exponential map of a skew-symmetric\n    matrix. By converting the matrix into a skew-symmetric form and applying the matrix\n    exponential, it produces an orthogonal matrix. This approach is particularly useful\n    in contexts where smooth transitions between matrices are required.\n\n    Non-square matrices are padded to the largest dimension to ensure that the matrix can\n    be converted to a skew-symmetric matrix. The resulting matrix is cropped to the original\n    dimension.\n\n    Args:\n        weight_shape (tuple): The shape of the weight matrix.\n        niters (int): Number of iterations for the series expansion approximation of the\n            matrix exponential. Default is 7.\n    \"\"\"\n    self.weight_shape = weight_shape\n    self.max_dim = max(weight_shape[-2:])\n    self.niters = niters\n    super(BatchedExponentialOrthogonalization, self).__init__()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedIdentity","title":"<code>BatchedIdentity</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedIdentity(nn.Module):\n    def __init__(self, weight_shape):\n        \"\"\"\n        Class representing a batched identity matrix with a specific weight shape. The\n        matrix is initialized based on the provided shape of the weights. It is a\n        convenient utility for applications where identity-like operations are\n        required in a batched manner.\n\n        Attributes:\n            weight_shape (Tuple[int, int]): A tuple representing the shape of the\n            weight matrix for each batch. (unused)\n\n        Args:\n            weight_shape: A tuple specifying the shape of the individual weight matrix.\n        \"\"\"\n        super(BatchedIdentity, self).__init__()\n\n    def forward(self, w):\n        return w\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedIdentity.__init__","title":"<code>__init__(weight_shape)</code>","text":"<p>Class representing a batched identity matrix with a specific weight shape. The matrix is initialized based on the provided shape of the weights. It is a convenient utility for applications where identity-like operations are required in a batched manner.</p> <p>Attributes:</p> Name Type Description <code>weight_shape</code> <code>Tuple[int, int]</code> <p>A tuple representing the shape of the</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <p>A tuple specifying the shape of the individual weight matrix.</p> required Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape):\n    \"\"\"\n    Class representing a batched identity matrix with a specific weight shape. The\n    matrix is initialized based on the provided shape of the weights. It is a\n    convenient utility for applications where identity-like operations are\n    required in a batched manner.\n\n    Attributes:\n        weight_shape (Tuple[int, int]): A tuple representing the shape of the\n        weight matrix for each batch. (unused)\n\n    Args:\n        weight_shape: A tuple specifying the shape of the individual weight matrix.\n    \"\"\"\n    super(BatchedIdentity, self).__init__()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedPowerIteration","title":"<code>BatchedPowerIteration</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedPowerIteration(nn.Module):\n    def __init__(self, weight_shape, power_it_niter=3, eps=1e-12):\n        \"\"\"\n        BatchedPowerIteration is a class that performs spectral normalization on weights\n        using the power iteration method in a batched manner. It initializes singular\n        vectors 'u' and 'v', which are used to approximate the largest singular value\n        of the associated weight matrix during training. The L2 normalization is applied\n        to stabilize these singular vector parameters.\n\n        Attributes:\n            weight_shape: tuple\n                Shape of the weight tensor. Normalization is applied to the last two dimensions.\n            power_it_niter: int\n                Number of iterations to perform for the power iteration method.\n            eps: float\n                A small constant to ensure numerical stability during calculations. Used in the power iteration\n                method to avoid dividing by zero.\n        \"\"\"\n        super(BatchedPowerIteration, self).__init__()\n        self.weight_shape = weight_shape\n        self.power_it_niter = power_it_niter\n        self.eps = eps\n        # init u\n        # u will be weight_shape[:-2] + (weight_shape[:-2], 1)\n        # v will be weight_shape[:-2] + (weight_shape[:-1], 1,)\n        self.u = nn.Parameter(\n            torch.Tensor(torch.randn(*weight_shape[:-2], weight_shape[-2], 1)),\n            requires_grad=False,\n        )\n        self.v = nn.Parameter(\n            torch.Tensor(torch.randn(*weight_shape[:-2], weight_shape[-1], 1)),\n            requires_grad=False,\n        )\n        parametrize.register_parametrization(\n            self, \"u\", L2Normalize(dtype=self.u.dtype, dim=(-2))\n        )\n        parametrize.register_parametrization(\n            self, \"v\", L2Normalize(dtype=self.v.dtype, dim=(-2))\n        )\n\n    def forward(self, X, init_u=None, n_iters=3, return_uv=True):\n        for _ in range(n_iters):\n            self.v = X.transpose(-1, -2) @ self.u\n            self.u = X @ self.v\n        # stop gradient on u and v\n        u = self.u.detach()\n        v = self.v.detach()\n        # but keep gradient on s\n        s = u.transpose(-1, -2) @ X @ v\n        return X / (s + self.eps)\n\n    def right_inverse(self, normalized_kernel):\n        # we assume that the kernel is normalized\n        return normalized_kernel.to(self.u.dtype)\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedPowerIteration.__init__","title":"<code>__init__(weight_shape, power_it_niter=3, eps=1e-12)</code>","text":"<p>BatchedPowerIteration is a class that performs spectral normalization on weights using the power iteration method in a batched manner. It initializes singular vectors 'u' and 'v', which are used to approximate the largest singular value of the associated weight matrix during training. The L2 normalization is applied to stabilize these singular vector parameters.</p> <p>Attributes:</p> Name Type Description <code>weight_shape</code> <p>tuple Shape of the weight tensor. Normalization is applied to the last two dimensions.</p> <code>power_it_niter</code> <p>int Number of iterations to perform for the power iteration method.</p> <code>eps</code> <p>float A small constant to ensure numerical stability during calculations. Used in the power iteration method to avoid dividing by zero.</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape, power_it_niter=3, eps=1e-12):\n    \"\"\"\n    BatchedPowerIteration is a class that performs spectral normalization on weights\n    using the power iteration method in a batched manner. It initializes singular\n    vectors 'u' and 'v', which are used to approximate the largest singular value\n    of the associated weight matrix during training. The L2 normalization is applied\n    to stabilize these singular vector parameters.\n\n    Attributes:\n        weight_shape: tuple\n            Shape of the weight tensor. Normalization is applied to the last two dimensions.\n        power_it_niter: int\n            Number of iterations to perform for the power iteration method.\n        eps: float\n            A small constant to ensure numerical stability during calculations. Used in the power iteration\n            method to avoid dividing by zero.\n    \"\"\"\n    super(BatchedPowerIteration, self).__init__()\n    self.weight_shape = weight_shape\n    self.power_it_niter = power_it_niter\n    self.eps = eps\n    # init u\n    # u will be weight_shape[:-2] + (weight_shape[:-2], 1)\n    # v will be weight_shape[:-2] + (weight_shape[:-1], 1,)\n    self.u = nn.Parameter(\n        torch.Tensor(torch.randn(*weight_shape[:-2], weight_shape[-2], 1)),\n        requires_grad=False,\n    )\n    self.v = nn.Parameter(\n        torch.Tensor(torch.randn(*weight_shape[:-2], weight_shape[-1], 1)),\n        requires_grad=False,\n    )\n    parametrize.register_parametrization(\n        self, \"u\", L2Normalize(dtype=self.u.dtype, dim=(-2))\n    )\n    parametrize.register_parametrization(\n        self, \"v\", L2Normalize(dtype=self.v.dtype, dim=(-2))\n    )\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedQROrthogonalization","title":"<code>BatchedQROrthogonalization</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedQROrthogonalization(nn.Module):\n    def __init__(self, weight_shape):\n        \"\"\"\n        Initialize the BatchedQROrthogonalization module.\n\n        This module uses QR decomposition to orthogonalize a weight matrix in a batched manner.\n        It computes the orthogonal component (`Q`) from the decomposition, ensuring that the\n        output satisfies orthogonality constraints.\n\n        Args:\n            weight_shape (tuple): The shape of the weight matrix to be orthogonalized.\n        \"\"\"\n        super(BatchedQROrthogonalization, self).__init__()\n\n    def forward(self, w):\n        \"\"\"\n        Perform QR decomposition to compute the orthogonalized weight matrix.\n\n        The QR decomposition splits the input matrix into an orthogonal matrix (`Q`) and\n        an upper triangular matrix (`R`). This module returns the orthogonal component.\n\n        Args:\n            w (torch.Tensor): The weight matrix to be orthogonalized.\n\n        Returns:\n            torch.Tensor: The orthogonalized weight matrix (`Q` from the QR decomposition).\n        \"\"\"\n        transpose = w.shape[-2] &lt; w.shape[-1]\n        if transpose:\n            w = w.transpose(-1, -2)\n        q, r = torch.linalg.qr(w, mode=\"reduced\")\n        # compute the sign of the diagonal of d\n        diag_sign = torch.sign(torch.diagonal(r, dim1=-2, dim2=-1)).unsqueeze(-2)\n        # multiply the sign with the diagonal of r\n        q = q * diag_sign\n        if transpose:\n            q = q.transpose(-1, -2)\n        return q.contiguous()\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedQROrthogonalization.__init__","title":"<code>__init__(weight_shape)</code>","text":"<p>Initialize the BatchedQROrthogonalization module.</p> <p>This module uses QR decomposition to orthogonalize a weight matrix in a batched manner. It computes the orthogonal component (<code>Q</code>) from the decomposition, ensuring that the output satisfies orthogonality constraints.</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <code>tuple</code> <p>The shape of the weight matrix to be orthogonalized.</p> required Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape):\n    \"\"\"\n    Initialize the BatchedQROrthogonalization module.\n\n    This module uses QR decomposition to orthogonalize a weight matrix in a batched manner.\n    It computes the orthogonal component (`Q`) from the decomposition, ensuring that the\n    output satisfies orthogonality constraints.\n\n    Args:\n        weight_shape (tuple): The shape of the weight matrix to be orthogonalized.\n    \"\"\"\n    super(BatchedQROrthogonalization, self).__init__()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedQROrthogonalization.forward","title":"<code>forward(w)</code>","text":"<p>Perform QR decomposition to compute the orthogonalized weight matrix.</p> <p>The QR decomposition splits the input matrix into an orthogonal matrix (<code>Q</code>) and an upper triangular matrix (<code>R</code>). This module returns the orthogonal component.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>Tensor</code> <p>The weight matrix to be orthogonalized.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The orthogonalized weight matrix (<code>Q</code> from the QR decomposition).</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def forward(self, w):\n    \"\"\"\n    Perform QR decomposition to compute the orthogonalized weight matrix.\n\n    The QR decomposition splits the input matrix into an orthogonal matrix (`Q`) and\n    an upper triangular matrix (`R`). This module returns the orthogonal component.\n\n    Args:\n        w (torch.Tensor): The weight matrix to be orthogonalized.\n\n    Returns:\n        torch.Tensor: The orthogonalized weight matrix (`Q` from the QR decomposition).\n    \"\"\"\n    transpose = w.shape[-2] &lt; w.shape[-1]\n    if transpose:\n        w = w.transpose(-1, -2)\n    q, r = torch.linalg.qr(w, mode=\"reduced\")\n    # compute the sign of the diagonal of d\n    diag_sign = torch.sign(torch.diagonal(r, dim1=-2, dim2=-1)).unsqueeze(-2)\n    # multiply the sign with the diagonal of r\n    q = q * diag_sign\n    if transpose:\n        q = q.transpose(-1, -2)\n    return q.contiguous()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.L2Normalize","title":"<code>L2Normalize</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class L2Normalize(nn.Module):\n    def __init__(self, dtype, dim=None):\n        \"\"\"\n        A class that performs L2 normalization for the given input tensor.\n\n        L2 normalization is a process that normalizes the input over a specified\n        dimension such that the sum of squares of the elements along that\n        dimension equals 1. It ensures that the resulting tensor has a unit norm.\n        This operation is widely used in machine learning and deep learning\n        applications to standardize feature representations.\n\n        Attributes:\n            dim (Optional[int]): The specific dimension along which normalization\n                is performed. If None, normalization is done over all dimensions.\n            dtype (Any): The data type of the tensor to be normalized.\n\n        Parameters:\n            dtype: The data type of the tensor to be normalized.\n            dim: An optional integer specifying the dimension along which to\n                normalize. If not provided, the input will be normalized globally\n                across all dimensions.\n        \"\"\"\n        super(L2Normalize, self).__init__()\n        self.dim = dim\n        self.dtype = dtype\n\n    def forward(self, x):\n        return x / (torch.norm(x, dim=self.dim, keepdim=True, dtype=self.dtype) + 1e-8)\n\n    def right_inverse(self, x):\n        return x / (torch.norm(x, dim=self.dim, keepdim=True, dtype=self.dtype) + 1e-8)\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.L2Normalize.__init__","title":"<code>__init__(dtype, dim=None)</code>","text":"<p>A class that performs L2 normalization for the given input tensor.</p> <p>L2 normalization is a process that normalizes the input over a specified dimension such that the sum of squares of the elements along that dimension equals 1. It ensures that the resulting tensor has a unit norm. This operation is widely used in machine learning and deep learning applications to standardize feature representations.</p> <p>Attributes:</p> Name Type Description <code>dim</code> <code>Optional[int]</code> <p>The specific dimension along which normalization is performed. If None, normalization is done over all dimensions.</p> <code>dtype</code> <code>Any</code> <p>The data type of the tensor to be normalized.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <p>The data type of the tensor to be normalized.</p> required <code>dim</code> <p>An optional integer specifying the dimension along which to normalize. If not provided, the input will be normalized globally across all dimensions.</p> <code>None</code> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, dtype, dim=None):\n    \"\"\"\n    A class that performs L2 normalization for the given input tensor.\n\n    L2 normalization is a process that normalizes the input over a specified\n    dimension such that the sum of squares of the elements along that\n    dimension equals 1. It ensures that the resulting tensor has a unit norm.\n    This operation is widely used in machine learning and deep learning\n    applications to standardize feature representations.\n\n    Attributes:\n        dim (Optional[int]): The specific dimension along which normalization\n            is performed. If None, normalization is done over all dimensions.\n        dtype (Any): The data type of the tensor to be normalized.\n\n    Parameters:\n        dtype: The data type of the tensor to be normalized.\n        dim: An optional integer specifying the dimension along which to\n            normalize. If not provided, the input will be normalized globally\n            across all dimensions.\n    \"\"\"\n    super(L2Normalize, self).__init__()\n    self.dim = dim\n    self.dtype = dtype\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.OrthoParams","title":"<code>OrthoParams</code>  <code>dataclass</code>","text":"<p>Represents the parameters and configurations used for orthogonalization and spectral normalization.</p> <p>This class encapsulates the necessary modules and settings required for performing spectral normalization and orthogonalization of tensors in a parameterized way. It accommodates various implementations of normalizers and orthogonalization techniques to provide flexibility in their application. This way we can easily switch between different normalization techniques inside our layer despite that each normalization have different parameters.</p> <p>Attributes:</p> Name Type Description <code>spectral_normalizer</code> <code>Callable[Tuple[int, ...], Module]</code> <p>A callable that produces a module for spectral normalization. Default is configured to use BatchedPowerIteration with specific parameters. This callable can be provided either as a <code>functool.partial</code> or as a <code>orthogonium.ClassParam</code>. It will recieve the shape of the weight tensor as its argument.</p> <code>orthogonalizer</code> <code>Callable[Tuple[int, ...], Module]</code> <p>A callable that produces a module for orthogonalization. Default is configured to use BatchedBjorckOrthogonalization with specific parameters. This callable can be provided either as a <code>functool.partial</code> or as a <code>orthogonium.ClassParam</code>. It will recieve the shape of the weight tensor as its argument.</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>@dataclass\nclass OrthoParams:\n    \"\"\"\n    Represents the parameters and configurations used for orthogonalization\n    and spectral normalization.\n\n    This class encapsulates the necessary modules and settings required\n    for performing spectral normalization and orthogonalization of tensors\n    in a parameterized way. It accommodates various implementations of\n    normalizers and orthogonalization techniques to provide flexibility\n    in their application. This way we can easily switch between different\n    normalization techniques inside our layer despite that each normalization\n    have different parameters.\n\n    Attributes:\n        spectral_normalizer (Callable[Tuple[int, ...], nn.Module]): A callable\n            that produces a module for spectral normalization. Default is\n            configured to use BatchedPowerIteration with specific parameters.\n            This callable can be provided either as a `functool.partial` or as a\n            `orthogonium.ClassParam`. It will recieve the shape of the weight tensor as its\n            argument.\n        orthogonalizer (Callable[Tuple[int, ...], nn.Module]): A callable\n            that produces a module for orthogonalization. Default is\n            configured to use BatchedBjorckOrthogonalization with specific\n            parameters. This callable can be provided either as a `functool.partial` or as a\n            `orthogonium.ClassParam`. It will recieve the shape of the weight tensor as its argument.\n    \"\"\"\n\n    # spectral_normalizer: Callable[Tuple[int, ...], nn.Module] = BatchedIdentity\n    spectral_normalizer: Callable[Tuple[int, ...], nn.Module] = ClassParam(  # type: ignore\n        BatchedPowerIteration, power_it_niter=3, eps=1e-6\n    )\n    orthogonalizer: Callable[Tuple[int, ...], nn.Module] = ClassParam(  # type: ignore\n        BatchedBjorckOrthogonalization,\n        beta=0.5,\n        niters=12,\n        pass_through=False,\n        # ClassParam(BatchedExponentialOrthogonalization, niters=12)\n        # BatchedCholeskyOrthogonalization,\n        # BatchedQROrthogonalization,\n    )\n</code></pre>"}]}