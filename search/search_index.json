{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#orthogonium-improved-implementations-of-orthogonal-layers","title":"\u2728 Orthogonium: Improved implementations of orthogonal layers","text":"<p>This library aims to centralize, standardize and improve methods to  build orthogonal layers, with a focus on convolutional layers . We noticed that a layer's implementation play a significant role in the final performance : a more efficient implementation  allows larger networks and more training steps within the same compute  budget. So our implementation differs from original papers in order to  be faster, to consume less memory or be more flexible.</p>"},{"location":"#what-is-included-in-this-library","title":"\ud83d\udcc3 What is included in this library ?","text":"Layer name Description Orthogonal ? Usage Status AOC (Adaptive-BCOP) The most scalable method to build orthogonal convolution. Allows control of kernel size, stride, groups dilation and convtranspose Orthogonal A flexible method for complex architectures. Preserve orthogonality and works on large scale images. done Adaptive-SC-Fac Same as previous layer but based on SC-Fac instead of BCOP, which claims a complete parametrization of separable convolutions Orthogonal Same as above pending Adaptive-SOC SOC modified to be: i) faster and memory efficient ii) handle stride, groups, dilation &amp; convtranspose Orthogonal Good for depthwise convolutions and cases where control over the kernel size is not required in progress SLL The original SLL layer, which is already quite efficient. 1-Lipschitz Well suited for residual blocks, it also contains ReLU activations. done SLL-AOC SLL-AOC is to the downsampling block what SLL is to the residual block (see ResNet paper) 1-Lipschitz Allows to construct a \"strided\" residual block than can change the number of channels. It adds a convolution in the residual path. done Sandwish-AOC Sandwish convolutions that uses AOC to replace the FFT. Allowing it to scale to large images. 1-Lipschitz pending Adaptive-ECO ECO modified to i) handle stride, groups &amp; convtranspose Orthogonal (low priority)"},{"location":"#directory-structure","title":"directory structure","text":"<pre><code>orthogonium\n\u251c\u2500\u2500 layers\n\u2502   \u251c\u2500\u2500 conv\n\u2502   \u2502   \u251c\u2500\u2500 AOC\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ortho_conv.py # contains AdaptiveOrthoConv2d layer\n\u2502   \u2502   \u251c\u2500\u2500 AdaptiveSOC\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 ortho_conv.py # contains AdaptiveSOCConv2d layer (untested)\n\u2502   \u2502   \u251c\u2500\u2500 SLL\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 sll_layer.py # contains SDPBasedLipschitzConv, SDPBasedLipschitzDense, SLLxAOCLipschitzResBlock\n\u2502   \u251c\u2500\u2500 legacy\n\u2502   \u2502   \u251c\u2500\u2500 original code of BCOP, SOC, Cayley etc.\n\u2502   \u251c\u2500\u2500 linear\n\u2502   \u2502   \u251c\u2500\u2500 ortho_linear.py # contains OrthoLinear layer (can be used with BB, QR and Exp parametrization)\n\u2502   \u251c\u2500\u2500 normalization.py # contains Batch centering and Layer centering\n\u2502   \u251c\u2500\u2500 custom_activations.py # contains custom activations for 1 lipschitz networks\n\u2502   \u251c\u2500\u2500 channel_shuffle.py # contains channel shuffle layer  \n\u251c\u2500\u2500 model_factory.py # factory function to construct various models for the zoo\n\u251c\u2500\u2500 losses # loss functions, VRA estimation\n</code></pre>"},{"location":"#aoc","title":"AOC:","text":"<p>AOC is a method that allows to build orthogonal convolutions with  an explicit kernel, that support all features like stride, conv transposed, grouped convolutions and dilation (and all compositions of these parameters). This approach is highly scalable, and can be applied to problems like Imagenet-1K.</p>"},{"location":"#adaptive-sc-fac","title":"Adaptive-SC-FAC:","text":"<p>As AOC is built on top of BCOP method, we can construct an equivalent method constructed on top of SC-Fac instead. This will allow to compare performance of the two methods given that they have very similar parametrization. (See our  paper for discussions about the similarities and differences between the two methods).</p>"},{"location":"#adaptive-soc","title":"Adaptive-SOC:","text":"<p>Adaptive-SOC blend the approach of AOC and SOC. It differs from SOC in the way that it is more memory efficient and  sometimes faster. It also allows to handle stride, groups, dilation and transposed convolutions. However, it does not allow to  control the kernel size explicitly as the resulting kernel size is larger than the requested kernel size.  It is due to the computation to the exponential of a kernel that increases the kernel size at each iteration.</p> <p>Its development is still in progress, so extra testing is still require to ensure exact orthogonality.</p>"},{"location":"#sll","title":"SLL:","text":"<p>SLL is a method that allows to construct small residual blocks with ReLU activations. We kept most to the original  implementation, and added <code>SLLxAOCLipschitzResBlock</code> that construct a down-sampling residual block by fusing SLL with  $AOC.</p>"},{"location":"#more-layers-are-coming-soon","title":"more layers are coming soon !","text":""},{"location":"#install-the-library","title":"\ud83c\udfe0 Install the library:","text":"<p>The library is available on pip,so you can install it by running the following command: <pre><code>pip install orthogonium\n</code></pre></p> <p>If you wish to deep dive in the code and edit your local versin, you can clone the repository and run the following command  to install it locally: <pre><code>git clone git@github.com:deel-ai/orthogonium.git\npip install -e .\n</code></pre></p>"},{"location":"#use-the-layer","title":"Use the layer:","text":"<pre><code>from orthogonium.layers.conv.AOC import AdaptiveOrthoConv2d, AdaptiveOrthoConvTranspose2d\nfrom orthogonium.reparametrizers import DEFAULT_ORTHO_PARAMS\n\n# use OrthoConv2d with the same params as torch.nn.Conv2d\nkernel_size = 3\nconv = AdaptiveOrthoConv2d(\n  kernel_size=kernel_size,\n  in_channels=256,\n  out_channels=256,\n  stride=2,\n  groups=16,\n    dilation=2,\n  padding_mode=\"circular\",\n    ortho_params=DEFAULT_ORTHO_PARAMS,\n)\n# conv.weight can be assigned to a torch.nn.Conv2d \n\n# this works similarly for ConvTranspose2d:\nconv_transpose = AdaptiveOrthoConvTranspose2d(\n    in_channels=256,\n    out_channels=256,\n    kernel_size=kernel_size,\n    stride=2,\n    dilation=2,\n    groups=16,\n    ortho_params=DEFAULT_ORTHO_PARAMS,\n)\n</code></pre>"},{"location":"#model-zoo","title":"\ud83d\udc2f Model Zoo","text":"<p>Stay tuned, a model zoo will be available soon !</p>"},{"location":"#disclaimer","title":"\ud83d\udca5Disclaimer","text":"<p>Given the great quality of the original implementations, orthogonium do not focus on reproducing exactly the results of the original papers, but rather on providing a more efficient implementation. Some degradations in the final provable  accuracy may be observed when reproducing the results of the original papers, we consider this acceptable is the gain  in terms of scalability is worth it. This library aims to provide more scalable and versatile implementations for people who seek to use orthogonal layers  in a larger scale setting.</p>"},{"location":"#ressources","title":"\ud83d\udd2d Ressources","text":""},{"location":"#1-lipschitz-cnns-and-orthogonal-cnns","title":"1 Lipschitz CNNs and orthogonal CNNs","text":"<ul> <li>1-Lipschitz Layers Compared: github and paper</li> <li>BCOP: github and paper</li> <li>SC-Fac: paper</li> <li>ECO: paper</li> <li>Cayley: github and paper</li> <li>LOT: github and paper</li> <li>ProjUNN-T: github and paper</li> <li>SLL: github and paper</li> <li>Sandwish: github and paper</li> <li>AOL: github and paper</li> <li>SOC: github and paper 1, paper 2</li> </ul>"},{"location":"#lipschitz-constant-evaluation","title":"Lipschitz constant evaluation","text":"<ul> <li>Spectral Norm of Convolutional Layers with Circular and Zero Paddings </li> <li>Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration</li> <li>github of the two papers</li> </ul>"},{"location":"#citations","title":"\ud83d\udcd6 Citations","text":"<p>If you find this repository useful for your research, please cite:</p> <pre><code>@misc{boissin2025adaptiveorthogonalconvolutionscheme,\n      title={An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures}, \n      author={Thibaut Boissin and Franck Mamalet and Thomas Fel and Agustin Martin Picard and Thomas Massena and Mathieu Serrurier},\n      year={2025},\n      eprint={2501.07930},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2501.07930}, \n}\n</code></pre>"},{"location":"#contributing","title":"\ud83c\udf7b Contributing","text":"<p>This library is still in a very early stage, so expect some bugs and missing features. Also, before the version 1.0.0, the API may change and no backward compatibility will be ensured (code is expected to keep working under minor changes but the loading of parametrized network could fail). This will allow a rapid integration of new features, if you project to release a trained architecture, exporting the convolutions to torch.nn.conv2D is advised (by saving the <code>weight</code>  attribute of a layer). If you plan to release a training script, fix the version in your requirements. In order to prioritize the development, we will focus on the most used layers and models. If you have a specific need, please open an issue, and we will try to address it as soon as possible.</p> <p>Also, if you have a model that you would like to share, please open a PR with the model and the training script. We will be happy to include it in the zoo.</p> <p>If you want to contribute, please open a PR with the new feature or bug fix. We will review it as soon as possible.</p>"},{"location":"#ongoing-developments","title":"Ongoing developments","text":"<p>Layers: - SOC:   - remove channels padding to handle ci != co efficiently   - enable groups   - enable support for native stride, transposition and dilation - AOL:   - torch implementation of AOL - Sandwish:   - import code   - plug AOC into Sandwish conv</p> <p>ZOO: - models from the paper</p>"},{"location":"CONTRIBUTING/","title":"Contributing","text":"<p>Thanks for taking the time to contribute!</p> <p>From opening a bug report to creating a pull request: every contribution is appreciated and welcome. If you're planning to implement a new feature or change the api please create an issue first. This way we can ensure that your precious work is not in vain.</p>"},{"location":"CONTRIBUTING/#setup-with-make","title":"Setup with make","text":"<ul> <li>Clone the repo <code>git clone git@github.com:thib-s/orthogonium.git</code>.</li> <li>Go to your freshly downloaded repo <code>cd orthogonium</code></li> <li>Create a virtual environment and install the necessary dependencies for development:</li> </ul> <p><code>make prepare-dev &amp;&amp; source orthogonium_dev_env/bin/activate</code>.</p> <p>Welcome to the team !</p>"},{"location":"CONTRIBUTING/#tests","title":"Tests","text":"<p>To run test <code>make test</code> This command activate your virtual environment and launch the <code>tox</code> command.</p> <p><code>tox</code> on the otherhand will do the following: - run pytest on the tests folder - run pylint on the deel-datasets main files</p> <p>Note: It is possible that pylint throw false-positive errors. If the linting test failed please check first pylint output to point out the reasons.</p> <p>Please, make sure you run all the tests at least once before opening a pull request.</p> <p>A word toward Pylint for those that don't know it:</p> <p>Pylint is a Python static code analysis tool which looks for programming errors, helps enforcing a coding standard, sniffs for code smells and offers simple refactoring suggestions.</p> <p>Basically, it will check that your code follow a certain number of convention. Any Pull Request will go through a Github workflow ensuring that your code respect the Pylint conventions (most of them at least).</p>"},{"location":"CONTRIBUTING/#submitting-changes","title":"Submitting Changes","text":"<p>After getting some feedback, push to your fork and submit a pull request. We may suggest some changes or improvements or alternatives, but for small changes your pull request should be accepted quickly (see Governance policy).</p> <p>Something that will increase the chance that your pull request is accepted:</p> <ul> <li>Write tests and ensure that the existing ones pass.</li> <li>If <code>make test</code> is succesful, you have fair chances to pass the CI workflows (linting and test)</li> <li>Follow the existing coding style and run <code>make check_all</code> to check all files format.</li> <li>Write a good commit message (we follow a lowercase convention).</li> <li>For a major fix/feature make sure your PR has an issue and if it doesn't, please create one. This would help discussion with the community, and polishing ideas in case of a new feature.</li> </ul>"},{"location":"api/activations/","title":"activations","text":""},{"location":"api/activations/#orthogonium.layers.custom_activations.Abs","title":"<code>Abs</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>class Abs(nn.Module):\n    def __init__(self):\n        \"\"\"\n        Initializes an instance of the Abs class.\n\n        This method is automatically called when a new object of the Abs class\n        is instantiated. It calls the initializer of its superclass to ensure\n        proper initialization of inherited class functionality, setting up\n        the required base structures or attributes.\n        \"\"\"\n        super(Abs, self).__init__()\n\n    def forward(self, z):\n        return torch.abs(z)\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.Abs.__init__","title":"<code>__init__()</code>","text":"<p>Initializes an instance of the Abs class.</p> <p>This method is automatically called when a new object of the Abs class is instantiated. It calls the initializer of its superclass to ensure proper initialization of inherited class functionality, setting up the required base structures or attributes.</p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    Initializes an instance of the Abs class.\n\n    This method is automatically called when a new object of the Abs class\n    is instantiated. It calls the initializer of its superclass to ensure\n    proper initialization of inherited class functionality, setting up\n    the required base structures or attributes.\n    \"\"\"\n    super(Abs, self).__init__()\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.HouseHolder","title":"<code>HouseHolder</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>class HouseHolder(nn.Module):\n    def __init__(self, channels, axis=1):\n        \"\"\"\n        A activation that applies a parameterized transformation via Householder\n        reflection technique. It is initialized with the number of input channels, which must\n        be even, and an axis that determines the dimension along which operations are applied.\n        This is a corrected version of the original implementation from Singla et al. (2019),\n        which features a 1/sqrt(2) scaling factor to be 1-Lipschitz.\n\n        Attributes:\n            theta (torch.nn.Parameter): Learnable parameter that determines the transformation\n                applied via Householder reflection.\n            axis (int): Dimension along which the operation is performed.\n\n        Args:\n            channels (int): Total number of input channels. Must be an even number.\n            axis (int): Dimension along which the transformation is applied. Default is 1.\n        \"\"\"\n        super(HouseHolder, self).__init__()\n        assert (channels % 2) == 0\n        eff_channels = channels // 2\n\n        self.theta = nn.Parameter(\n            0.5 * np.pi * torch.ones(1, eff_channels, 1, 1), requires_grad=True\n        )\n        self.axis = axis\n\n    def forward(self, z):\n        theta = self.theta\n        x, y = z.split(z.shape[self.axis] // 2, self.axis)\n\n        selector = (x * torch.sin(0.5 * theta)) - (y * torch.cos(0.5 * theta))\n\n        a_2 = x * torch.cos(theta) + y * torch.sin(theta)\n        b_2 = x * torch.sin(theta) - y * torch.cos(theta)\n\n        a = x * (selector &lt;= 0) + a_2 * (selector &gt; 0)\n        b = y * (selector &lt;= 0) + b_2 * (selector &gt; 0)\n        return torch.cat([a, b], dim=self.axis) / SQRT_2\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.HouseHolder.__init__","title":"<code>__init__(channels, axis=1)</code>","text":"<p>A activation that applies a parameterized transformation via Householder reflection technique. It is initialized with the number of input channels, which must be even, and an axis that determines the dimension along which operations are applied. This is a corrected version of the original implementation from Singla et al. (2019), which features a 1/sqrt(2) scaling factor to be 1-Lipschitz.</p> <p>Attributes:</p> Name Type Description <code>theta</code> <code>Parameter</code> <p>Learnable parameter that determines the transformation applied via Householder reflection.</p> <code>axis</code> <code>int</code> <p>Dimension along which the operation is performed.</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>Total number of input channels. Must be an even number.</p> required <code>axis</code> <code>int</code> <p>Dimension along which the transformation is applied. Default is 1.</p> <code>1</code> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>def __init__(self, channels, axis=1):\n    \"\"\"\n    A activation that applies a parameterized transformation via Householder\n    reflection technique. It is initialized with the number of input channels, which must\n    be even, and an axis that determines the dimension along which operations are applied.\n    This is a corrected version of the original implementation from Singla et al. (2019),\n    which features a 1/sqrt(2) scaling factor to be 1-Lipschitz.\n\n    Attributes:\n        theta (torch.nn.Parameter): Learnable parameter that determines the transformation\n            applied via Householder reflection.\n        axis (int): Dimension along which the operation is performed.\n\n    Args:\n        channels (int): Total number of input channels. Must be an even number.\n        axis (int): Dimension along which the transformation is applied. Default is 1.\n    \"\"\"\n    super(HouseHolder, self).__init__()\n    assert (channels % 2) == 0\n    eff_channels = channels // 2\n\n    self.theta = nn.Parameter(\n        0.5 * np.pi * torch.ones(1, eff_channels, 1, 1), requires_grad=True\n    )\n    self.axis = axis\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.HouseHolder_Order_2","title":"<code>HouseHolder_Order_2</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>class HouseHolder_Order_2(nn.Module):\n    def __init__(self, channels, axis=1):\n        \"\"\"\n        Represents a layer or module that performs operations using Householder\n        transformations of order 2, parameterized by angles corresponding to\n        each group of channels. This is a corrected version of the original\n        implementation from Singla et al. (2019), which features a 1/sqrt(2)\n        scaling factor to be 1-Lipschitz.\n\n        Attributes:\n            num_groups (int): The number of groups, which is half the number\n            of channels provided as input.\n\n            axis (int): The axis along which the computation is performed.\n\n            theta0 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n            representing the first set of angles (in radians) used in the\n            parameterization.\n\n            theta1 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n            representing the second set of angles (in radians) used in the\n            parameterization.\n\n            theta2 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n            representing the third set of angles (in radians) used in the\n            parameterization.\n\n        Args:\n            channels (int): The total number of input channels. Must be an even\n            number, as it will be split into groups.\n\n            axis (int, optional): Specifies the axis for computations. Defaults\n            to 1.\n        \"\"\"\n        super(HouseHolder_Order_2, self).__init__()\n        assert (channels % 2) == 0\n        self.num_groups = channels // 2\n        self.axis = axis\n\n        self.theta0 = nn.Parameter(\n            (np.pi * torch.rand(self.num_groups)), requires_grad=True\n        )\n        self.theta1 = nn.Parameter(\n            (np.pi * torch.rand(self.num_groups)), requires_grad=True\n        )\n        self.theta2 = nn.Parameter(\n            (np.pi * torch.rand(self.num_groups)), requires_grad=True\n        )\n\n    def forward(self, z):\n        theta0 = torch.clamp(self.theta0.view(1, -1, 1, 1), 0.0, 2 * np.pi)\n\n        x, y = z.split(z.shape[self.axis] // 2, self.axis)\n        z_theta = (torch.atan2(y, x) - (0.5 * theta0)) % (2 * np.pi)\n\n        theta1 = torch.clamp(self.theta1.view(1, -1, 1, 1), 0.0, 2 * np.pi)\n        theta2 = torch.clamp(self.theta2.view(1, -1, 1, 1), 0.0, 2 * np.pi)\n        theta3 = 2 * np.pi - theta1\n        theta4 = 2 * np.pi - theta2\n\n        ang1 = 0.5 * (theta1)\n        ang2 = 0.5 * (theta1 + theta2)\n        ang3 = 0.5 * (theta1 + theta2 + theta3)\n        ang4 = 0.5 * (theta1 + theta2 + theta3 + theta4)\n\n        select1 = torch.logical_and(z_theta &gt;= 0, z_theta &lt; ang1)\n        select2 = torch.logical_and(z_theta &gt;= ang1, z_theta &lt; ang2)\n        select3 = torch.logical_and(z_theta &gt;= ang2, z_theta &lt; ang3)\n        select4 = torch.logical_and(z_theta &gt;= ang3, z_theta &lt; ang4)\n\n        a1 = x\n        b1 = y\n\n        a2 = x * torch.cos(theta0 + theta1) + y * torch.sin(theta0 + theta1)\n        b2 = x * torch.sin(theta0 + theta1) - y * torch.cos(theta0 + theta1)\n\n        a3 = x * torch.cos(theta2) + y * torch.sin(theta2)\n        b3 = -x * torch.sin(theta2) + y * torch.cos(theta2)\n\n        a4 = x * torch.cos(theta0) + y * torch.sin(theta0)\n        b4 = x * torch.sin(theta0) - y * torch.cos(theta0)\n\n        a = (a1 * select1) + (a2 * select2) + (a3 * select3) + (a4 * select4)\n        b = (b1 * select1) + (b2 * select2) + (b3 * select3) + (b4 * select4)\n\n        z = torch.cat([a, b], dim=self.axis) / SQRT_2\n        return z\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.HouseHolder_Order_2.__init__","title":"<code>__init__(channels, axis=1)</code>","text":"<p>Represents a layer or module that performs operations using Householder transformations of order 2, parameterized by angles corresponding to each group of channels. This is a corrected version of the original implementation from Singla et al. (2019), which features a 1/sqrt(2) scaling factor to be 1-Lipschitz.</p> <p>Attributes:</p> Name Type Description <code>num_groups</code> <code>int</code> <p>The number of groups, which is half the number</p> <code>axis</code> <code>int</code> <p>The axis along which the computation is performed.</p> <code>theta0</code> <code>Parameter</code> <p>A tensor parameter of shape <code>(num_groups,)</code></p> <code>theta1</code> <code>Parameter</code> <p>A tensor parameter of shape <code>(num_groups,)</code></p> <code>theta2</code> <code>Parameter</code> <p>A tensor parameter of shape <code>(num_groups,)</code></p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>The total number of input channels. Must be an even</p> required <code>axis</code> <code>int</code> <p>Specifies the axis for computations. Defaults</p> <code>1</code> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>def __init__(self, channels, axis=1):\n    \"\"\"\n    Represents a layer or module that performs operations using Householder\n    transformations of order 2, parameterized by angles corresponding to\n    each group of channels. This is a corrected version of the original\n    implementation from Singla et al. (2019), which features a 1/sqrt(2)\n    scaling factor to be 1-Lipschitz.\n\n    Attributes:\n        num_groups (int): The number of groups, which is half the number\n        of channels provided as input.\n\n        axis (int): The axis along which the computation is performed.\n\n        theta0 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n        representing the first set of angles (in radians) used in the\n        parameterization.\n\n        theta1 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n        representing the second set of angles (in radians) used in the\n        parameterization.\n\n        theta2 (torch.nn.Parameter): A tensor parameter of shape `(num_groups,)`\n        representing the third set of angles (in radians) used in the\n        parameterization.\n\n    Args:\n        channels (int): The total number of input channels. Must be an even\n        number, as it will be split into groups.\n\n        axis (int, optional): Specifies the axis for computations. Defaults\n        to 1.\n    \"\"\"\n    super(HouseHolder_Order_2, self).__init__()\n    assert (channels % 2) == 0\n    self.num_groups = channels // 2\n    self.axis = axis\n\n    self.theta0 = nn.Parameter(\n        (np.pi * torch.rand(self.num_groups)), requires_grad=True\n    )\n    self.theta1 = nn.Parameter(\n        (np.pi * torch.rand(self.num_groups)), requires_grad=True\n    )\n    self.theta2 = nn.Parameter(\n        (np.pi * torch.rand(self.num_groups)), requires_grad=True\n    )\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.MaxMin","title":"<code>MaxMin</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>class MaxMin(nn.Module):\n    def __init__(self, axis=1):\n        \"\"\"\n        This class implements the MaxMin activation function. Which is a\n        pairwise activation function that returns the maximum and minimum (ordered)\n        of each pair of elements in the input tensor.\n\n        Parameters\n            axis : int, default=1 the axis along which to apply the activation function.\n\n        \"\"\"\n        self.axis = axis\n        super(MaxMin, self).__init__()\n\n    def forward(self, z):\n        a, b = z.split(z.shape[self.axis] // 2, self.axis)\n        c, d = torch.max(a, b), torch.min(a, b)\n        return torch.cat([c, d], dim=self.axis)\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.MaxMin.__init__","title":"<code>__init__(axis=1)</code>","text":"<p>This class implements the MaxMin activation function. Which is a pairwise activation function that returns the maximum and minimum (ordered) of each pair of elements in the input tensor.</p> <p>Parameters     axis : int, default=1 the axis along which to apply the activation function.</p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>def __init__(self, axis=1):\n    \"\"\"\n    This class implements the MaxMin activation function. Which is a\n    pairwise activation function that returns the maximum and minimum (ordered)\n    of each pair of elements in the input tensor.\n\n    Parameters\n        axis : int, default=1 the axis along which to apply the activation function.\n\n    \"\"\"\n    self.axis = axis\n    super(MaxMin, self).__init__()\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.SoftHuber","title":"<code>SoftHuber</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>class SoftHuber(nn.Module):\n    def __init__(self, delta=0.05):\n        \"\"\"\n        Initializes the SoftHuber class.\n        This class implements the Soft Huber loss function, which is a\n        differentiable approximation of the Huber loss. The Soft Huber loss\n        behaves like abs(x) when the absolute error is large and like x**2\n        when the absolute error is small. The transition between these two\n        behaviors is controlled by the delta parameter.\n\n        Parameters:\n            delta (float): The threshold at which to switch between L1 and L2 loss.\n        \"\"\"\n        super(SoftHuber, self).__init__()\n        self.delta = delta\n\n    def forward(self, z):\n        # we dont multiply by delta**2 in order to have a Lipschitz constant of 1\n        return self.delta * (torch.sqrt(1 + (z / self.delta) ** 2) - 1)\n</code></pre>"},{"location":"api/activations/#orthogonium.layers.custom_activations.SoftHuber.__init__","title":"<code>__init__(delta=0.05)</code>","text":"<p>Initializes the SoftHuber class. This class implements the Soft Huber loss function, which is a differentiable approximation of the Huber loss. The Soft Huber loss behaves like abs(x) when the absolute error is large and like x**2 when the absolute error is small. The transition between these two behaviors is controlled by the delta parameter.</p> <p>Parameters:</p> Name Type Description Default <code>delta</code> <code>float</code> <p>The threshold at which to switch between L1 and L2 loss.</p> <code>0.05</code> Source code in <code>orthogonium\\layers\\custom_activations.py</code> <pre><code>def __init__(self, delta=0.05):\n    \"\"\"\n    Initializes the SoftHuber class.\n    This class implements the Soft Huber loss function, which is a\n    differentiable approximation of the Huber loss. The Soft Huber loss\n    behaves like abs(x) when the absolute error is large and like x**2\n    when the absolute error is small. The transition between these two\n    behaviors is controlled by the delta parameter.\n\n    Parameters:\n        delta (float): The threshold at which to switch between L1 and L2 loss.\n    \"\"\"\n    super(SoftHuber, self).__init__()\n    self.delta = delta\n</code></pre>"},{"location":"api/aoc/","title":"Aoc","text":"<p>The most scalable method to build orthogonal convolution. Allows control of kernel size,  stride, groups dilation and transposed convolutions.</p> <p>The classes <code>AdaptiveOrthoConv2d</code> and <code>AdaptiveOrthoConv2d</code> are not classes,  but factory function to selecte bewteen 3 different parametrizations, as depicted in the following figure:</p> <p></p>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d","title":"<code>AdaptiveOrthoConv2d(in_channels, out_channels, kernel_size, stride=1, padding='same', dilation=1, groups=1, bias=True, padding_mode='circular', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel size and stride. This is the implementation for the <code>Adaptive Orthogonal Convolution</code> scheme [1]. It aims to be scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers. This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</p>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features","title":"Key Features:","text":"<pre><code>- Enforces orthogonality, preserving gradient norms.\n- Supports native striding, dilation, grouped convolutions, and flexible padding.\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RKOConv2d`.\n- When stride == 1, the layer is a `FastBlockConv2d`.\n- Otherwise, the layer is a `BcopRkoConv2d`.\n</code></pre> Note <ul> <li>This implementation also work under zero padding, it lipschitz constant is still tight, but it looses     orthogonality.orthogonality on the image border.</li> <li>the unit tesing validated for a tolerance of 1e-4 under various orthogonalization schemes (see     reparametrizers). Only Cholesky based methods were validated for a lower tolerance of 5e-2.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>str or _size_2_t</code> <p>Padding mode or size. Default is \"same\".</p> <code>'same'</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input to output channels. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"circular\".</p> <code>'circular'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>Conv2d</code> <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p> <p>Raises:</p> Type Description <code>`ValueError`</code> <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> References <ul> <li>[1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025). An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures. https://arxiv.org/abs/2501.07930</li> </ul> Source code in <code>orthogonium\\layers\\conv\\AOC\\ortho_conv.py</code> <pre><code>def AdaptiveOrthoConv2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: Union[str, _size_2_t] = \"same\",\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"circular\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.Conv2d:\n    \"\"\"\n    Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel\n    size and stride. This is the implementation for the `Adaptive Orthogonal Convolution` scheme [1]. It aims to be\n    scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers.\n    This layer also intend to be compatible with all the feature of the `nn.Conv2d` class (e.g., striding, dilation,\n    grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a\n    standard convolutional layer, but the weight are constrained to be orthogonal.\n\n    Key Features:\n    -------------\n        - Enforces orthogonality, preserving gradient norms.\n        - Supports native striding, dilation, grouped convolutions, and flexible padding.\n\n    Behavior:\n    ---------\n        - When kernel_size == stride, the layer is an `RKOConv2d`.\n        - When stride == 1, the layer is a `FastBlockConv2d`.\n        - Otherwise, the layer is a `BcopRkoConv2d`.\n\n    Note:\n        - This implementation also work under zero padding, it lipschitz constant is still tight, but it looses\n            orthogonality.orthogonality on the image border.\n        - the unit tesing validated for a tolerance of 1e-4 under various orthogonalization schemes (see\n            reparametrizers). Only Cholesky based methods were validated for a lower tolerance of 5e-2.\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the convolution. Default is 1.\n        padding (str or _size_2_t, optional): Padding mode or size. Default is \"same\".\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        padding_mode (str, optional): Padding mode. Default is \"circular\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).\n\n    Raises:\n        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n\n\n    References:\n        - [1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n        &lt;https://arxiv.org/abs/2501.07930&gt;\n    \"\"\"\n\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RKOConv2d\n    elif (stride == 1) or ((in_channels &gt;= out_channels) and (dilation &gt; 1)):\n        convclass = FastBlockConv2d\n    else:\n        convclass = BcopRkoConv2d\n    return convclass(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=groups,\n        bias=bias,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d","title":"<code>AdaptiveOrthoConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on kernel size and stride. This is the implementation for the <code>Adaptive Orthogonal Convolution</code> scheme [1]. It aims to be scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers. This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</p>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features","title":"Key Features:","text":"<pre><code>- Ensures orthogonality in transpose convolutions for stable gradient propagation.\n- Supports dilation, grouped operations, and efficient kernel construction.\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RkoConvTranspose2d`.\n- When stride == 1, the layer is a `FastBlockConvTranspose2D`.\n- Otherwise, the layer is a `BcopRkoConvTranspose2d`.\n</code></pre> Note <ul> <li>This implementation also work under zero padding, it lipschitz constant is still tight, but it looses     orthogonality.orthogonality on the image border.</li> <li>The current implementation of the torch.nn.ConvTranspose2d does not support circular padding. One can     implement padding manually by add a padding layer before and setting padding = (0,0).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the transpose convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>_size_2_t</code> <p>Padding size. Default is 0.</p> <code>0</code> <code>output_padding</code> <code>_size_2_t</code> <p>Additional size for output. Default is 0.</p> <code>0</code> <code>groups</code> <code>int</code> <p>Number of groups. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"zeros\".</p> <code>'zeros'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>ConvTranspose2d</code> <p>A configured instance of <code>nn.ConvTranspose2d</code> (one of <code>RkoConvTranspose2d</code>, <code>FastBlockConvTranspose2D</code>, or <code>BcopRkoConvTranspose2d</code>).</p> <p>Raises: - <code>ValueError</code>: If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> References <ul> <li>[1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025). An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures. https://arxiv.org/abs/2501.07930</li> </ul> Source code in <code>orthogonium\\layers\\conv\\AOC\\ortho_conv.py</code> <pre><code>def AdaptiveOrthoConvTranspose2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.ConvTranspose2d:\n    \"\"\"\n    Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on kernel\n    size and stride. This is the implementation for the `Adaptive Orthogonal Convolution` scheme [1]. It aims to be\n    scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers.\n    This layer also intend to be compatible with all the feature of the `nn.Conv2d` class (e.g., striding, dilation,\n    grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a\n    standard convolutional layer, but the weight are constrained to be orthogonal.\n\n    Key Features:\n    -------------\n        - Ensures orthogonality in transpose convolutions for stable gradient propagation.\n        - Supports dilation, grouped operations, and efficient kernel construction.\n\n    Behavior:\n    ---------\n        - When kernel_size == stride, the layer is an `RkoConvTranspose2d`.\n        - When stride == 1, the layer is a `FastBlockConvTranspose2D`.\n        - Otherwise, the layer is a `BcopRkoConvTranspose2d`.\n\n\n    Note:\n        - This implementation also work under zero padding, it lipschitz constant is still tight, but it looses\n            orthogonality.orthogonality on the image border.\n        - The current implementation of the torch.nn.ConvTranspose2d does not support circular padding. One can\n            implement padding manually by add a padding layer before and setting padding = (0,0).\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the transpose convolution. Default is 1.\n        padding (_size_2_t, optional): Padding size. Default is 0.\n        output_padding (_size_2_t, optional): Additional size for output. Default is 0.\n        groups (int, optional): Number of groups. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        padding_mode (str, optional): Padding mode. Default is \"zeros\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.ConvTranspose2d` (one of `RkoConvTranspose2d`, `FastBlockConvTranspose2D`, or `BcopRkoConvTranspose2d`).\n\n    **Raises:**\n    - `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n\n\n    References:\n        - [1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n        &lt;https://arxiv.org/abs/2501.07930&gt;\n    \"\"\"\n\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RkoConvTranspose2d\n    elif stride == 1:\n        convclass = FastBlockConvTranspose2D\n    else:\n        convclass = BcopRkoConvTranspose2d\n    return convclass(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        output_padding=output_padding,\n        groups=groups,\n        bias=bias,\n        dilation=dilation,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer","title":"<code>BCOPTrivializer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>class BCOPTrivializer(nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        groups,\n    ):\n        \"\"\"This module is used to generate orthogonal kernels for the BCOP layer. It takes\n        as input a matrix PQ of shape (groups, 2*kernel_size, c, c//2) and returns a kernel\n        of shape (c, c, kernel_size, kernel_size) that is orthogonal.\n\n        Args:\n            in_channels (int): number of input channels\n            out_channels (int): number of output channels\n            kernel_size (int): size of the kernel\n            groups (int): number of groups\n        \"\"\"\n        super(BCOPTrivializer, self).__init__()\n        self.kernel_size = kernel_size\n        self.groups = groups\n        self.out_channels = out_channels\n        self.in_channels = in_channels\n        self.min_channels = min(in_channels, out_channels)\n        self.max_channels = max(in_channels, out_channels)\n        self.transpose = out_channels &lt; in_channels\n        self.num_kernels = 2 * kernel_size\n\n    def forward(self, PQ):\n        ident = (\n            torch.eye(self.max_channels // self.groups, device=PQ.device)\n            .unsqueeze(0)\n            .unsqueeze(0)\n        )\n        # PQ contains 2*(kernel_size - 1) + 2 matrices of shape (c, c//2)\n        # the 2 first matrices will be composed to build a (c, c) matrix\n        # this (c, c) matrix will be used to build the first 1x1 conv\n        # the remaining matrices will be used to build the 2x2 convs as\n        # described in BCOP paper\n        ####\n        # first we compute PQ@PQ.t (used by both 1x1 and 2x2 convs)\n        # we can rewrite PQ@PQ.t as an einsum\n        PQ = torch.einsum(\"gijl,gikl-&gt;gijk\", PQ, PQ)\n        # PQ = PQ @ PQ.transpose(-1, -2)\n        # we build the 1x1 conv using the two first matrices\n        # we construct the (c, c) matrix by computing (I - 2*PQ[0]) @ (I - 2*PQ[1])\n        # this is an extension of Householder reflection to the matrix case where\n        # instead of reflecting a vector and compose c matrices, we reflect 2\n        # (c, c//2) matrices. This results in an orthogonal matrix, but the fact that\n        # any orthogonal matrix can be decomposed this way is yet to be proven.\n        c11 = ident - 2 * PQ[:, 0]\n        c11 = c11 @ (ident - 2 * PQ[:, 1])\n        # reshape the matrix to build a 1x1 conv\n        c11 = c11.view(\n            self.max_channels,\n            self.max_channels // self.groups,\n            1,\n            1,\n        )\n        # if the number of channels is different, we need to remove the extra channels\n        # this results in a row/column othogonal matrix. It is still more efficient than\n        # doing a separate orthogonalization (as shapes differs).\n        if self.in_channels != self.out_channels:\n            c11 = c11[:, : self.min_channels // self.groups, :, :]\n\n        # build all 2x2 convs in parallel\n        # half of the matrices will be used to create a 2x1 conv while the other half\n        # will be used to create a 1x2 conv. The 2x1 and 1x2 convs will be composed\n        # to build a 2x2 conv. c12 and c21 are notation abuse, since the tensors represent\n        # 1x1 convs (it is the vertical/horizontal stacking of c12/c21 with (I-c12) and (I-c21)\n        # that will result in a 1x2/2x1 conv)\n        c12 = PQ[:, 2 : 2 + (self.kernel_size - 1), :, :]\n        c21 = PQ[:, 2 + (self.kernel_size - 1) :, :, :]\n        c22 = block_orth(\n            c12, c21\n        )  # this is an efficient and parallel way to compute the 2x2 convs\n        # i used to belive that transposing half of the matrices would alleviate the expressiveness\n        # issue, but it is not notable.\n        # c22[1::2] = -c22[1::2].flip(-1, -2)\n\n        # we now need to compose the 2x2 convs to build the k*k kernel\n        # by using the associativity of the block conv operator we can\n        # run the steps of the BCOP algorithm in parallel: we groups the\n        # 2x2 convs in pairs and apply the block conv operator to each pair\n        # until we have a single conv. If k-1 is a power of two this algorithm\n        # run in log(k-1) steps. (naive associative scan algorithm)\n        # The following condition ensures the loop terminates and prevents it\n        # from running indefinitely in singular cases where c22.shape[0] becomes 1.\n        while (c22.shape[0] % 2 == 0) and (c22.shape[0] &gt; 1):\n            mid = c22.shape[0] // 2\n            c22 = fast_batched_matrix_conv(c22[:mid], c22[mid:], self.groups)\n        # we finally compose the 1x1 conv with the kxk conv\n        res = c11\n        for i in range(c22.shape[0]):  # c22.shape[0] == 1 if k-1 is a power of two\n            res = fast_matrix_conv(res, c22[i], self.groups)\n        # since it is less expensive to compute the transposed kernel when co &lt; ci\n        # we transpose the kernel if needed\n        if self.transpose:\n            res = transpose_kernel(res, self.groups, flip=False)\n        # it seems more efficient to make the kernel contiguous since it will be used\n        # in a convolution\n        return res.contiguous()\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, groups)</code>","text":"<p>This module is used to generate orthogonal kernels for the BCOP layer. It takes as input a matrix PQ of shape (groups, 2*kernel_size, c, c//2) and returns a kernel of shape (c, c, kernel_size, kernel_size) that is orthogonal.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>number of input channels</p> required <code>out_channels</code> <code>int</code> <p>number of output channels</p> required <code>kernel_size</code> <code>int</code> <p>size of the kernel</p> required <code>groups</code> <code>int</code> <p>number of groups</p> required Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels,\n    out_channels,\n    kernel_size,\n    groups,\n):\n    \"\"\"This module is used to generate orthogonal kernels for the BCOP layer. It takes\n    as input a matrix PQ of shape (groups, 2*kernel_size, c, c//2) and returns a kernel\n    of shape (c, c, kernel_size, kernel_size) that is orthogonal.\n\n    Args:\n        in_channels (int): number of input channels\n        out_channels (int): number of output channels\n        kernel_size (int): size of the kernel\n        groups (int): number of groups\n    \"\"\"\n    super(BCOPTrivializer, self).__init__()\n    self.kernel_size = kernel_size\n    self.groups = groups\n    self.out_channels = out_channels\n    self.in_channels = in_channels\n    self.min_channels = min(in_channels, out_channels)\n    self.max_channels = max(in_channels, out_channels)\n    self.transpose = out_channels &lt; in_channels\n    self.num_kernels = 2 * kernel_size\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d","title":"<code>FastBlockConv2d</code>","text":"<p>               Bases: <code>Conv2d</code></p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>class FastBlockConv2d(nn.Conv2d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: Union[str, _size_2_t] = \"same\",\n        dilation: _size_2_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = \"circular\",\n        ortho_params: OrthoParams = OrthoParams(),\n    ):\n        \"\"\"\n        Fast implementation of the Block Circulant Orthogonal Parametrization (BCOP) for convolutional layers.\n        This approach changes the original BCOP algorithm to make it more scalable and efficient. This implementation\n        rewrite efficiently the block convolution operator as a single convolution operation. Also the iterative algorithm\n        is parallelized in the associative scan fashion.\n\n        This layer is a drop-in replacement for the nn.Conv2d layer. It is orthogonal and Lipschitz continuous while maintaining\n        the same interface as the Con2d. Also this method has an explicit kernel, whihc allows to compute the singular values of\n        the convolutional layer.\n\n        Striding is not supported when out_channels &gt; in_channels. Real striding is supported in BcopRkoConv2d. The use of\n        OrthogonalConv2d is recommended.\n        \"\"\"\n        super(FastBlockConv2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            dilation,\n            groups,\n            bias,\n            padding_mode,\n        )\n\n        # raise runtime error if kernel size &gt;= stride\n        if self.kernel_size[0] &lt; self.stride[0] or self.kernel_size[1] &lt; self.stride[1]:\n            raise ValueError(\n                \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n            )\n        if (\n            (self.stride[0] &gt; 1 or self.stride[1] &gt; 1) and (out_channels &gt; in_channels)\n        ) or (\n            self.stride[0] &gt; self.kernel_size[0] or self.stride[1] &gt; self.kernel_size[1]\n        ):\n            raise ValueError(\n                \"stride &gt; 1 is not supported when out_channels &gt; in_channels, \"\n                \"use TODO layer instead\"\n            )\n        if (\n            (self.out_channels &gt;= self.in_channels)\n            and (((self.dilation[0] % self.stride[0]) == 0) and (self.stride[0] &gt; 1))\n            and (((self.dilation[1] % self.stride[1]) == 0) and (self.stride[1] &gt; 1))\n        ):\n            raise ValueError(\n                \"dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.\"\n            )\n        del self.weight\n        attach_bcop_weight(\n            self,\n            \"weight\",\n            (\n                out_channels,\n                in_channels // groups,\n                self.kernel_size[0],\n                self.kernel_size[1],\n            ),\n            groups,\n            ortho_params=ortho_params,\n        )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, padding='same', dilation=1, groups=1, bias=True, padding_mode='circular', ortho_params=OrthoParams())</code>","text":"<p>Fast implementation of the Block Circulant Orthogonal Parametrization (BCOP) for convolutional layers. This approach changes the original BCOP algorithm to make it more scalable and efficient. This implementation rewrite efficiently the block convolution operator as a single convolution operation. Also the iterative algorithm is parallelized in the associative scan fashion.</p> <p>This layer is a drop-in replacement for the nn.Conv2d layer. It is orthogonal and Lipschitz continuous while maintaining the same interface as the Con2d. Also this method has an explicit kernel, whihc allows to compute the singular values of the convolutional layer.</p> <p>Striding is not supported when out_channels &gt; in_channels. Real striding is supported in BcopRkoConv2d. The use of OrthogonalConv2d is recommended.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: Union[str, _size_2_t] = \"same\",\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"circular\",\n    ortho_params: OrthoParams = OrthoParams(),\n):\n    \"\"\"\n    Fast implementation of the Block Circulant Orthogonal Parametrization (BCOP) for convolutional layers.\n    This approach changes the original BCOP algorithm to make it more scalable and efficient. This implementation\n    rewrite efficiently the block convolution operator as a single convolution operation. Also the iterative algorithm\n    is parallelized in the associative scan fashion.\n\n    This layer is a drop-in replacement for the nn.Conv2d layer. It is orthogonal and Lipschitz continuous while maintaining\n    the same interface as the Con2d. Also this method has an explicit kernel, whihc allows to compute the singular values of\n    the convolutional layer.\n\n    Striding is not supported when out_channels &gt; in_channels. Real striding is supported in BcopRkoConv2d. The use of\n    OrthogonalConv2d is recommended.\n    \"\"\"\n    super(FastBlockConv2d, self).__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        groups,\n        bias,\n        padding_mode,\n    )\n\n    # raise runtime error if kernel size &gt;= stride\n    if self.kernel_size[0] &lt; self.stride[0] or self.kernel_size[1] &lt; self.stride[1]:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if (\n        (self.stride[0] &gt; 1 or self.stride[1] &gt; 1) and (out_channels &gt; in_channels)\n    ) or (\n        self.stride[0] &gt; self.kernel_size[0] or self.stride[1] &gt; self.kernel_size[1]\n    ):\n        raise ValueError(\n            \"stride &gt; 1 is not supported when out_channels &gt; in_channels, \"\n            \"use TODO layer instead\"\n        )\n    if (\n        (self.out_channels &gt;= self.in_channels)\n        and (((self.dilation[0] % self.stride[0]) == 0) and (self.stride[0] &gt; 1))\n        and (((self.dilation[1] % self.stride[1]) == 0) and (self.stride[1] &gt; 1))\n    ):\n        raise ValueError(\n            \"dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.\"\n        )\n    del self.weight\n    attach_bcop_weight(\n        self,\n        \"weight\",\n        (\n            out_channels,\n            in_channels // groups,\n            self.kernel_size[0],\n            self.kernel_size[1],\n        ),\n        groups,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D","title":"<code>FastBlockConvTranspose2D</code>","text":"<p>               Bases: <code>ConvTranspose2d</code></p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>class FastBlockConvTranspose2D(nn.ConvTranspose2d):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        kernel_size: _size_2_t,\n        stride: _size_2_t = 1,\n        padding: _size_2_t = 0,\n        output_padding: _size_2_t = 0,\n        groups: int = 1,\n        bias: bool = True,\n        dilation: _size_2_t = 1,\n        padding_mode: str = \"zeros\",\n        ortho_params: OrthoParams = OrthoParams(),\n    ):\n        \"\"\"\n        Extention of the BCOP algorithm to transposed convolutions. This implementation\n        uses the same algorithm as the FlashBCOP layer, but the layer acts as a transposed\n        convolutional layer.\n        \"\"\"\n        super(FastBlockConvTranspose2D, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride,\n            padding,\n            output_padding,\n            groups,\n            bias,\n            dilation,\n            padding_mode,\n        )\n        if (\n            (self.out_channels &lt;= self.in_channels)\n            and (((self.dilation[0] % self.stride[0]) == 0) and (self.stride[0] &gt; 1))\n            and (((self.dilation[1] % self.stride[1]) == 0) and (self.stride[1] &gt; 1))\n        ):\n            raise ValueError(\n                \"dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.\"\n            )\n        if self.kernel_size[0] &lt; self.stride[0] or self.kernel_size[1] &lt; self.stride[1]:\n            raise ValueError(\n                \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n            )\n        if (\n            ((max(in_channels, out_channels) // groups) &lt; 2)\n            and (self.kernel_size[0] != self.stride[0])\n            and (self.kernel_size[1] != self.stride[1])\n        ):\n            raise ValueError(\"inner conv must have at least 2 channels\")\n        del self.weight\n        attach_bcop_weight(\n            self,\n            \"weight\",\n            (\n                in_channels,\n                out_channels // self.groups,\n                self.kernel_size[0],\n                self.kernel_size[1],\n            ),\n            groups,\n            ortho_params=ortho_params,\n        )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', ortho_params=OrthoParams())</code>","text":"<p>Extention of the BCOP algorithm to transposed convolutions. This implementation uses the same algorithm as the FlashBCOP layer, but the layer acts as a transposed convolutional layer.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    ortho_params: OrthoParams = OrthoParams(),\n):\n    \"\"\"\n    Extention of the BCOP algorithm to transposed convolutions. This implementation\n    uses the same algorithm as the FlashBCOP layer, but the layer acts as a transposed\n    convolutional layer.\n    \"\"\"\n    super(FastBlockConvTranspose2D, self).__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        bias,\n        dilation,\n        padding_mode,\n    )\n    if (\n        (self.out_channels &lt;= self.in_channels)\n        and (((self.dilation[0] % self.stride[0]) == 0) and (self.stride[0] &gt; 1))\n        and (((self.dilation[1] % self.stride[1]) == 0) and (self.stride[1] &gt; 1))\n    ):\n        raise ValueError(\n            \"dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if self.kernel_size[0] &lt; self.stride[0] or self.kernel_size[1] &lt; self.stride[1]:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if (\n        ((max(in_channels, out_channels) // groups) &lt; 2)\n        and (self.kernel_size[0] != self.stride[0])\n        and (self.kernel_size[1] != self.stride[1])\n    ):\n        raise ValueError(\"inner conv must have at least 2 channels\")\n    del self.weight\n    attach_bcop_weight(\n        self,\n        \"weight\",\n        (\n            in_channels,\n            out_channels // self.groups,\n            self.kernel_size[0],\n            self.kernel_size[1],\n        ),\n        groups,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.attach_bcop_weight","title":"<code>attach_bcop_weight(layer, weight_name, kernel_shape, groups, ortho_params=OrthoParams())</code>","text":"<p>Attach a weight to a layer and parametrize it with the BCOPTrivializer module. The attached weight will be the kernel of an orthogonal convolutional layer.</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Module</code> <p>layer to which the weight will be attached</p> required <code>weight_name</code> <code>str</code> <p>name of the weight</p> required <code>kernel_shape</code> <code>tuple</code> <p>shape of the kernel (out_channels, in_channels/groups, kernel_size, kernel_size)</p> required <code>groups</code> <code>int</code> <p>number of groups</p> required <code>bjorck_params</code> <code>BjorckParams</code> <p>parameters of the Bjorck orthogonalization. Defaults to BjorckParams().</p> required <p>Returns:</p> Type Description <p>torch.Tensor: a handle to the attached weight</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def attach_bcop_weight(\n    layer, weight_name, kernel_shape, groups, ortho_params: OrthoParams = OrthoParams()\n):\n    \"\"\"\n    Attach a weight to a layer and parametrize it with the BCOPTrivializer module.\n    The attached weight will be the kernel of an orthogonal convolutional layer.\n\n    Args:\n        layer (torch.nn.Module): layer to which the weight will be attached\n        weight_name (str): name of the weight\n        kernel_shape (tuple): shape of the kernel (out_channels, in_channels/groups, kernel_size, kernel_size)\n        groups (int): number of groups\n        bjorck_params (BjorckParams, optional): parameters of the Bjorck orthogonalization. Defaults to BjorckParams().\n\n    Returns:\n        torch.Tensor: a handle to the attached weight\n    \"\"\"\n    out_channels, in_channels, kernel_size, k2 = kernel_shape\n    in_channels *= groups  # compute the real number of input channels\n    assert kernel_size == k2, \"only square kernels are supported for the moment\"\n    max_channels = max(in_channels, out_channels)\n    num_kernels = (\n        2 * kernel_size\n    )  # the number of projectors needed to create the kernel\n    # register projectors matrices\n    layer.register_parameter(\n        weight_name,\n        torch.nn.Parameter(\n            torch.Tensor(\n                groups,\n                num_kernels,\n                (max_channels // groups),\n                (max_channels // (groups * 2)),\n            ),\n            requires_grad=True,\n        ),\n    )\n    weight = getattr(layer, weight_name)\n    torch.nn.init.orthogonal_(weight)\n    if weight.shape[-1] == 1:\n        # if max_channels//groups == 1, we can use L2 normalization\n        # instead of Bjorck orthogonalization which is significantly faster\n        parametrize.register_parametrization(\n            layer,\n            weight_name,\n            L2Normalize(dtype=weight.dtype, dim=(-2)),\n        )\n    else:\n        # register power iteration and Bjorck orthogonalization\n        parametrize.register_parametrization(\n            layer,\n            weight_name,\n            ortho_params.spectral_normalizer(weight_shape=weight.shape),\n        )\n        parametrize.register_parametrization(\n            layer,\n            weight_name,\n            ortho_params.orthogonalizer(\n                weight_shape=weight.shape,\n            ),\n        )\n    # now we have orthogonal projectors, we can build the orthogonal kernel\n    parametrize.register_parametrization(\n        layer,\n        weight_name,\n        BCOPTrivializer(\n            in_channels,\n            out_channels,\n            kernel_size,\n            groups,\n        ),\n        unsafe=True,\n    )\n    return weight\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.block_orth","title":"<code>block_orth(p1, p2)</code>","text":"<p>Construct a 2x2 orthogonal matrix from two orthogonal orthogonal projectors. Each projector can be seen as a 1x1 convolution, hence the stacking spatial stacking of [pi, I-pi] can be seen as a 2x1 or 1x2 orthogonal convolution. By using the block convolution operator, we can compute the 2x2 orthogonal conv. In this specific case, we can write the whole operation as a single einsum.</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Tensor</code> <p>orthogonal projector of shape (g, x, c, c) where g is the number of groups, x is a batch dimension (allowing to compute the operation in parallel) and c is the number of channels.</p> required <code>p2</code> <code>Tensor</code> <p>orthogonal projector of shape (g, x, c, c) also.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: orthogonal 2x2 conv of shape (x, g*c, c, 2, 2)</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def block_orth(p1, p2):\n    \"\"\"Construct a 2x2 orthogonal matrix from two orthogonal orthogonal projectors.\n    Each projector can be seen as a 1x1 convolution, hence the stacking spatial stacking\n    of [pi, I-pi] can be seen as a 2x1 or 1x2 orthogonal convolution. By using the block\n    convolution operator, we can compute the 2x2 orthogonal conv. In this specific case,\n    we can write the whole operation as a single einsum.\n\n    Args:\n        p1 (torch.Tensor): orthogonal projector of shape (g, x, c, c) where g is the number\n            of groups, x is a batch dimension (allowing to compute the operation in parallel)\n            and c is the number of channels.\n        p2 (torch.Tensor): orthogonal projector of shape (g, x, c, c) also.\n\n    Returns:\n        torch.Tensor: orthogonal 2x2 conv of shape (x, g*c, c, 2, 2)\n    \"\"\"\n    assert p1.shape == p2.shape\n    g, x, n, n2 = p1.shape\n    eye = torch.eye(n, device=p1.device, dtype=p1.dtype)\n    # sorry for using x as a batch dimension, but this einsum was hard to write (thank you unit tests!)\n    res = torch.einsum(\n        \"bgxij,cgxjk-&gt;xgikbc\", torch.stack([p1, eye - p1]), torch.stack([p2, eye - p2])\n    )\n    # we reshape the result to get a 2x2 conv kernel\n    res = res.reshape(x, g * n, n, 2, 2)\n    return res\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.conv_singular_values_numpy","title":"<code>conv_singular_values_numpy(kernel, input_shape)</code>","text":"<p>Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers. In International Conference on Learning Representations, 2019.</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def conv_singular_values_numpy(kernel, input_shape):\n    \"\"\"\n    Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers.\n    In International Conference on Learning Representations, 2019.\n    \"\"\"\n    kernel = np.transpose(kernel, [0, 3, 4, 1, 2])  # g, k1, k2, ci, co\n    transforms = np.fft.fft2(kernel, input_shape, axes=[1, 2])  # g, k1, k2, ci, co\n    try:\n        svs = np.linalg.svd(\n            transforms, compute_uv=False, full_matrices=False\n        )  # g, k1, k2, min(ci, co)\n        stable_rank = (np.mean(svs) ** 2) / svs.max()\n        return svs.min(), svs.max(), stable_rank\n    except np.linalg.LinAlgError:\n        print(\"numerical error in svd, returning only largest singular value\")\n        return None, np.linalg.norm(transforms, axis=(1, 2), ord=2), None\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_batched_matrix_conv","title":"<code>fast_batched_matrix_conv(m1, m2, groups=1)</code>","text":"<p>Compute the convolution of two matrices using the block convolution operator. This is exactly the same as fast_matrix_conv but with an additional batch dimension. This is useful when we want to compute the convolution of multiple matrices in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>m1</code> <code>Tensor</code> <p>matrix of shape (b, c2, c1/g, k1, k2)</p> required <code>m2</code> <code>Tensor</code> <p>matrix of shape (b, c3, c2/g, l1, l2)</p> required <code>groups</code> <code>int</code> <p>number of groups. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <p>torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape</p> <p>(b, c3, c1, k+l-1, k+l-1)</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def fast_batched_matrix_conv(m1, m2, groups=1):\n    \"\"\"Compute the convolution of two matrices using the block convolution operator.\n    This is exactly the same as fast_matrix_conv but with an additional batch dimension.\n    This is useful when we want to compute the convolution of multiple matrices in parallel.\n\n    Args:\n        m1 (torch.Tensor): matrix of shape (b, c2, c1/g, k1, k2)\n        m2 (torch.Tensor): matrix of shape (b, c3, c2/g, l1, l2)\n        groups (int, optional): number of groups. Defaults to 1.\n\n    Returns:\n        torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape\n        (b, c3, c1, k+l-1, k+l-1)\n    \"\"\"\n    b, m, n, k1, k2 = m1.shape\n    b2, nb, mb, l1, l2 = m2.shape\n    assert m == mb * groups\n    assert b == b2\n    m1 = m1.view(b * m, n, k1, k2)\n    m2 = m2.view(b * nb, mb, l1, l2)\n    # Rearrange m1 for conv\n    m1 = m1.transpose(0, 1)  # n*m*k1*k2\n    # Rearrange m2 for conv\n    m2 = m2.flip(-2, -1)\n    r2 = torch.nn.functional.conv2d(m1, m2, groups=groups * b, padding=(l1 - 1, l2 - 1))\n    # Rearrange result\n    r2 = r2.view(n, b, nb, k1 + l1 - 1, k2 + l2 - 1)\n    r2 = r2.permute(1, 2, 0, 3, 4)\n    return r2\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_matrix_conv","title":"<code>fast_matrix_conv(m1, m2, groups=1)</code>","text":"<p>Compute the convolution of two matrices using the block convolution operator. The original algorithm can be written as a single convolution operation, which unlock the massive parallelism of the convolution operator. This implementation is also more memory efficient than the original algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>m1</code> <code>Tensor</code> <p>matrix of shape (c2, c1/g, k1, k2)</p> required <code>m2</code> <code>Tensor</code> <p>matrix of shape (c3, c2/g, l1, l2)</p> required <code>groups</code> <code>int</code> <p>number of groups. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <p>torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape</p> <p>(c3, c1, k+l-1, k+l-1)</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def fast_matrix_conv(m1, m2, groups=1):\n    \"\"\"Compute the convolution of two matrices using the block convolution operator.\n    The original algorithm can be written as a single convolution operation, which\n    unlock the massive parallelism of the convolution operator. This implementation\n    is also more memory efficient than the original algorithm.\n\n    Args:\n        m1 (torch.Tensor): matrix of shape (c2, c1/g, k1, k2)\n        m2 (torch.Tensor): matrix of shape (c3, c2/g, l1, l2)\n        groups (int, optional): number of groups. Defaults to 1.\n\n    Returns:\n        torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape\n        (c3, c1, k+l-1, k+l-1)\n\n    \"\"\"\n    # m1 is m*n*k1*k2\n    # m2 is nb*m*l1*l2\n    m, n, k1, k2 = m1.shape\n    nb, mb, l1, l2 = m2.shape\n    assert m == mb * groups\n\n    # Rearrange m1 for conv\n    m1 = m1.transpose(0, 1)  # n*m*k1*k2\n\n    # Rearrange m2 for conv\n    m2 = m2.flip(-2, -1)\n\n    # Run conv, output shape nb*n*(k+l-1)*(k+l-1)\n    r2 = torch.nn.functional.conv2d(m1, m2, groups=groups, padding=(l1 - 1, l2 - 1))\n\n    # Rearrange result\n    return r2.transpose(0, 1)  # n*nb*(k+l-1)*(k+l-1)\n</code></pre>"},{"location":"api/aoc/#orthogonium.layers.conv.AOC.fast_block_ortho_conv.transpose_kernel","title":"<code>transpose_kernel(p, groups, flip=True)</code>","text":"<p>Compute the transpose of a kernel. This is done by transposing the kernel and flipping it along the last two dimensions. This operation is equivalent to the transpose of the convolution operator (when the stride is 1)</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Tensor</code> <p>kernel of shape (cig, cog, k1, k2)</p> required <code>groups</code> <code>int</code> <p>number of groups</p> required <code>flip</code> <code>bool</code> <p>if True, the kernel will be flipped. Defaults to True. False can be used when the is no need to flip the kernel.</p> <code>True</code> <p>Returns:</p> Type Description <p>torch.Tensor: transposed kernel of shape (cog, cig, k1, k2)</p> Source code in <code>orthogonium\\layers\\conv\\AOC\\fast_block_ortho_conv.py</code> <pre><code>def transpose_kernel(p, groups, flip=True):\n    \"\"\"Compute the transpose of a kernel. This is done by transposing the kernel and\n    flipping it along the last two dimensions. This operation is equivalent to the\n    transpose of the convolution operator (when the stride is 1)\n\n    Args:\n        p (torch.Tensor): kernel of shape (cig, cog, k1, k2)\n        groups (int): number of groups\n        flip (bool, optional): if True, the kernel will be flipped. Defaults to True.\n            False can be used when the is no need to flip the kernel.\n\n    Returns:\n        torch.Tensor: transposed kernel of shape (cog, cig, k1, k2)\n    \"\"\"\n    cig, cog, k1, k2 = p.shape\n    cig = cig // groups\n    # we do not perform flip since it does not affect orthogonality\n    p = p.view(groups, cig, cog, k1, k2)\n    p = p.transpose(1, 2)\n    if flip:\n        p = p.flip(-1, -2)\n    # merge groups to get the final kernel\n    p = p.reshape(cog * groups, cig, k1, k2)\n    return p\n</code></pre>"},{"location":"api/conv/","title":"convolutions","text":""},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d","title":"<code>AdaptiveOrthoConv2d(in_channels, out_channels, kernel_size, stride=1, padding='same', dilation=1, groups=1, bias=True, padding_mode='circular', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel size and stride. This is the implementation for the <code>Adaptive Orthogonal Convolution</code> scheme [1]. It aims to be scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers. This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</p>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features","title":"Key Features:","text":"<pre><code>- Enforces orthogonality, preserving gradient norms.\n- Supports native striding, dilation, grouped convolutions, and flexible padding.\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RKOConv2d`.\n- When stride == 1, the layer is a `FastBlockConv2d`.\n- Otherwise, the layer is a `BcopRkoConv2d`.\n</code></pre> Note <ul> <li>This implementation also work under zero padding, it lipschitz constant is still tight, but it looses     orthogonality.orthogonality on the image border.</li> <li>the unit tesing validated for a tolerance of 1e-4 under various orthogonalization schemes (see     reparametrizers). Only Cholesky based methods were validated for a lower tolerance of 5e-2.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>str or _size_2_t</code> <p>Padding mode or size. Default is \"same\".</p> <code>'same'</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input to output channels. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"circular\".</p> <code>'circular'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>Conv2d</code> <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p> <p>Raises:</p> Type Description <code>`ValueError`</code> <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> References <ul> <li>[1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025). An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures. https://arxiv.org/abs/2501.07930</li> </ul> Source code in <code>orthogonium\\layers\\conv\\AOC\\ortho_conv.py</code> <pre><code>def AdaptiveOrthoConv2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: Union[str, _size_2_t] = \"same\",\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"circular\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.Conv2d:\n    \"\"\"\n    Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel\n    size and stride. This is the implementation for the `Adaptive Orthogonal Convolution` scheme [1]. It aims to be\n    scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers.\n    This layer also intend to be compatible with all the feature of the `nn.Conv2d` class (e.g., striding, dilation,\n    grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a\n    standard convolutional layer, but the weight are constrained to be orthogonal.\n\n    Key Features:\n    -------------\n        - Enforces orthogonality, preserving gradient norms.\n        - Supports native striding, dilation, grouped convolutions, and flexible padding.\n\n    Behavior:\n    ---------\n        - When kernel_size == stride, the layer is an `RKOConv2d`.\n        - When stride == 1, the layer is a `FastBlockConv2d`.\n        - Otherwise, the layer is a `BcopRkoConv2d`.\n\n    Note:\n        - This implementation also work under zero padding, it lipschitz constant is still tight, but it looses\n            orthogonality.orthogonality on the image border.\n        - the unit tesing validated for a tolerance of 1e-4 under various orthogonalization schemes (see\n            reparametrizers). Only Cholesky based methods were validated for a lower tolerance of 5e-2.\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the convolution. Default is 1.\n        padding (str or _size_2_t, optional): Padding mode or size. Default is \"same\".\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        padding_mode (str, optional): Padding mode. Default is \"circular\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).\n\n    Raises:\n        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n\n\n    References:\n        - [1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n        &lt;https://arxiv.org/abs/2501.07930&gt;\n    \"\"\"\n\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RKOConv2d\n    elif (stride == 1) or ((in_channels &gt;= out_channels) and (dilation &gt; 1)):\n        convclass = FastBlockConv2d\n    else:\n        convclass = BcopRkoConv2d\n    return convclass(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=groups,\n        bias=bias,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d","title":"<code>AdaptiveOrthoConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on kernel size and stride. This is the implementation for the <code>Adaptive Orthogonal Convolution</code> scheme [1]. It aims to be scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers. This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</p>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features","title":"Key Features:","text":"<pre><code>- Ensures orthogonality in transpose convolutions for stable gradient propagation.\n- Supports dilation, grouped operations, and efficient kernel construction.\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RkoConvTranspose2d`.\n- When stride == 1, the layer is a `FastBlockConvTranspose2D`.\n- Otherwise, the layer is a `BcopRkoConvTranspose2d`.\n</code></pre> Note <ul> <li>This implementation also work under zero padding, it lipschitz constant is still tight, but it looses     orthogonality.orthogonality on the image border.</li> <li>The current implementation of the torch.nn.ConvTranspose2d does not support circular padding. One can     implement padding manually by add a padding layer before and setting padding = (0,0).</li> </ul> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the transpose convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>_size_2_t</code> <p>Padding size. Default is 0.</p> <code>0</code> <code>output_padding</code> <code>_size_2_t</code> <p>Additional size for output. Default is 0.</p> <code>0</code> <code>groups</code> <code>int</code> <p>Number of groups. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"zeros\".</p> <code>'zeros'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>ConvTranspose2d</code> <p>A configured instance of <code>nn.ConvTranspose2d</code> (one of <code>RkoConvTranspose2d</code>, <code>FastBlockConvTranspose2D</code>, or <code>BcopRkoConvTranspose2d</code>).</p> <p>Raises: - <code>ValueError</code>: If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> References <ul> <li>[1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025). An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures. https://arxiv.org/abs/2501.07930</li> </ul> Source code in <code>orthogonium\\layers\\conv\\AOC\\ortho_conv.py</code> <pre><code>def AdaptiveOrthoConvTranspose2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.ConvTranspose2d:\n    \"\"\"\n    Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on kernel\n    size and stride. This is the implementation for the `Adaptive Orthogonal Convolution` scheme [1]. It aims to be\n    scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers.\n    This layer also intend to be compatible with all the feature of the `nn.Conv2d` class (e.g., striding, dilation,\n    grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a\n    standard convolutional layer, but the weight are constrained to be orthogonal.\n\n    Key Features:\n    -------------\n        - Ensures orthogonality in transpose convolutions for stable gradient propagation.\n        - Supports dilation, grouped operations, and efficient kernel construction.\n\n    Behavior:\n    ---------\n        - When kernel_size == stride, the layer is an `RkoConvTranspose2d`.\n        - When stride == 1, the layer is a `FastBlockConvTranspose2D`.\n        - Otherwise, the layer is a `BcopRkoConvTranspose2d`.\n\n\n    Note:\n        - This implementation also work under zero padding, it lipschitz constant is still tight, but it looses\n            orthogonality.orthogonality on the image border.\n        - The current implementation of the torch.nn.ConvTranspose2d does not support circular padding. One can\n            implement padding manually by add a padding layer before and setting padding = (0,0).\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the transpose convolution. Default is 1.\n        padding (_size_2_t, optional): Padding size. Default is 0.\n        output_padding (_size_2_t, optional): Additional size for output. Default is 0.\n        groups (int, optional): Number of groups. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        padding_mode (str, optional): Padding mode. Default is \"zeros\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.ConvTranspose2d` (one of `RkoConvTranspose2d`, `FastBlockConvTranspose2D`, or `BcopRkoConvTranspose2d`).\n\n    **Raises:**\n    - `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n\n\n    References:\n        - [1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n        &lt;https://arxiv.org/abs/2501.07930&gt;\n    \"\"\"\n\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RkoConvTranspose2d\n    elif stride == 1:\n        convclass = FastBlockConvTranspose2D\n    else:\n        convclass = BcopRkoConvTranspose2d\n    return convclass(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        output_padding=output_padding,\n        groups=groups,\n        bias=bias,\n        dilation=dilation,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer--ssl-derived-1-lipschitz-layers","title":"SSL derived 1-Lipschitz Layers","text":"<p>This module implements several 1-Lipschitz residual blocks, inspired by and extending the SDP-based Lipschitz Layers (SLL) from [1]. Specifically:</p> <ul> <li> <p><code>SDPBasedLipschitzResBlock</code>   The original version of the 1-Lipschitz convolutional residual block. It enforces Lipschitz   constraints by rescaling activation outputs according to an estimate of the operator norm.</p> </li> <li> <p><code>SLLxAOCLipschitzResBlock</code>   An extended version of the SLL approach described in [1], combined with additional orthogonal   convolutions to handle stride, kernel-size, or channel-dimension changes. It fuses multiple   convolutions via the block convolution, thereby preserving the 1-Lipschitz property while enabling   strided downsampling or modifying input/output channels.</p> </li> <li> <p><code>AOCLipschitzResBlock</code>   A variant of the original Lipschitz block where the core convolution is replaced by an   <code>AdaptiveOrthoConv2d</code>. It maintains the 1-Lipschitz property with orthogonal weight   parameterization while providing efficient convolution implementations.</p> </li> </ul>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer--references","title":"References","text":"<p>[1] Alexandre Araujo, Aaron J Havens, Blaise Delattre, Alexandre Allauzen, and Bin Hu. A unified alge- braic perspective on lipschitz neural networks. In The Eleventh International Conference on Learning Representations, 2023 [2] Thibaut Boissin, Franck Mamalet, Thomas Fel, Agustin Martin Picard, Thomas Massena, Mathieu Serrurier, An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures</p>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer--notes-on-the-sll-approach","title":"Notes on the SLL approach","text":"<p>In [1], the SLL layer for convolutions is a 1-Lipschitz residual operation defined approximately as:</p> \\[ y = x - \\mathbf{K}^T \\star (t \\times  \\sigma(\\mathbf{K} \\star x + b)), \\] <p>where \\(\\mathbf{K}\\) represents a toeplitz (convolution) matrix that represent a 1-Lipschitz operator. This is done in practice by computing a normalization vector \\(\\mathbf{t}\\) and rescaling the activation outputs by \\(\\mathbf{t}\\).</p> <p>By default, the SLL formulation does not allow strides or changes in the number of channels. To address these issues, <code>SLLxAOCLipschitzResBlock</code> adds extra orthogonal convolutions before and/or after the main SLL operation. These additional convolutions can be merged via block convolution (Proposition 1 in [2]) to maintain 1-Lipschitz behavior while enabling stride and/or channel changes.</p> <p>When \\(\\mathbf{K}\\), \\(\\mathbf{K}_{pre}\\), and \\(\\mathbf{K}_{post}\\) each correspond to 2\u00d72 convolutions, the resulting block effectively contains two 3\u00d73 convolutions in one branch and a single 4\u00d74 stride-2 convolution in the skip branch\u2014quite similar to typical ResNet blocks.</p>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock","title":"<code>AOCLipschitzResBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>class AOCLipschitzResBlock(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        inner_dim_factor: int,\n        kernel_size: _size_2_t,\n        dilation: _size_2_t = 1,\n        groups: int = 1,\n        bias: bool = True,\n        padding_mode: str = \"circular\",\n        ortho_params: OrthoParams = OrthoParams(),\n    ):\n        \"\"\"\n        A Lipschitz residual block in which the main convolution is replaced by\n        `AdaptiveOrthoConv2d` (AOC). This preserves 1-Lipschitz (or lower) behavior through\n        an orthogonal parameterization, without explicitly computing a scaling factor `t`.\n\n        $$\n        y = x - \\mathbf{K}^T \\\\star (\\sigma(\\\\mathbf{K} \\\\star x + b)),\n        $$\n\n        **Args**:\n          - `in_channels` (int): Number of input channels.\n          - `inner_dim_factor` (int): Multiplier for internal representation size.\n          - `kernel_size` (_size_2_t): Convolution kernel size.\n          - `dilation` (_size_2_t, optional): Default is 1.\n          - `groups` (int, optional): Default is 1.\n          - `bias` (bool, optional): If True, adds a learnable bias. Default is True.\n          - `padding_mode` (str, optional): `'circular'` or `'zeros'`. Default is `'circular'`.\n          - `ortho_params` (OrthoParams, optional): Orthogonal parameterization settings. Default is `OrthoParams()`.\n\n\n        References:\n            - [1] Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.\n            A Unified Algebraic Perspective on Lipschitz Neural Networks.\n            In The Eleventh International Conference on Learning Representations.\n            &lt;https://arxiv.org/abs/2303.03169&gt;\n            - [2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n            An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n            &lt;https://arxiv.org/abs/2501.07930&gt;\n        \"\"\"\n        super().__init__()\n\n        inner_dim = int(in_channels * inner_dim_factor)\n        self.activation = nn.ReLU()\n\n        if padding_mode not in [\"circular\", \"zeros\"]:\n            raise ValueError(\"padding_mode must be either 'circular' or 'zeros'\")\n        if padding_mode == \"circular\":\n            self.padding = 0  # will be handled by the padding function\n        else:\n            self.padding = kernel_size // 2\n\n        self.in_conv = AdaptiveOrthoConv2d(\n            in_channels,\n            inner_dim,\n            kernel_size=kernel_size,\n            stride=1,\n            padding=\"same\",\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n            padding_mode=padding_mode,\n            ortho_params=ortho_params,\n        )\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n        self.groups = groups\n        self.bias = bias\n        self.padding_mode = padding_mode\n\n    def forward(self, x):\n        kernel = self.in_conv.weight\n        # conv\n        res = x\n        if self.padding_mode == \"circular\":\n            res = F.pad(\n                res,\n                (self.padding,) * 4,\n                mode=\"circular\",\n                value=0,\n            )\n        res = F.conv2d(\n            res,\n            kernel,\n            bias=self.in_conv.bias,\n            padding=0,\n            groups=self.groups,\n        )\n        # activation\n        res = self.activation(res)\n        # conv transpose\n        if self.padding_mode == \"circular\":\n            res = F.pad(\n                res,\n                (self.padding,) * 4,\n                mode=\"circular\",\n                value=0,\n            )\n        res = 2 * F.conv_transpose2d(res, kernel, padding=0, groups=self.groups)\n        # residual\n        out = x - res\n        return out\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock.__init__","title":"<code>__init__(in_channels, inner_dim_factor, kernel_size, dilation=1, groups=1, bias=True, padding_mode='circular', ortho_params=OrthoParams())</code>","text":"<p>A Lipschitz residual block in which the main convolution is replaced by <code>AdaptiveOrthoConv2d</code> (AOC). This preserves 1-Lipschitz (or lower) behavior through an orthogonal parameterization, without explicitly computing a scaling factor <code>t</code>.</p> \\[ y = x - \\mathbf{K}^T \\star (\\sigma(\\mathbf{K} \\star x + b)), \\] <p>Args:   - <code>in_channels</code> (int): Number of input channels.   - <code>inner_dim_factor</code> (int): Multiplier for internal representation size.   - <code>kernel_size</code> (_size_2_t): Convolution kernel size.   - <code>dilation</code> (_size_2_t, optional): Default is 1.   - <code>groups</code> (int, optional): Default is 1.   - <code>bias</code> (bool, optional): If True, adds a learnable bias. Default is True.   - <code>padding_mode</code> (str, optional): <code>'circular'</code> or <code>'zeros'</code>. Default is <code>'circular'</code>.   - <code>ortho_params</code> (OrthoParams, optional): Orthogonal parameterization settings. Default is <code>OrthoParams()</code>.</p> References <ul> <li>[1] Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B. A Unified Algebraic Perspective on Lipschitz Neural Networks. In The Eleventh International Conference on Learning Representations. https://arxiv.org/abs/2303.03169</li> <li>[2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025). An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures. https://arxiv.org/abs/2501.07930</li> </ul> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    inner_dim_factor: int,\n    kernel_size: _size_2_t,\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"circular\",\n    ortho_params: OrthoParams = OrthoParams(),\n):\n    \"\"\"\n    A Lipschitz residual block in which the main convolution is replaced by\n    `AdaptiveOrthoConv2d` (AOC). This preserves 1-Lipschitz (or lower) behavior through\n    an orthogonal parameterization, without explicitly computing a scaling factor `t`.\n\n    $$\n    y = x - \\mathbf{K}^T \\\\star (\\sigma(\\\\mathbf{K} \\\\star x + b)),\n    $$\n\n    **Args**:\n      - `in_channels` (int): Number of input channels.\n      - `inner_dim_factor` (int): Multiplier for internal representation size.\n      - `kernel_size` (_size_2_t): Convolution kernel size.\n      - `dilation` (_size_2_t, optional): Default is 1.\n      - `groups` (int, optional): Default is 1.\n      - `bias` (bool, optional): If True, adds a learnable bias. Default is True.\n      - `padding_mode` (str, optional): `'circular'` or `'zeros'`. Default is `'circular'`.\n      - `ortho_params` (OrthoParams, optional): Orthogonal parameterization settings. Default is `OrthoParams()`.\n\n\n    References:\n        - [1] Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.\n        A Unified Algebraic Perspective on Lipschitz Neural Networks.\n        In The Eleventh International Conference on Learning Representations.\n        &lt;https://arxiv.org/abs/2303.03169&gt;\n        - [2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n        &lt;https://arxiv.org/abs/2501.07930&gt;\n    \"\"\"\n    super().__init__()\n\n    inner_dim = int(in_channels * inner_dim_factor)\n    self.activation = nn.ReLU()\n\n    if padding_mode not in [\"circular\", \"zeros\"]:\n        raise ValueError(\"padding_mode must be either 'circular' or 'zeros'\")\n    if padding_mode == \"circular\":\n        self.padding = 0  # will be handled by the padding function\n    else:\n        self.padding = kernel_size // 2\n\n    self.in_conv = AdaptiveOrthoConv2d(\n        in_channels,\n        inner_dim,\n        kernel_size=kernel_size,\n        stride=1,\n        padding=\"same\",\n        dilation=dilation,\n        groups=groups,\n        bias=bias,\n        padding_mode=padding_mode,\n        ortho_params=ortho_params,\n    )\n    self.kernel_size = kernel_size\n    self.dilation = dilation\n    self.groups = groups\n    self.bias = bias\n    self.padding_mode = padding_mode\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense","title":"<code>SDPBasedLipschitzDense</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>class SDPBasedLipschitzDense(nn.Module):\n    def __init__(self, in_features, out_features, inner_dim, **kwargs):\n        \"\"\"\n        A 1-Lipschitz fully-connected layer (dense version). Similar to the convolutional\n        SLL approach, but operates on vectors:\n\n        $$\n        y = x - K^T \\\\times (t \\\\times \\sigma(K \\\\times x + b)),\n        $$\n\n        **Args**:\n          - `in_features` (int): Input size.\n          - `out_features` (int): Output size (must match `in_features` to remain 1-Lipschitz).\n          - `inner_dim` (int): The internal dimension used for the transform.\n\n\n        References:\n            - Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.\n            A Unified Algebraic Perspective on Lipschitz Neural Networks.\n            In The Eleventh International Conference on Learning Representations.\n            &lt;https://arxiv.org/abs/2303.03169&gt;\n        \"\"\"\n        super().__init__()\n\n        inner_dim = inner_dim if inner_dim != -1 else in_features\n        self.activation = nn.ReLU()\n\n        self.weight = nn.Parameter(torch.empty(inner_dim, in_features))\n        self.bias = nn.Parameter(torch.empty(1, inner_dim))\n        self.q = nn.Parameter(torch.randn(inner_dim))\n\n        nn.init.xavier_normal_(self.weight)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 / np.sqrt(fan_in)\n        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n\n    def compute_t(self):\n        q = torch.exp(self.q)\n        q_inv = torch.exp(-self.q)\n        t = torch.abs(\n            torch.einsum(\"i,ik,kj,j -&gt; ij\", q_inv, self.weight, self.weight.T, q)\n        ).sum(1)\n        t = safe_inv(t)\n        return t\n\n    def forward(self, x):\n        t = self.compute_t()\n        res = F.linear(x, self.weight)\n        res = res + self.bias\n        res = t * self.activation(res)\n        res = 2 * F.linear(res, self.weight.T)\n        out = x - res\n        return out\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense.__init__","title":"<code>__init__(in_features, out_features, inner_dim, **kwargs)</code>","text":"<p>A 1-Lipschitz fully-connected layer (dense version). Similar to the convolutional SLL approach, but operates on vectors:</p> \\[ y = x - K^T \\times (t \\times \\sigma(K \\times x + b)), \\] <p>Args:   - <code>in_features</code> (int): Input size.   - <code>out_features</code> (int): Output size (must match <code>in_features</code> to remain 1-Lipschitz).   - <code>inner_dim</code> (int): The internal dimension used for the transform.</p> References <ul> <li>Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B. A Unified Algebraic Perspective on Lipschitz Neural Networks. In The Eleventh International Conference on Learning Representations. https://arxiv.org/abs/2303.03169</li> </ul> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>def __init__(self, in_features, out_features, inner_dim, **kwargs):\n    \"\"\"\n    A 1-Lipschitz fully-connected layer (dense version). Similar to the convolutional\n    SLL approach, but operates on vectors:\n\n    $$\n    y = x - K^T \\\\times (t \\\\times \\sigma(K \\\\times x + b)),\n    $$\n\n    **Args**:\n      - `in_features` (int): Input size.\n      - `out_features` (int): Output size (must match `in_features` to remain 1-Lipschitz).\n      - `inner_dim` (int): The internal dimension used for the transform.\n\n\n    References:\n        - Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.\n        A Unified Algebraic Perspective on Lipschitz Neural Networks.\n        In The Eleventh International Conference on Learning Representations.\n        &lt;https://arxiv.org/abs/2303.03169&gt;\n    \"\"\"\n    super().__init__()\n\n    inner_dim = inner_dim if inner_dim != -1 else in_features\n    self.activation = nn.ReLU()\n\n    self.weight = nn.Parameter(torch.empty(inner_dim, in_features))\n    self.bias = nn.Parameter(torch.empty(1, inner_dim))\n    self.q = nn.Parameter(torch.randn(inner_dim))\n\n    nn.init.xavier_normal_(self.weight)\n    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n    bound = 1 / np.sqrt(fan_in)\n    nn.init.uniform_(self.bias, -bound, bound)  # bias init\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock","title":"<code>SDPBasedLipschitzResBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>class SDPBasedLipschitzResBlock(nn.Module):\n    def __init__(self, cin, inner_dim_factor, kernel_size=3, groups=1, **kwargs):\n        \"\"\"\n         Original 1-Lipschitz convolutional residual block, based on the SDP-based Lipschitz\n        layer (SLL) approach [1]. It has a structure akin to:\n\n        out = x - 2 * ConvTranspose( t * ReLU(Conv(x) + bias) )\n\n        where `t` is a channel-wise scaling factor ensuring a Lipschitz constant \u2264 1.\n\n        !!! note\n            By default, `SDPBasedLipschitzResBlock` assumes `cin == cout` and does **not** handle\n            stride changes outside the skip connection (i.e., typically used when stride=1 or 2\n            for downsampling in a standard residual architecture).\n\n        **Args**:\n          - `cin` (int): Number of input channels.\n          - `cout` (int): Number of output channels.\n          - `inner_dim_factor` (float): Multiplier for the intermediate dimensionality.\n          - `kernel_size` (int, optional): Size of the convolution kernel. Default is 3.\n          - `groups` (int, optional): Number of groups for the convolution. Default is 1.\n          - `**kwargs`: Additional keyword arguments (unused).\n\n\n        References:\n            - Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.\n            A Unified Algebraic Perspective on Lipschitz Neural Networks.\n            In The Eleventh International Conference on Learning Representations.\n            &lt;https://arxiv.org/abs/2303.03169&gt;\n        \"\"\"\n        super().__init__()\n\n        inner_dim = int(cin * inner_dim_factor)\n        self.activation = nn.ReLU()\n        self.groups = groups\n\n        self.padding = kernel_size // 2\n\n        self.kernel = nn.Parameter(\n            torch.randn(inner_dim, cin // groups, kernel_size, kernel_size)\n        )\n        parametrize.register_parametrization(\n            self,\n            \"kernel\",\n            AOLReparametrizer(\n                inner_dim,\n                groups=groups,\n            ),\n        )\n        self.bias = nn.Parameter(torch.empty(1, inner_dim, 1, 1))\n        self.q = nn.Parameter(torch.ones(inner_dim, 1, 1, 1))\n\n        nn.init.xavier_normal_(self.kernel)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.kernel)\n        bound = 1 / np.sqrt(fan_in)\n        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n\n    def forward(self, x):\n        res = F.conv2d(x, self.kernel, padding=self.padding, groups=self.groups)\n        res = res + self.bias\n        res = self.activation(res)\n        with parametrize.cached():\n            res = 2 * F.conv_transpose2d(\n                res, self.kernel, padding=self.padding, groups=self.groups\n            )\n        out = x - res\n        return out\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock.__init__","title":"<code>__init__(cin, inner_dim_factor, kernel_size=3, groups=1, **kwargs)</code>","text":"<p>Original 1-Lipschitz convolutional residual block, based on the SDP-based Lipschitz layer (SLL) approach [1]. It has a structure akin to:</p> <p>out = x - 2 * ConvTranspose( t * ReLU(Conv(x) + bias) )</p> <p>where <code>t</code> is a channel-wise scaling factor ensuring a Lipschitz constant \u2264 1.</p> <p>Note</p> <p>By default, <code>SDPBasedLipschitzResBlock</code> assumes <code>cin == cout</code> and does not handle stride changes outside the skip connection (i.e., typically used when stride=1 or 2 for downsampling in a standard residual architecture).</p> <p>Args:   - <code>cin</code> (int): Number of input channels.   - <code>cout</code> (int): Number of output channels.   - <code>inner_dim_factor</code> (float): Multiplier for the intermediate dimensionality.   - <code>kernel_size</code> (int, optional): Size of the convolution kernel. Default is 3.   - <code>groups</code> (int, optional): Number of groups for the convolution. Default is 1.   - <code>**kwargs</code>: Additional keyword arguments (unused).</p> References <ul> <li>Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B. A Unified Algebraic Perspective on Lipschitz Neural Networks. In The Eleventh International Conference on Learning Representations. https://arxiv.org/abs/2303.03169</li> </ul> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>def __init__(self, cin, inner_dim_factor, kernel_size=3, groups=1, **kwargs):\n    \"\"\"\n     Original 1-Lipschitz convolutional residual block, based on the SDP-based Lipschitz\n    layer (SLL) approach [1]. It has a structure akin to:\n\n    out = x - 2 * ConvTranspose( t * ReLU(Conv(x) + bias) )\n\n    where `t` is a channel-wise scaling factor ensuring a Lipschitz constant \u2264 1.\n\n    !!! note\n        By default, `SDPBasedLipschitzResBlock` assumes `cin == cout` and does **not** handle\n        stride changes outside the skip connection (i.e., typically used when stride=1 or 2\n        for downsampling in a standard residual architecture).\n\n    **Args**:\n      - `cin` (int): Number of input channels.\n      - `cout` (int): Number of output channels.\n      - `inner_dim_factor` (float): Multiplier for the intermediate dimensionality.\n      - `kernel_size` (int, optional): Size of the convolution kernel. Default is 3.\n      - `groups` (int, optional): Number of groups for the convolution. Default is 1.\n      - `**kwargs`: Additional keyword arguments (unused).\n\n\n    References:\n        - Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.\n        A Unified Algebraic Perspective on Lipschitz Neural Networks.\n        In The Eleventh International Conference on Learning Representations.\n        &lt;https://arxiv.org/abs/2303.03169&gt;\n    \"\"\"\n    super().__init__()\n\n    inner_dim = int(cin * inner_dim_factor)\n    self.activation = nn.ReLU()\n    self.groups = groups\n\n    self.padding = kernel_size // 2\n\n    self.kernel = nn.Parameter(\n        torch.randn(inner_dim, cin // groups, kernel_size, kernel_size)\n    )\n    parametrize.register_parametrization(\n        self,\n        \"kernel\",\n        AOLReparametrizer(\n            inner_dim,\n            groups=groups,\n        ),\n    )\n    self.bias = nn.Parameter(torch.empty(1, inner_dim, 1, 1))\n    self.q = nn.Parameter(torch.ones(inner_dim, 1, 1, 1))\n\n    nn.init.xavier_normal_(self.kernel)\n    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.kernel)\n    bound = 1 / np.sqrt(fan_in)\n    nn.init.uniform_(self.bias, -bound, bound)  # bias init\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock","title":"<code>SLLxAOCLipschitzResBlock</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>class SLLxAOCLipschitzResBlock(nn.Module):\n    def __init__(\n        self, cin, cout, inner_dim_factor, kernel_size=3, stride=2, groups=1, **kwargs\n    ):\n        \"\"\"\n        Extended SLL-based convolutional residual block. Supports arbitrary kernel sizes,\n        strides, and changes in the number of channels by integrating additional\n        orthogonal convolutions *and* fusing them via `\\mathbconv` [1].\n\n        The forward pass follows:\n\n        $$\n        y = (\\mathbf{K}_{post} \\circledast \\mathbf{K}_{pre}) \\\\star x - (\\mathbf{K}_{post} \\circledast \\mathbf{K}^T) \\\\star (t \\\\times  \\sigma(( \\mathbf{K} \\circledast \\mathbf{K}_{pre}) \\\\star x + b)),\n        $$\n\n        where $\\mathbf{K}_{pre}$ and $\\mathbf{K}_{post}$ are obtained with AOC.\n\n\n        &lt;img src=\"../../assets/SLL_3.png\" alt=\"illustration of SLL x AOC\" width=\"600\"&gt;\n\n\n\n        where the kernel `\\kernel{K}` may effectively be expanded by pre/post AOC layers to\n        handle stride and channel changes. This approach is described in \"Improving\n        SDP-based Lipschitz Layers\" of [1].\n\n        **Args**:\n          - `cin` (int): Number of input channels.\n          - `inner_dim_factor` (float): Multiplier for the internal channel dimension.\n          - `kernel_size` (int, optional): Base kernel size for the SLL portion. Default is 3.\n          - `stride` (int, optional): Stride for the skip connection. Default is 2.\n          - `groups` (int, optional): Number of groups for the convolution. Default is 1.\n          - `**kwargs`: Additional options (unused).\n\n\n\n        References:\n            - Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n            An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n            &lt;https://arxiv.org/abs/2501.07930&gt;\n        \"\"\"\n        super().__init__()\n        inner_kernel_size = kernel_size - (stride - 1)\n        self.skip_kernel_size = stride + (stride // 2)\n        inner_dim = int(cout * inner_dim_factor)\n        self.activation = nn.ReLU()\n        self.stride = stride\n        self.groups = groups\n        self.padding = kernel_size // 2\n        self.kernel = nn.Parameter(\n            torch.randn(\n                inner_dim, cin // self.groups, inner_kernel_size, inner_kernel_size\n            )\n        )\n        parametrize.register_parametrization(\n            self,\n            \"kernel\",\n            AOLReparametrizer(\n                inner_dim,\n                groups=groups,\n            ),\n        )\n        self.bias = nn.Parameter(torch.empty(1, inner_dim, 1, 1))\n        self.q = nn.Parameter(torch.ones(inner_dim, 1, 1, 1))\n\n        nn.init.xavier_normal_(self.kernel)\n        fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.kernel)\n        bound = 1 / np.sqrt(fan_in)\n        nn.init.uniform_(self.bias, -bound, bound)  # bias init\n\n        self.pre_conv = AdaptiveOrthoConv2d(\n            cin, cin, kernel_size=stride, stride=1, bias=False, padding=0, groups=groups\n        )\n        self.post_conv = AdaptiveOrthoConv2d(\n            cin,\n            cout,\n            kernel_size=stride,\n            stride=stride,\n            bias=False,\n            padding=0,\n            groups=groups,\n        )\n\n    def forward(self, x):\n        # compute t\n        # print(self.pre_conv.weight.shape, self.kernel.shape, self.post_conv.weight.shape)\n        kernel_1a = fast_matrix_conv(\n            self.pre_conv.weight, self.kernel, groups=self.groups\n        )\n        with parametrize.cached():\n            kernel_1b = fast_matrix_conv(\n                transpose_kernel(self.kernel, groups=self.groups),\n                self.post_conv.weight,\n                groups=self.groups,\n            )\n            kernel_2 = fast_matrix_conv(\n                self.pre_conv.weight, self.post_conv.weight, groups=self.groups\n            )\n            # first branch\n            # fuse pre conv with kernel\n            res = F.conv2d(x, kernel_1a, padding=self.padding, groups=self.groups)\n            res = res + self.bias\n            res = self.activation(res)\n            res = 2 * F.conv2d(\n                res,\n                kernel_1b,\n                padding=self.padding,\n                stride=self.stride,\n                groups=self.groups,\n            )\n            # residual branch\n            x = F.conv2d(\n                x,\n                kernel_2,\n                padding=self.skip_kernel_size // 2,\n                stride=self.stride,\n                groups=self.groups,\n            )\n        # skip connection\n        out = x - res\n        return out\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock.__init__","title":"<code>__init__(cin, cout, inner_dim_factor, kernel_size=3, stride=2, groups=1, **kwargs)</code>","text":"<p>Extended SLL-based convolutional residual block. Supports arbitrary kernel sizes, strides, and changes in the number of channels by integrating additional orthogonal convolutions and fusing them via <code>\\mathbconv</code> [1].</p> <p>The forward pass follows:</p> \\[ y = (\\mathbf{K}_{post} \\circledast \\mathbf{K}_{pre}) \\star x - (\\mathbf{K}_{post} \\circledast \\mathbf{K}^T) \\star (t \\times  \\sigma(( \\mathbf{K} \\circledast \\mathbf{K}_{pre}) \\star x + b)), \\] <p>where \\(\\mathbf{K}_{pre}\\) and \\(\\mathbf{K}_{post}\\) are obtained with AOC.</p> <p></p> <p>where the kernel <code>\\kernel{K}</code> may effectively be expanded by pre/post AOC layers to handle stride and channel changes. This approach is described in \"Improving SDP-based Lipschitz Layers\" of [1].</p> <p>Args:   - <code>cin</code> (int): Number of input channels.   - <code>inner_dim_factor</code> (float): Multiplier for the internal channel dimension.   - <code>kernel_size</code> (int, optional): Base kernel size for the SLL portion. Default is 3.   - <code>stride</code> (int, optional): Stride for the skip connection. Default is 2.   - <code>groups</code> (int, optional): Number of groups for the convolution. Default is 1.   - <code>**kwargs</code>: Additional options (unused).</p> References <ul> <li>Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025). An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures. https://arxiv.org/abs/2501.07930</li> </ul> Source code in <code>orthogonium\\layers\\conv\\SLL\\sll_layer.py</code> <pre><code>def __init__(\n    self, cin, cout, inner_dim_factor, kernel_size=3, stride=2, groups=1, **kwargs\n):\n    \"\"\"\n    Extended SLL-based convolutional residual block. Supports arbitrary kernel sizes,\n    strides, and changes in the number of channels by integrating additional\n    orthogonal convolutions *and* fusing them via `\\mathbconv` [1].\n\n    The forward pass follows:\n\n    $$\n    y = (\\mathbf{K}_{post} \\circledast \\mathbf{K}_{pre}) \\\\star x - (\\mathbf{K}_{post} \\circledast \\mathbf{K}^T) \\\\star (t \\\\times  \\sigma(( \\mathbf{K} \\circledast \\mathbf{K}_{pre}) \\\\star x + b)),\n    $$\n\n    where $\\mathbf{K}_{pre}$ and $\\mathbf{K}_{post}$ are obtained with AOC.\n\n\n    &lt;img src=\"../../assets/SLL_3.png\" alt=\"illustration of SLL x AOC\" width=\"600\"&gt;\n\n\n\n    where the kernel `\\kernel{K}` may effectively be expanded by pre/post AOC layers to\n    handle stride and channel changes. This approach is described in \"Improving\n    SDP-based Lipschitz Layers\" of [1].\n\n    **Args**:\n      - `cin` (int): Number of input channels.\n      - `inner_dim_factor` (float): Multiplier for the internal channel dimension.\n      - `kernel_size` (int, optional): Base kernel size for the SLL portion. Default is 3.\n      - `stride` (int, optional): Stride for the skip connection. Default is 2.\n      - `groups` (int, optional): Number of groups for the convolution. Default is 1.\n      - `**kwargs`: Additional options (unused).\n\n\n\n    References:\n        - Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n        &lt;https://arxiv.org/abs/2501.07930&gt;\n    \"\"\"\n    super().__init__()\n    inner_kernel_size = kernel_size - (stride - 1)\n    self.skip_kernel_size = stride + (stride // 2)\n    inner_dim = int(cout * inner_dim_factor)\n    self.activation = nn.ReLU()\n    self.stride = stride\n    self.groups = groups\n    self.padding = kernel_size // 2\n    self.kernel = nn.Parameter(\n        torch.randn(\n            inner_dim, cin // self.groups, inner_kernel_size, inner_kernel_size\n        )\n    )\n    parametrize.register_parametrization(\n        self,\n        \"kernel\",\n        AOLReparametrizer(\n            inner_dim,\n            groups=groups,\n        ),\n    )\n    self.bias = nn.Parameter(torch.empty(1, inner_dim, 1, 1))\n    self.q = nn.Parameter(torch.ones(inner_dim, 1, 1, 1))\n\n    nn.init.xavier_normal_(self.kernel)\n    fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.kernel)\n    bound = 1 / np.sqrt(fan_in)\n    nn.init.uniform_(self.bias, -bound, bound)  # bias init\n\n    self.pre_conv = AdaptiveOrthoConv2d(\n        cin, cin, kernel_size=stride, stride=1, bias=False, padding=0, groups=groups\n    )\n    self.post_conv = AdaptiveOrthoConv2d(\n        cin,\n        cout,\n        kernel_size=stride,\n        stride=stride,\n        bias=False,\n        padding=0,\n        groups=groups,\n    )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOL.aol.AOLConv2D","title":"<code>AOLConv2D</code>","text":"<p>               Bases: <code>Conv2d</code></p> Source code in <code>orthogonium\\layers\\conv\\AOL\\aol.py</code> <pre><code>class AOLConv2D(nn.Conv2d):\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        groups=1,\n        bias=True,\n        padding_mode=\"zeros\",\n        device=None,\n        dtype=None,\n        niter=1,\n    ):\n        \"\"\"\n        Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce\n        almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer\n        is guaranteed to be less than 1.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            kernel_size (int or tuple): Size of the convolution kernel.\n            stride (int or tuple, optional): Stride of the convolution. Default is 1.\n            padding (int or tuple, optional): Padding size. Default is 0.\n            dilation (int or tuple, optional): Dilation rate. Default is 1.\n            groups (int, optional): Number of groups. Default is 1.\n            bias (bool, optional): Whether to include a learnable bias. Default is True.\n            padding_mode (str, optional): Padding mode. Default is \"zeros\".\n            device (torch.device, optional): Device to store the layer parameters. Default is None.\n            dtype (torch.dtype, optional): Data type to store the layer parameters. Default is None.\n\n\n        References:\n            `[1] Prach, B., &amp; Lampert, C. H. (2022).\n                   \"Almost-orthogonal layers for efficient general-purpose lipschitz networks.\"\n                   ECCV.`&lt;https://arxiv.org/abs/2208.03160&gt;`_\n        \"\"\"\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias,\n            padding_mode=padding_mode,\n            device=device,\n            dtype=dtype,\n        )\n        self.niter = niter\n\n        parametrize.register_parametrization(\n            self,\n            \"weight\",\n            MultiStepAOLReparametrizer(\n                min(out_channels, in_channels),\n                groups=groups,\n                niter=niter,\n            ),\n        )\n\n    def reset_parameters(self) -&gt; None:\n        r\"\"\"Resets parameters of the module. This includes the weight and bias\n        parameters, if they are used.\n        \"\"\"\n        super().reset_parameters()\n        # # Reset the parametrization\n        # init kernel using the orthogonal kernel\n        if not (\n            self.in_channels // self.groups == 0\n            and self.out_channels // self.groups == 0\n        ):\n            self.kernel = conv_orthogonal_(\n                self.weight,\n                stride=self.stride,\n                groups=self.groups,\n            )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOL.aol.AOLConv2D.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None, niter=1)</code>","text":"<p>Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer is guaranteed to be less than 1.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>int or tuple</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>int or tuple</code> <p>Stride of the convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>int or tuple</code> <p>Padding size. Default is 0.</p> <code>0</code> <code>dilation</code> <code>int or tuple</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of groups. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"zeros\".</p> <code>'zeros'</code> <code>device</code> <code>device</code> <p>Device to store the layer parameters. Default is None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type to store the layer parameters. Default is None.</p> <code>None</code> References <p><code>[1] Prach, B., &amp; Lampert, C. H. (2022).        \"Almost-orthogonal layers for efficient general-purpose lipschitz networks.\"        ECCV.</code>https://arxiv.org/abs/2208.03160`_</p> Source code in <code>orthogonium\\layers\\conv\\AOL\\aol.py</code> <pre><code>def __init__(\n    self,\n    in_channels,\n    out_channels,\n    kernel_size,\n    stride=1,\n    padding=0,\n    dilation=1,\n    groups=1,\n    bias=True,\n    padding_mode=\"zeros\",\n    device=None,\n    dtype=None,\n    niter=1,\n):\n    \"\"\"\n    Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce\n    almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer\n    is guaranteed to be less than 1.\n\n    Args:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (int or tuple): Size of the convolution kernel.\n        stride (int or tuple, optional): Stride of the convolution. Default is 1.\n        padding (int or tuple, optional): Padding size. Default is 0.\n        dilation (int or tuple, optional): Dilation rate. Default is 1.\n        groups (int, optional): Number of groups. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        padding_mode (str, optional): Padding mode. Default is \"zeros\".\n        device (torch.device, optional): Device to store the layer parameters. Default is None.\n        dtype (torch.dtype, optional): Data type to store the layer parameters. Default is None.\n\n\n    References:\n        `[1] Prach, B., &amp; Lampert, C. H. (2022).\n               \"Almost-orthogonal layers for efficient general-purpose lipschitz networks.\"\n               ECCV.`&lt;https://arxiv.org/abs/2208.03160&gt;`_\n    \"\"\"\n    super().__init__(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        dilation=dilation,\n        groups=groups,\n        bias=bias,\n        padding_mode=padding_mode,\n        device=device,\n        dtype=dtype,\n    )\n    self.niter = niter\n\n    parametrize.register_parametrization(\n        self,\n        \"weight\",\n        MultiStepAOLReparametrizer(\n            min(out_channels, in_channels),\n            groups=groups,\n            niter=niter,\n        ),\n    )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOL.aol.AOLConv2D.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Resets parameters of the module. This includes the weight and bias parameters, if they are used.</p> Source code in <code>orthogonium\\layers\\conv\\AOL\\aol.py</code> <pre><code>def reset_parameters(self) -&gt; None:\n    r\"\"\"Resets parameters of the module. This includes the weight and bias\n    parameters, if they are used.\n    \"\"\"\n    super().reset_parameters()\n    # # Reset the parametrization\n    # init kernel using the orthogonal kernel\n    if not (\n        self.in_channels // self.groups == 0\n        and self.out_channels // self.groups == 0\n    ):\n        self.kernel = conv_orthogonal_(\n            self.weight,\n            stride=self.stride,\n            groups=self.groups,\n        )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D","title":"<code>AOLConvTranspose2D</code>","text":"<p>               Bases: <code>ConvTranspose2d</code></p> Source code in <code>orthogonium\\layers\\conv\\AOL\\aol.py</code> <pre><code>class AOLConvTranspose2D(nn.ConvTranspose2d):\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        output_padding=0,\n        groups=1,\n        bias=True,\n        dilation=1,\n        padding_mode=\"zeros\",\n        device=None,\n        dtype=None,\n        niter=1,\n    ):\n        \"\"\"\n        Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce\n        almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer\n        is guaranteed to be less than 1.\n\n        Args:\n            in_channels (int): Number of input channels.\n            out_channels (int): Number of output channels.\n            kernel_size (int or tuple): Size of the convolution kernel.\n            stride (int or tuple, optional): Stride of the convolution. Default is 1.\n            padding (int or tuple, optional): Padding size. Default is 0.\n            output_padding (int or tuple, optional): Additional size added to the output shape. Default is 0.\n            groups (int, optional): Number of groups. Default is 1.\n            bias (bool, optional): Whether to include a learnable bias. Default is True.\n            dilation (int or tuple, optional): Dilation rate. Default is 1.\n            padding_mode (str, optional): Padding mode. Default is \"zeros\".\n            device (torch.device, optional): Device to store the layer parameters. Default is None.\n            dtype (torch.dtype, optional): Data type to store the layer parameters. Default is None.\n\n\n        References:\n            `[1] Prach, B., &amp; Lampert, C. H. (2022).\n                   \"Almost-orthogonal layers for efficient general-purpose lipschitz networks.\"\n                   ECCV.`&lt;https://arxiv.org/abs/2208.03160&gt;`_\n        \"\"\"\n        super().__init__(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            output_padding=output_padding,\n            groups=groups,\n            bias=bias,\n            dilation=dilation,\n            padding_mode=padding_mode,\n            device=device,\n            dtype=dtype,\n        )\n        self.niter = niter\n\n        # Register the same AOLReparametrizer\n        parametrize.register_parametrization(\n            self,\n            \"weight\",\n            MultiStepAOLReparametrizer(\n                min(out_channels, in_channels), groups=groups, niter=niter\n            ),\n        )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D.__init__","title":"<code>__init__(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None, niter=1)</code>","text":"<p>Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer is guaranteed to be less than 1.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>int or tuple</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>int or tuple</code> <p>Stride of the convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>int or tuple</code> <p>Padding size. Default is 0.</p> <code>0</code> <code>output_padding</code> <code>int or tuple</code> <p>Additional size added to the output shape. Default is 0.</p> <code>0</code> <code>groups</code> <code>int</code> <p>Number of groups. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>dilation</code> <code>int or tuple</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"zeros\".</p> <code>'zeros'</code> <code>device</code> <code>device</code> <p>Device to store the layer parameters. Default is None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type to store the layer parameters. Default is None.</p> <code>None</code> References <p><code>[1] Prach, B., &amp; Lampert, C. H. (2022).        \"Almost-orthogonal layers for efficient general-purpose lipschitz networks.\"        ECCV.</code>https://arxiv.org/abs/2208.03160`_</p> Source code in <code>orthogonium\\layers\\conv\\AOL\\aol.py</code> <pre><code>def __init__(\n    self,\n    in_channels,\n    out_channels,\n    kernel_size,\n    stride=1,\n    padding=0,\n    output_padding=0,\n    groups=1,\n    bias=True,\n    dilation=1,\n    padding_mode=\"zeros\",\n    device=None,\n    dtype=None,\n    niter=1,\n):\n    \"\"\"\n    Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce\n    almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer\n    is guaranteed to be less than 1.\n\n    Args:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (int or tuple): Size of the convolution kernel.\n        stride (int or tuple, optional): Stride of the convolution. Default is 1.\n        padding (int or tuple, optional): Padding size. Default is 0.\n        output_padding (int or tuple, optional): Additional size added to the output shape. Default is 0.\n        groups (int, optional): Number of groups. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        dilation (int or tuple, optional): Dilation rate. Default is 1.\n        padding_mode (str, optional): Padding mode. Default is \"zeros\".\n        device (torch.device, optional): Device to store the layer parameters. Default is None.\n        dtype (torch.dtype, optional): Data type to store the layer parameters. Default is None.\n\n\n    References:\n        `[1] Prach, B., &amp; Lampert, C. H. (2022).\n               \"Almost-orthogonal layers for efficient general-purpose lipschitz networks.\"\n               ECCV.`&lt;https://arxiv.org/abs/2208.03160&gt;`_\n    \"\"\"\n    super().__init__(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=kernel_size,\n        stride=stride,\n        padding=padding,\n        output_padding=output_padding,\n        groups=groups,\n        bias=bias,\n        dilation=dilation,\n        padding_mode=padding_mode,\n        device=device,\n        dtype=dtype,\n    )\n    self.niter = niter\n\n    # Register the same AOLReparametrizer\n    parametrize.register_parametrization(\n        self,\n        \"weight\",\n        MultiStepAOLReparametrizer(\n            min(out_channels, in_channels), groups=groups, niter=niter\n        ),\n    )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer","title":"<code>MultiStepAOLReparametrizer</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\layers\\conv\\AOL\\aol.py</code> <pre><code>class MultiStepAOLReparametrizer(nn.Module):\n    def __init__(self, nb_features, groups, niter=4):\n        super(MultiStepAOLReparametrizer, self).__init__()\n        self.groups = groups\n        self.nb_features = nb_features\n        self.niter = niter\n        self.q = nn.Parameter(torch.ones(nb_features, 1, 1, 1))\n\n    def forward(self, kernel):\n        co, cig, ks, ks2 = kernel.shape\n        if co // self.groups &gt;= cig:\n            kernel = transpose_kernel(kernel, self.groups, flip=True)\n        kkt = kernel\n        log_curr_norm = 0\n        for i in range(self.niter):\n            kkt_norm = kkt.norm().detach()\n            kkt = kkt / kkt_norm\n            log_curr_norm = 2 * (log_curr_norm + kkt_norm.log())\n            kkt = fast_matrix_conv(\n                transpose_kernel(kkt, self.groups, flip=True), kkt, self.groups\n            )\n\n        inverse_power = 2 ** (-self.niter)\n        t = torch.abs(kkt)\n        q = torch.exp(self.q)\n        q_inv = torch.exp(-self.q)\n        t = q_inv * t * q\n        t = t.sum((1, 2, 3)).pow(inverse_power)\n        norm = torch.exp(log_curr_norm * inverse_power)\n        t = t * norm\n        t = t.reshape(-1, 1, 1, 1)\n        kernel = kernel / t\n        if co // self.groups &gt;= cig:\n            kernel = transpose_kernel(kernel, self.groups, flip=True)\n        return kernel\n\n    def right_inverse(self, kernel):\n        return kernel\n\n    def reset_parameters(self):\n        \"\"\"\n        Resets the parameters of the reparametrizer.\n        \"\"\"\n        # Reset the q parameter to its initial value\n        self.q.data.fill_(1.0)\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer.reset_parameters","title":"<code>reset_parameters()</code>","text":"<p>Resets the parameters of the reparametrizer.</p> Source code in <code>orthogonium\\layers\\conv\\AOL\\aol.py</code> <pre><code>def reset_parameters(self):\n    \"\"\"\n    Resets the parameters of the reparametrizer.\n    \"\"\"\n    # Reset the q parameter to its initial value\n    self.q.data.fill_(1.0)\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d","title":"<code>AdaptiveSOCConv2d(in_channels, out_channels, kernel_size, stride=1, padding='same', dilation=1, groups=1, bias=True, padding_mode='circular', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel size and stride. This is a modified implementation of the <code>Skew orthogonal convolution</code> [1], with significant modification from the original paper:</p> <ul> <li>This implementation provide an explicit kernel (which is larger the original kernel size) so the forward is done     in a single iteration. As described in [2].</li> <li>This implementation avoid the use of channels padding to handle case where cin != cout. Similarly, stride is     handled natively using the ad adaptive scheme.</li> <li>the fantastic four method is replaced by AOL which allows to reduce the number of iterations required to     converge.</li> </ul> <p>It aims to be more scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers. This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</p> Note <ul> <li>this implementation changes the size of the kernel, which also change the padding semantics. Please adjust     the padding according to the kernel size and the number of iterations.</li> <li>current unit testing use a tolerance of 8e-2 sor this layer can be expected to be 1.08 lipschitz continuous.     Similarly, the stable rank is evaluated loosely (must be greater than 0.5).</li> </ul>"},{"location":"api/conv/#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--key-features","title":"Key Features:","text":"<pre><code>- Enforces orthogonality, preserving gradient norms.\n- Supports native striding, dilation, grouped convolutions, and flexible padding.\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RKOConv2d`.\n- When stride == 1, the layer is a `FastBlockConv2d`.\n- Otherwise, the layer is a `BcopRkoConv2d`.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>str or _size_2_t</code> <p>Padding mode or size. Default is \"same\".</p> <code>'same'</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input to output channels. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"circular\".</p> <code>'circular'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>Conv2d</code> <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p> <p>Raises:</p> Type Description <code>`ValueError`</code> <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> References <ul> <li>[1] Singla, S., &amp; Feizi, S. (2021, July). Skew orthogonal convolutions. In International Conference on Machine Learning (pp. 9756-9766). PMLR.https://arxiv.org/abs/2105.11417</li> <li>[2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025). An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures. https://arxiv.org/abs/2501.07930</li> </ul> Source code in <code>orthogonium\\layers\\conv\\adaptiveSOC\\ortho_conv.py</code> <pre><code>def AdaptiveSOCConv2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: Union[str, _size_2_t] = \"same\",\n    dilation: _size_2_t = 1,\n    groups: int = 1,\n    bias: bool = True,\n    padding_mode: str = \"circular\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.Conv2d:\n    \"\"\"\n    Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel\n    size and stride. This is a modified implementation of the `Skew orthogonal convolution` [1], with significant\n    modification from the original paper:\n\n\n    - This implementation provide an explicit kernel (which is larger the original kernel size) so the forward is done\n        in a single iteration. As described in [2].\n    - This implementation avoid the use of channels padding to handle case where cin != cout. Similarly, stride is\n        handled natively using the ad adaptive scheme.\n    - the fantastic four method is replaced by AOL which allows to reduce the number of iterations required to\n        converge.\n\n    It aims to be more scalable to large networks and large image sizes, while enforcing orthogonality in the\n    convolutional layers. This layer also intend to be compatible with all the feature of the `nn.Conv2d` class\n    (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward\n    operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.\n\n    Note:\n        - this implementation changes the size of the kernel, which also change the padding semantics. Please adjust\n            the padding according to the kernel size and the number of iterations.\n        - current unit testing use a tolerance of 8e-2 sor this layer can be expected to be 1.08 lipschitz continuous.\n            Similarly, the stable rank is evaluated loosely (must be greater than 0.5).\n\n    Key Features:\n    -------------\n        - Enforces orthogonality, preserving gradient norms.\n        - Supports native striding, dilation, grouped convolutions, and flexible padding.\n\n    Behavior:\n    -------------\n        - When kernel_size == stride, the layer is an `RKOConv2d`.\n        - When stride == 1, the layer is a `FastBlockConv2d`.\n        - Otherwise, the layer is a `BcopRkoConv2d`.\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the convolution. Default is 1.\n        padding (str or _size_2_t, optional): Padding mode or size. Default is \"same\".\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        padding_mode (str, optional): Padding mode. Default is \"circular\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).\n\n    Raises:\n        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n\n\n    References:\n        - [1] Singla, S., &amp; Feizi, S. (2021, July). Skew orthogonal convolutions. In International Conference\n        on Machine Learning (pp. 9756-9766). PMLR.&lt;https://arxiv.org/abs/2105.11417&gt;\n        - [2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n        &lt;https://arxiv.org/abs/2501.07930&gt;\n    \"\"\"\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RKOConv2d\n    elif stride == 1:\n        convclass = FastSOC\n    else:\n        convclass = SOCRkoConv2d\n    return convclass(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        groups,\n        bias,\n        padding_mode,\n        # ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d","title":"<code>AdaptiveSOCConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', ortho_params=OrthoParams())</code>","text":"<p>Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on kernel size and stride. This is a modified implementation of the <code>Skew orthogonal convolution</code> [1], with significant modification from the original paper:</p> <ul> <li>This implementation provide an explicit kernel (which is larger the original kernel size) so the forward is done     in a single iteration. As described in [2].</li> <li>This implementation avoid the use of channels padding to handle case where cin != cout. Similarly, stride is     handled natively using the ad adaptive scheme.</li> <li>the fantastic four method is replaced by AOL which allows to reduce the number of iterations required to     converge.</li> </ul> <p>It aims to be more scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers. This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</p> Note <ul> <li>this implementation changes the size of the kernel, which also change the padding semantics. Please adjust     the padding according to the kernel size and the number of iterations.</li> <li>current unit testing use a tolerance of 8e-2 sor this layer can be expected to be 1.08 lipschitz continuous.     Similarly, the stable rank is evaluated loosely (must be greater than 0.5).</li> </ul>"},{"location":"api/conv/#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--key-features","title":"Key Features:","text":"<pre><code>- Enforces orthogonality, preserving gradient norms.\n- Supports native striding, dilation, grouped convolutions, and flexible padding.\n</code></pre>"},{"location":"api/conv/#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--behavior","title":"Behavior:","text":"<pre><code>- When kernel_size == stride, the layer is an `RKOConv2d`.\n- When stride == 1, the layer is a `FastBlockConv2d`.\n- Otherwise, the layer is a `BcopRkoConv2d`.\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int</code> <p>Number of input channels.</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels.</p> required <code>kernel_size</code> <code>_size_2_t</code> <p>Size of the convolution kernel.</p> required <code>stride</code> <code>_size_2_t</code> <p>Stride of the convolution. Default is 1.</p> <code>1</code> <code>padding</code> <code>str or _size_2_t</code> <p>Padding mode or size. Default is \"same\".</p> <code>0</code> <code>dilation</code> <code>_size_2_t</code> <p>Dilation rate. Default is 1.</p> <code>1</code> <code>groups</code> <code>int</code> <p>Number of blocked connections from input to output channels. Default is 1.</p> <code>1</code> <code>bias</code> <code>bool</code> <p>Whether to include a learnable bias. Default is True.</p> <code>True</code> <code>padding_mode</code> <code>str</code> <p>Padding mode. Default is \"circular\".</p> <code>'zeros'</code> <code>ortho_params</code> <code>OrthoParams</code> <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p> <code>OrthoParams()</code> <p>Returns:</p> Type Description <code>ConvTranspose2d</code> <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p> <p>Raises:</p> Type Description <code>`ValueError`</code> <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p> References <ul> <li>[1] Singla, S., &amp; Feizi, S. (2021, July). Skew orthogonal convolutions. In International Conference on Machine Learning (pp. 9756-9766). PMLR.https://arxiv.org/abs/2105.11417</li> <li>[2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025). An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures. https://arxiv.org/abs/2501.07930</li> </ul> Source code in <code>orthogonium\\layers\\conv\\adaptiveSOC\\ortho_conv.py</code> <pre><code>def AdaptiveSOCConvTranspose2d(\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_2_t,\n    stride: _size_2_t = 1,\n    padding: _size_2_t = 0,\n    output_padding: _size_2_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_2_t = 1,\n    padding_mode: str = \"zeros\",\n    ortho_params: OrthoParams = OrthoParams(),\n) -&gt; nn.ConvTranspose2d:\n    \"\"\"\n    Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on\n    kernel size and stride. This is a modified implementation of the `Skew orthogonal convolution` [1], with significant\n    modification from the original paper:\n\n    - This implementation provide an explicit kernel (which is larger the original kernel size) so the forward is done\n        in a single iteration. As described in [2].\n    - This implementation avoid the use of channels padding to handle case where cin != cout. Similarly, stride is\n        handled natively using the ad adaptive scheme.\n    - the fantastic four method is replaced by AOL which allows to reduce the number of iterations required to\n        converge.\n\n    It aims to be more scalable to large networks and large image sizes, while enforcing orthogonality in the\n    convolutional layers. This layer also intend to be compatible with all the feature of the `nn.Conv2d` class\n    (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward\n    operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.\n\n    Note:\n        - this implementation changes the size of the kernel, which also change the padding semantics. Please adjust\n            the padding according to the kernel size and the number of iterations.\n        - current unit testing use a tolerance of 8e-2 sor this layer can be expected to be 1.08 lipschitz continuous.\n            Similarly, the stable rank is evaluated loosely (must be greater than 0.5).\n\n    Key Features:\n    -------------\n        - Enforces orthogonality, preserving gradient norms.\n        - Supports native striding, dilation, grouped convolutions, and flexible padding.\n\n    Behavior:\n    -------------\n        - When kernel_size == stride, the layer is an `RKOConv2d`.\n        - When stride == 1, the layer is a `FastBlockConv2d`.\n        - Otherwise, the layer is a `BcopRkoConv2d`.\n\n    Arguments:\n        in_channels (int): Number of input channels.\n        out_channels (int): Number of output channels.\n        kernel_size (_size_2_t): Size of the convolution kernel.\n        stride (_size_2_t, optional): Stride of the convolution. Default is 1.\n        padding (str or _size_2_t, optional): Padding mode or size. Default is \"same\".\n        dilation (_size_2_t, optional): Dilation rate. Default is 1.\n        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.\n        bias (bool, optional): Whether to include a learnable bias. Default is True.\n        padding_mode (str, optional): Padding mode. Default is \"circular\".\n        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.\n\n    Returns:\n        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).\n\n    Raises:\n        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.\n\n\n    References:\n        - [1] Singla, S., &amp; Feizi, S. (2021, July). Skew orthogonal convolutions. In International Conference\n        on Machine Learning (pp. 9756-9766). PMLR.&lt;https://arxiv.org/abs/2105.11417&gt;\n        - [2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).\n        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.\n        &lt;https://arxiv.org/abs/2501.07930&gt;\n    \"\"\"\n    if kernel_size &lt; stride:\n        raise ValueError(\n            \"kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.\"\n        )\n    if kernel_size == stride:\n        convclass = RkoConvTranspose2d\n    elif stride == 1:\n        convclass = SOCTranspose\n    else:\n        convclass = SOCRkoConvTranspose2d\n    return convclass(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        output_padding,\n        groups,\n        bias,\n        dilation,\n        padding_mode,\n        # ortho_params=ortho_params,\n    )\n</code></pre>"},{"location":"api/linear/","title":"linear layers","text":""},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.OrthoLinear","title":"<code>OrthoLinear</code>","text":"<p>               Bases: <code>Linear</code></p> Source code in <code>orthogonium\\layers\\linear\\ortho_linear.py</code> <pre><code>class OrthoLinear(nn.Linear):\n    def __init__(\n        self,\n        in_features: int,\n        out_features: int,\n        bias: bool = True,\n        ortho_params: OrthoParams = OrthoParams(),\n    ):\n        \"\"\"\n        Initializes an orthogonal linear layer with customizable orthogonalization parameters.\n\n        Attributes:\n            in_features : int\n                Number of input features.\n            out_features : int\n                Number of output features.\n            bias : bool\n                Whether to include a bias term in the layer. Default is True.\n            ortho_params : OrthoParams\n                Parameters for orthogonalization and spectral normalization. Default is the\n                default instance of OrthoParams.\n\n        Parameters:\n            in_features : int\n                The size of each input sample.\n            out_features : int\n                The size of each output sample.\n            bias : bool\n                Indicates if the layer should include a learnable bias parameter.\n            ortho_params : OrthoParams\n                An object containing orthogonalization and normalization configurations.\n\n        Notes\n        -----\n        The layer is initialized with orthogonal weights using `torch.nn.init.orthogonal_`.\n        Weight parameters are further parametrized for both spectral normalization and\n        orthogonal constraints using the provided `OrthoParams` object.\n        \"\"\"\n        super(OrthoLinear, self).__init__(in_features, out_features, bias=bias)\n        torch.nn.init.orthogonal_(self.weight)\n        parametrize.register_parametrization(\n            self,\n            \"weight\",\n            ortho_params.spectral_normalizer(\n                weight_shape=(self.out_features, self.in_features)\n            ),\n        )\n        parametrize.register_parametrization(\n            self, \"weight\", ortho_params.orthogonalizer(weight_shape=self.weight.shape)\n        )\n\n    def singular_values(self):\n        svs = np.linalg.svd(\n            self.weight.detach().cpu().numpy(), full_matrices=False, compute_uv=False\n        )\n        stable_rank = np.sum((np.mean(svs) ** 2)) / (svs.max() ** 2)\n        return svs.min(), svs.max(), stable_rank\n</code></pre>"},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.OrthoLinear.__init__","title":"<code>__init__(in_features, out_features, bias=True, ortho_params=OrthoParams())</code>","text":"<p>Initializes an orthogonal linear layer with customizable orthogonalization parameters.</p> <p>Attributes:</p> Name Type Description <code>in_features</code> <p>int Number of input features.</p> <code>out_features</code> <p>int Number of output features.</p> <code>bias</code> <p>bool Whether to include a bias term in the layer. Default is True.</p> <code>ortho_params</code> <p>OrthoParams Parameters for orthogonalization and spectral normalization. Default is the default instance of OrthoParams.</p> <p>Parameters:</p> Name Type Description Default <code>in_features</code> <p>int The size of each input sample.</p> required <code>out_features</code> <p>int The size of each output sample.</p> required <code>bias</code> <p>bool Indicates if the layer should include a learnable bias parameter.</p> <code>True</code> <code>ortho_params</code> <p>OrthoParams An object containing orthogonalization and normalization configurations.</p> <code>OrthoParams()</code>"},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.OrthoLinear.__init__--notes","title":"Notes","text":"<p>The layer is initialized with orthogonal weights using <code>torch.nn.init.orthogonal_</code>. Weight parameters are further parametrized for both spectral normalization and orthogonal constraints using the provided <code>OrthoParams</code> object.</p> Source code in <code>orthogonium\\layers\\linear\\ortho_linear.py</code> <pre><code>def __init__(\n    self,\n    in_features: int,\n    out_features: int,\n    bias: bool = True,\n    ortho_params: OrthoParams = OrthoParams(),\n):\n    \"\"\"\n    Initializes an orthogonal linear layer with customizable orthogonalization parameters.\n\n    Attributes:\n        in_features : int\n            Number of input features.\n        out_features : int\n            Number of output features.\n        bias : bool\n            Whether to include a bias term in the layer. Default is True.\n        ortho_params : OrthoParams\n            Parameters for orthogonalization and spectral normalization. Default is the\n            default instance of OrthoParams.\n\n    Parameters:\n        in_features : int\n            The size of each input sample.\n        out_features : int\n            The size of each output sample.\n        bias : bool\n            Indicates if the layer should include a learnable bias parameter.\n        ortho_params : OrthoParams\n            An object containing orthogonalization and normalization configurations.\n\n    Notes\n    -----\n    The layer is initialized with orthogonal weights using `torch.nn.init.orthogonal_`.\n    Weight parameters are further parametrized for both spectral normalization and\n    orthogonal constraints using the provided `OrthoParams` object.\n    \"\"\"\n    super(OrthoLinear, self).__init__(in_features, out_features, bias=bias)\n    torch.nn.init.orthogonal_(self.weight)\n    parametrize.register_parametrization(\n        self,\n        \"weight\",\n        ortho_params.spectral_normalizer(\n            weight_shape=(self.out_features, self.in_features)\n        ),\n    )\n    parametrize.register_parametrization(\n        self, \"weight\", ortho_params.orthogonalizer(weight_shape=self.weight.shape)\n    )\n</code></pre>"},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.UnitNormLinear","title":"<code>UnitNormLinear</code>","text":"<p>               Bases: <code>Linear</code></p> Source code in <code>orthogonium\\layers\\linear\\ortho_linear.py</code> <pre><code>class UnitNormLinear(nn.Linear):\n    def __init__(\n        self,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"\n        A custom PyTorch Linear layer that ensures weights are normalized to unit norm along a specified dimension.\n\n        This class extends the torch.nn.Linear module and modifies the weight\n        matrix to maintain orthogonal initialization and unit norm\n        normalization during training. In this specific case, each output can be viewed as the result of a 1-Lipschitz\n        function. This means that the whole function in more than 1-Lipschitz but that each output taken independently\n        is 1-Lipschitz.\n\n        Attributes:\n            weight: The learnable weight tensor with orthogonal initialization\n                and enforced unit norm parametrization.\n\n        Args:\n            *args: Variable length positional arguments passed to the base\n                Linear class.\n            **kwargs: Variable length keyword arguments passed to the base\n                Linear class.\n        \"\"\"\n        super(UnitNormLinear, self).__init__(*args, **kwargs)\n        torch.nn.init.orthogonal_(self.weight)\n        parametrize.register_parametrization(\n            self,\n            \"weight\",\n            L2Normalize(dtype=self.weight.dtype, dim=1),\n        )\n\n    def singular_values(self):\n        svs = np.linalg.svd(\n            self.weight.detach().cpu().numpy(), full_matrices=False, compute_uv=False\n        )\n        stable_rank = np.sum(np.mean(svs) ** 2) / (svs.max() ** 2)\n        return svs.min(), svs.max(), stable_rank\n</code></pre>"},{"location":"api/linear/#orthogonium.layers.linear.ortho_linear.UnitNormLinear.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>A custom PyTorch Linear layer that ensures weights are normalized to unit norm along a specified dimension.</p> <p>This class extends the torch.nn.Linear module and modifies the weight matrix to maintain orthogonal initialization and unit norm normalization during training. In this specific case, each output can be viewed as the result of a 1-Lipschitz function. This means that the whole function in more than 1-Lipschitz but that each output taken independently is 1-Lipschitz.</p> <p>Attributes:</p> Name Type Description <code>weight</code> <p>The learnable weight tensor with orthogonal initialization and enforced unit norm parametrization.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length positional arguments passed to the base Linear class.</p> <code>()</code> <code>**kwargs</code> <p>Variable length keyword arguments passed to the base Linear class.</p> <code>{}</code> Source code in <code>orthogonium\\layers\\linear\\ortho_linear.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    **kwargs,\n):\n    \"\"\"\n    A custom PyTorch Linear layer that ensures weights are normalized to unit norm along a specified dimension.\n\n    This class extends the torch.nn.Linear module and modifies the weight\n    matrix to maintain orthogonal initialization and unit norm\n    normalization during training. In this specific case, each output can be viewed as the result of a 1-Lipschitz\n    function. This means that the whole function in more than 1-Lipschitz but that each output taken independently\n    is 1-Lipschitz.\n\n    Attributes:\n        weight: The learnable weight tensor with orthogonal initialization\n            and enforced unit norm parametrization.\n\n    Args:\n        *args: Variable length positional arguments passed to the base\n            Linear class.\n        **kwargs: Variable length keyword arguments passed to the base\n            Linear class.\n    \"\"\"\n    super(UnitNormLinear, self).__init__(*args, **kwargs)\n    torch.nn.init.orthogonal_(self.weight)\n    parametrize.register_parametrization(\n        self,\n        \"weight\",\n        L2Normalize(dtype=self.weight.dtype, dim=1),\n    )\n</code></pre>"},{"location":"api/losses/","title":"losses","text":""},{"location":"api/losses/#orthogonium.losses.CosineLoss","title":"<code>CosineLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\losses.py</code> <pre><code>class CosineLoss(nn.Module):\n    def __init__(self):\n        \"\"\"\n        A class that implements the Cosine Loss for measuring the cosine similarity\n        between predictions and targets. Designed for use in scenarios involving\n        angle-based loss calculations or similarity measurements.\n\n        Attributes:\n            None\n\n        \"\"\"\n        super(CosineLoss, self).__init__()\n\n    def forward(self, yp, yt):\n        return -torch.nn.functional.cosine_similarity(\n            yp, torch.nn.functional.one_hot(yt, yp.shape[1])\n        ).mean()\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.CosineLoss.__init__","title":"<code>__init__()</code>","text":"<p>A class that implements the Cosine Loss for measuring the cosine similarity between predictions and targets. Designed for use in scenarios involving angle-based loss calculations or similarity measurements.</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def __init__(self):\n    \"\"\"\n    A class that implements the Cosine Loss for measuring the cosine similarity\n    between predictions and targets. Designed for use in scenarios involving\n    angle-based loss calculations or similarity measurements.\n\n    Attributes:\n        None\n\n    \"\"\"\n    super(CosineLoss, self).__init__()\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.LossXent","title":"<code>LossXent</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\losses.py</code> <pre><code>class LossXent(nn.Module):\n    def __init__(self, n_classes, offset=2.12132, temperature=0.25):\n        \"\"\"\n        A custom loss function class for cross-entropy calculation.\n\n        This class initializes a cross-entropy loss criterion along with additional\n        parameters, such as an offset and a temperature factor, to allow a finer control over\n        the accuracy/robustness tradeoff during training.\n\n        Attributes:\n            criterion (nn.CrossEntropyLoss): The PyTorch cross-entropy loss criterion.\n            n_classes (int): The number of classes present in the dataset.\n            offset (float): An offset value for customizing the loss computation.\n            temperature (float): A temperature factor for scaling logits during loss calculation.\n\n        Parameters:\n            n_classes (int): The number of classes in the dataset.\n            offset (float, optional): The offset value for loss computation. Default is 2.12132.\n            temperature (float, optional): The temperature scaling factor. Default is 0.25.\n        \"\"\"\n        super(LossXent, self).__init__()\n        self.criterion = nn.CrossEntropyLoss()\n        self.n_classes = n_classes\n        self.offset = offset\n        self.temperature = temperature\n\n    def __call__(self, outputs, labels):\n        one_hot_labels = torch.nn.functional.one_hot(labels, num_classes=self.n_classes)\n        offset_outputs = outputs - self.offset * one_hot_labels\n        offset_outputs /= self.temperature\n        loss = self.criterion(offset_outputs, labels) * self.temperature\n        return loss\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.LossXent.__init__","title":"<code>__init__(n_classes, offset=2.12132, temperature=0.25)</code>","text":"<p>A custom loss function class for cross-entropy calculation.</p> <p>This class initializes a cross-entropy loss criterion along with additional parameters, such as an offset and a temperature factor, to allow a finer control over the accuracy/robustness tradeoff during training.</p> <p>Attributes:</p> Name Type Description <code>criterion</code> <code>CrossEntropyLoss</code> <p>The PyTorch cross-entropy loss criterion.</p> <code>n_classes</code> <code>int</code> <p>The number of classes present in the dataset.</p> <code>offset</code> <code>float</code> <p>An offset value for customizing the loss computation.</p> <code>temperature</code> <code>float</code> <p>A temperature factor for scaling logits during loss calculation.</p> <p>Parameters:</p> Name Type Description Default <code>n_classes</code> <code>int</code> <p>The number of classes in the dataset.</p> required <code>offset</code> <code>float</code> <p>The offset value for loss computation. Default is 2.12132.</p> <code>2.12132</code> <code>temperature</code> <code>float</code> <p>The temperature scaling factor. Default is 0.25.</p> <code>0.25</code> Source code in <code>orthogonium\\losses.py</code> <pre><code>def __init__(self, n_classes, offset=2.12132, temperature=0.25):\n    \"\"\"\n    A custom loss function class for cross-entropy calculation.\n\n    This class initializes a cross-entropy loss criterion along with additional\n    parameters, such as an offset and a temperature factor, to allow a finer control over\n    the accuracy/robustness tradeoff during training.\n\n    Attributes:\n        criterion (nn.CrossEntropyLoss): The PyTorch cross-entropy loss criterion.\n        n_classes (int): The number of classes present in the dataset.\n        offset (float): An offset value for customizing the loss computation.\n        temperature (float): A temperature factor for scaling logits during loss calculation.\n\n    Parameters:\n        n_classes (int): The number of classes in the dataset.\n        offset (float, optional): The offset value for loss computation. Default is 2.12132.\n        temperature (float, optional): The temperature scaling factor. Default is 0.25.\n    \"\"\"\n    super(LossXent, self).__init__()\n    self.criterion = nn.CrossEntropyLoss()\n    self.n_classes = n_classes\n    self.offset = offset\n    self.temperature = temperature\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss","title":"<code>SoftHKRMulticlassLoss</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\losses.py</code> <pre><code>class SoftHKRMulticlassLoss(torch.nn.Module):\n    def __init__(\n        self,\n        alpha=10.0,\n        min_margin=1.0,\n        alpha_mean=0.99,\n        temperature=1.0,\n    ):\n        \"\"\"\n        The multiclass version of HKR with softmax. This is done by computing\n        the HKR term over each class and averaging the results.\n\n        Note that `y_true` could be either one-hot encoded, +/-1 values.\n\n\n        Args:\n            alpha (float): regularization factor (0 &lt;= alpha &lt;= 1),\n                0 for KR only, 1 for hinge only\n            min_margin (float): margin to enforce.\n            temperature (float): factor for softmax  temperature\n                (higher value increases the weight of the highest non y_true logits)\n            alpha_mean (float): geometric mean factor\n            one_hot_ytrue (bool): set to True when y_true are one hot encoded (0 or 1),\n                and False when y_true already signed bases (for instance +/-1)\n            reduction: passed to tf.keras.Loss constructor\n            name (str): passed to tf.keras.Loss constructor\n\n        \"\"\"\n        assert (alpha &gt;= 0) and (alpha &lt;= 1), \"alpha must in [0,1]\"\n        self.alpha = torch.tensor(alpha, dtype=torch.float32)\n        self.min_margin_v = min_margin\n        self.alpha_mean = alpha_mean\n\n        self.current_mean = torch.tensor((self.min_margin_v,), dtype=torch.float32)\n        \"\"\"    constraint=lambda x: torch.clamp(x, 0.005, 1000),\n            name=\"current_mean\",\n        )\"\"\"\n\n        self.temperature = temperature * self.min_margin_v\n        if alpha == 1.0:  # alpha = 1.0 =&gt; hinge only\n            self.fct = self.multiclass_hinge_soft\n        else:\n            if alpha == 0.0:  # alpha = 0.0 =&gt; KR only\n                self.fct = self.kr_soft\n            else:\n                self.fct = self.hkr\n\n        super(SoftHKRMulticlassLoss, self).__init__()\n\n    def clamp_current_mean(self, x):\n        return torch.clamp(x, 0.005, 1000)\n\n    def _update_mean(self, y_pred):\n        self.current_mean = self.current_mean.to(y_pred.device)\n        current_global_mean = torch.mean(torch.abs(y_pred)).to(\n            dtype=self.current_mean.dtype\n        )\n        current_global_mean = (\n            self.alpha_mean * self.current_mean\n            + (1 - self.alpha_mean) * current_global_mean\n        )\n        self.current_mean = self.clamp_current_mean(current_global_mean).detach()\n        total_mean = current_global_mean\n        total_mean = torch.clamp(total_mean, self.min_margin_v, 20000)\n        return total_mean\n\n    def computeTemperatureSoftMax(self, y_true, y_pred):\n        total_mean = self._update_mean(y_pred)\n        current_temperature = (\n            torch.clamp(self.temperature / total_mean, 0.005, 250)\n            .to(dtype=y_pred.dtype)\n            .detach()\n        )\n        min_value = torch.tensor(torch.finfo(torch.float32).min, dtype=y_pred.dtype).to(\n            device=y_pred.device\n        )\n        opposite_values = torch.where(\n            y_true &gt; 0, min_value, current_temperature * y_pred\n        )\n        F_soft_KR = torch.softmax(opposite_values, dim=-1)\n        one_value = torch.tensor(1.0, dtype=F_soft_KR.dtype).to(device=y_pred.device)\n        F_soft_KR = torch.where(y_true &gt; 0, one_value, F_soft_KR)\n        return F_soft_KR\n\n    def signed_y_pred(self, y_true, y_pred):\n        \"\"\"Return for each item sign(y_true)*y_pred.\"\"\"\n        sign_y_true = torch.where(y_true &gt; 0, 1, -1)  # switch to +/-1\n        sign_y_true = sign_y_true.to(dtype=y_pred.dtype)\n        return y_pred * sign_y_true\n\n    def multiclass_hinge_preproc(self, signed_y_pred, min_margin):\n        \"\"\"From multiclass_hinge(y_true, y_pred, min_margin)\n        simplified to use precalculated signed_y_pred\"\"\"\n        # compute the elementwise hinge term\n        hinge = torch.nn.functional.relu(min_margin / 2.0 - signed_y_pred)\n        return hinge\n\n    def multiclass_hinge_soft_preproc(self, signed_y_pred, F_soft_KR):\n        hinge = self.multiclass_hinge_preproc(signed_y_pred, self.min_margin_v)\n        b = hinge * F_soft_KR\n        b = torch.sum(b, axis=-1)\n        return b\n\n    def multiclass_hinge_soft(self, y_true, y_pred):\n        F_soft_KR = self.computeTemperatureSoftMax(y_true, y_pred)\n        signed_y_pred = self.signed_y_pred(y_true, y_pred)\n        return self.multiclass_hinge_soft_preproc(signed_y_pred, F_soft_KR)\n\n    def kr_soft_preproc(self, signed_y_pred, F_soft_KR):\n        kr = -signed_y_pred\n        a = kr * F_soft_KR\n        a = torch.sum(a, axis=-1)\n        return a\n\n    def kr_soft(self, y_true, y_pred):\n        F_soft_KR = self.computeTemperatureSoftMax(y_true, y_pred)\n        signed_y_pred = self.signed_y_pred(y_true, y_pred)\n        return self.kr_soft_preproc(signed_y_pred, F_soft_KR)\n\n    def hkr(self, y_true, y_pred):\n        F_soft_KR = self.computeTemperatureSoftMax(y_true, y_pred)\n        signed_y_pred = self.signed_y_pred(y_true, y_pred)\n\n        loss_softkr = self.kr_soft_preproc(signed_y_pred, F_soft_KR)\n\n        loss_softhinge = self.multiclass_hinge_soft_preproc(signed_y_pred, F_soft_KR)\n        return (1 - self.alpha) * loss_softkr + self.alpha * loss_softhinge\n\n    def forward(self, input: torch.Tensor, target: torch.Tensor) -&gt; torch.Tensor:\n        target = torch.nn.functional.one_hot(target, num_classes=input.shape[1])\n        if not (isinstance(input, torch.Tensor)):  # required for dtype.max\n            input = torch.Tensor(input, dtype=input.dtype)\n        if not (isinstance(target, torch.Tensor)):\n            target = torch.Tensor(target, dtype=input.dtype)\n        loss_batch = self.fct(target, input)\n        return torch.mean(loss_batch)\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss.current_mean","title":"<code>current_mean = torch.tensor((self.min_margin_v), dtype=torch.float32)</code>  <code>instance-attribute</code>","text":"<p>constraint=lambda x: torch.clamp(x, 0.005, 1000),     name=\"current_mean\", )</p>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss.__init__","title":"<code>__init__(alpha=10.0, min_margin=1.0, alpha_mean=0.99, temperature=1.0)</code>","text":"<p>The multiclass version of HKR with softmax. This is done by computing the HKR term over each class and averaging the results.</p> <p>Note that <code>y_true</code> could be either one-hot encoded, +/-1 values.</p> <p>Parameters:</p> Name Type Description Default <code>alpha</code> <code>float</code> <p>regularization factor (0 &lt;= alpha &lt;= 1), 0 for KR only, 1 for hinge only</p> <code>10.0</code> <code>min_margin</code> <code>float</code> <p>margin to enforce.</p> <code>1.0</code> <code>temperature</code> <code>float</code> <p>factor for softmax  temperature (higher value increases the weight of the highest non y_true logits)</p> <code>1.0</code> <code>alpha_mean</code> <code>float</code> <p>geometric mean factor</p> <code>0.99</code> <code>one_hot_ytrue</code> <code>bool</code> <p>set to True when y_true are one hot encoded (0 or 1), and False when y_true already signed bases (for instance +/-1)</p> required <code>reduction</code> <p>passed to tf.keras.Loss constructor</p> required <code>name</code> <code>str</code> <p>passed to tf.keras.Loss constructor</p> required Source code in <code>orthogonium\\losses.py</code> <pre><code>def __init__(\n    self,\n    alpha=10.0,\n    min_margin=1.0,\n    alpha_mean=0.99,\n    temperature=1.0,\n):\n    \"\"\"\n    The multiclass version of HKR with softmax. This is done by computing\n    the HKR term over each class and averaging the results.\n\n    Note that `y_true` could be either one-hot encoded, +/-1 values.\n\n\n    Args:\n        alpha (float): regularization factor (0 &lt;= alpha &lt;= 1),\n            0 for KR only, 1 for hinge only\n        min_margin (float): margin to enforce.\n        temperature (float): factor for softmax  temperature\n            (higher value increases the weight of the highest non y_true logits)\n        alpha_mean (float): geometric mean factor\n        one_hot_ytrue (bool): set to True when y_true are one hot encoded (0 or 1),\n            and False when y_true already signed bases (for instance +/-1)\n        reduction: passed to tf.keras.Loss constructor\n        name (str): passed to tf.keras.Loss constructor\n\n    \"\"\"\n    assert (alpha &gt;= 0) and (alpha &lt;= 1), \"alpha must in [0,1]\"\n    self.alpha = torch.tensor(alpha, dtype=torch.float32)\n    self.min_margin_v = min_margin\n    self.alpha_mean = alpha_mean\n\n    self.current_mean = torch.tensor((self.min_margin_v,), dtype=torch.float32)\n    \"\"\"    constraint=lambda x: torch.clamp(x, 0.005, 1000),\n        name=\"current_mean\",\n    )\"\"\"\n\n    self.temperature = temperature * self.min_margin_v\n    if alpha == 1.0:  # alpha = 1.0 =&gt; hinge only\n        self.fct = self.multiclass_hinge_soft\n    else:\n        if alpha == 0.0:  # alpha = 0.0 =&gt; KR only\n            self.fct = self.kr_soft\n        else:\n            self.fct = self.hkr\n\n    super(SoftHKRMulticlassLoss, self).__init__()\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss.multiclass_hinge_preproc","title":"<code>multiclass_hinge_preproc(signed_y_pred, min_margin)</code>","text":"<p>From multiclass_hinge(y_true, y_pred, min_margin) simplified to use precalculated signed_y_pred</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def multiclass_hinge_preproc(self, signed_y_pred, min_margin):\n    \"\"\"From multiclass_hinge(y_true, y_pred, min_margin)\n    simplified to use precalculated signed_y_pred\"\"\"\n    # compute the elementwise hinge term\n    hinge = torch.nn.functional.relu(min_margin / 2.0 - signed_y_pred)\n    return hinge\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.SoftHKRMulticlassLoss.signed_y_pred","title":"<code>signed_y_pred(y_true, y_pred)</code>","text":"<p>Return for each item sign(y_true)*y_pred.</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def signed_y_pred(self, y_true, y_pred):\n    \"\"\"Return for each item sign(y_true)*y_pred.\"\"\"\n    sign_y_true = torch.where(y_true &gt; 0, 1, -1)  # switch to +/-1\n    sign_y_true = sign_y_true.to(dtype=y_pred.dtype)\n    return y_pred * sign_y_true\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.VRA","title":"<code>VRA(output, class_indices, last_layer_type='classwise', L=1.0, eps=36 / 255, return_certs=False)</code>","text":"<p>Compute the verified robust accuracy (VRA) of a model's output.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <p>torch.Tensor The output of the model.</p> required <code>class_indices</code> <p>torch.Tensor The indices of the correct classes. Should not be one-hot encoded.</p> required <code>last_layer_type</code> <p>str The type of the last layer of the model. Should be either \"classwise\" (L-lip per class) or \"global\" (L-lip globally).</p> <code>'classwise'</code> <code>L</code> <p>float The Lipschitz constant of the model.</p> <code>1.0</code> <code>eps</code> <p>float The perturbation size.</p> <code>36 / 255</code> <code>return_certs</code> <p>bool Whether to return the certificates instead of the VRA.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>vra</code> <p>torch.Tensor The VRA of the model.</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def VRA(\n    output,\n    class_indices,\n    last_layer_type=\"classwise\",\n    L=1.0,\n    eps=36 / 255,\n    return_certs=False,\n):\n    \"\"\"Compute the verified robust accuracy (VRA) of a model's output.\n\n    Args:\n        output : torch.Tensor\n            The output of the model.\n        class_indices : torch.Tensor\n            The indices of the correct classes. Should not be one-hot encoded.\n        last_layer_type : str\n            The type of the last layer of the model. Should be either \"classwise\" (L-lip per class) or \"global\" (L-lip globally).\n        L : float\n            The Lipschitz constant of the model.\n        eps : float\n            The perturbation size.\n        return_certs : bool\n            Whether to return the certificates instead of the VRA.\n\n    Returns:\n        vra : torch.Tensor\n            The VRA of the model.\n    \"\"\"\n    batch_size = output.shape[0]\n    batch_indices = torch.arange(batch_size)\n\n    # get the values of the correct class\n    output_class_indices = output[batch_indices, class_indices]\n    # get the values of the top class that is not the correct class\n    # create a mask indicating the correct class\n    onehot = torch.zeros_like(output).cuda()\n    onehot[torch.arange(output.shape[0]), class_indices] = 1.0\n    # subtracting a large number from the correct class to ensure it is not the max\n    # doing so will allow us to find the top of the output that is not the correct class\n    output_trunc = output - onehot * 1e6\n    output_nextmax = torch.max(output_trunc, dim=1)[0]\n    # now we can compute the certificates\n    output_diff = output_class_indices - output_nextmax\n    if last_layer_type == \"global\":\n        den = math.sqrt(2) * L\n    elif last_layer_type == \"classwise\":\n        den = 2 * L\n    else:\n        raise ValueError(\n            \"[VRA] last_layer_type should be either 'global' or 'classwise'\"\n        )\n    certs = output_diff / den\n    # now we can compute the vra\n    # vra is percentage of certs &gt; eps\n    vra = (certs &gt; eps).float()\n    if return_certs:\n        return certs\n    return vra\n</code></pre>"},{"location":"api/losses/#orthogonium.losses.check_last_linear_layer_type","title":"<code>check_last_linear_layer_type(model)</code>","text":"<p>Determines the type of the last linear layer in a given model.</p> <p>This function inspects the architecture of the model and identifies the last linear layer of specific types (nn.Linear, OrthoLinear, UnitNormLinear). It then returns a string indicating the type of the last linear layer based on its class. This allows to determine the parameter to use for computing the VRA of a model's output.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <p>The model containing layers to be inspected.</p> required <p>Returns:</p> Name Type Description <code>str</code> <p>A string indicating the type of the last linear layer.  The possible values are:      - \"global\" if the layer is of type OrthoLinear.      - \"classwise\" if the layer is of type UnitNormLinear.      - \"unknown\" if the layer is of any other type or if no        linear layer is found.</p> Source code in <code>orthogonium\\losses.py</code> <pre><code>def check_last_linear_layer_type(model):\n    \"\"\"\n    Determines the type of the last linear layer in a given model.\n\n    This function inspects the architecture of the model and identifies the last\n    linear layer of specific types (nn.Linear, OrthoLinear, UnitNormLinear). It\n    then returns a string indicating the type of the last linear layer based on\n    its class. This allows to determine the parameter to use for computing the\n    VRA of a model's output.\n\n    Args:\n        model: The model containing layers to be inspected.\n\n    Returns:\n        str: A string indicating the type of the last linear layer.\n             The possible values are:\n                 - \"global\" if the layer is of type OrthoLinear.\n                 - \"classwise\" if the layer is of type UnitNormLinear.\n                 - \"unknown\" if the layer is of any other type or if no\n                   linear layer is found.\n    \"\"\"\n    # Find the last linear layer in the model\n    last_linear_layer = None\n    layers = list(model.children())\n    for layer in reversed(layers):\n        if (\n            isinstance(layer, nn.Linear)\n            or isinstance(layer, OrthoLinear)\n            or isinstance(layer, UnitNormLinear)\n        ):\n            last_linear_layer = layer\n            break\n\n    # Check the type of the last linear layer\n    if isinstance(last_linear_layer, OrthoLinear):\n        return \"global\"\n    elif isinstance(last_linear_layer, UnitNormLinear):\n        return \"classwise\"\n    else:\n        return \"unknown\"\n</code></pre>"},{"location":"api/reparametrizers/","title":"reparametrizers","text":""},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BJORCK_PASS_THROUGH_ORTHO_PARAMS","title":"<code>BJORCK_PASS_THROUGH_ORTHO_PARAMS = OrthoParams(spectral_normalizer=ClassParam(BatchedPowerIteration, power_it_niter=3, eps=0.0001), orthogonalizer=ClassParam(BatchedBjorckOrthogonalization, beta=0.5, niters=12, pass_through=True))</code>  <code>module-attribute</code>","text":"<p>Orthogonalization parameters that use the Bjorck orthogonalization method with a pass-through optimization. This configuration greatly reduces the consumed memory but at the cost of a slower convergence and worst perfomances.</p>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.CHOLESKY_ORTHO_PARAMS","title":"<code>CHOLESKY_ORTHO_PARAMS = OrthoParams(spectral_normalizer=BatchedIdentity, orthogonalizer=ClassParam(BatchedCholeskyOrthogonalization))</code>  <code>module-attribute</code>","text":"<p>Setting that use the Cholesky orthogonalization method. This method is memory and time efficient but cannot converge to the exact orthogonal matrix (tests passing with epsilon=5e-5 meaning the layer may be 1.05 lipschitz).</p>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.CHOLESKY_STABLE_ORTHO_PARAMS","title":"<code>CHOLESKY_STABLE_ORTHO_PARAMS = OrthoParams(spectral_normalizer=BatchedIdentity, orthogonalizer=ClassParam(BatchedCholeskyOrthogonalization, stable=True))</code>  <code>module-attribute</code>","text":"<p>Setting that use the Cholesky orthogonalization method and stores some values for backward to ensure numerical  stability.</p>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.DEFAULT_ORTHO_PARAMS","title":"<code>DEFAULT_ORTHO_PARAMS = OrthoParams()</code>  <code>module-attribute</code>","text":"<p>The default orthogonalization parameters used by our library.  Suitable for most applications and includes: - A <code>BatchedPowerIteration</code> for spectral normalization - A <code>BatchedBjorckOrthogonalization</code> for orthogonalization</p>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.DEFAULT_TEST_ORTHO_PARAMS","title":"<code>DEFAULT_TEST_ORTHO_PARAMS = OrthoParams(spectral_normalizer=ClassParam(BatchedPowerIteration, power_it_niter=4, eps=0.0001), orthogonalizer=ClassParam(BatchedBjorckOrthogonalization, beta=0.5, niters=25))</code>  <code>module-attribute</code>","text":"<p>Setting with more iterations to ensure that test passes with epsilon=1e-4.</p>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.EXP_ORTHO_PARAMS","title":"<code>EXP_ORTHO_PARAMS = OrthoParams(spectral_normalizer=ClassParam(BatchedPowerIteration, power_it_niter=3, eps=1e-06), orthogonalizer=ClassParam(BatchedExponentialOrthogonalization, niters=12))</code>  <code>module-attribute</code>","text":"<p>Setting that use the exponential orthogonalization method with 12 iterations. The matrix is pre-conditionned  with the power iteration method.</p>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.QR_ORTHO_PARAMS","title":"<code>QR_ORTHO_PARAMS = OrthoParams(spectral_normalizer=ClassParam(BatchedPowerIteration, power_it_niter=3, eps=0.001), orthogonalizer=ClassParam(BatchedQROrthogonalization))</code>  <code>module-attribute</code>","text":"<p>Setting that use the QR orthogonalization method. The matrix is pre-conditionned  with the power iteration method.</p>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedBjorckOrthogonalization","title":"<code>BatchedBjorckOrthogonalization</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedBjorckOrthogonalization(nn.Module):\n    def __init__(self, weight_shape, beta=0.5, niters=12, pass_through=False):\n        \"\"\"\n        Initialize the BatchedBjorckOrthogonalization module.\n\n        This module implements the Bj\u00f6rck orthogonalization method, which iteratively refines\n        a weight matrix towards orthogonality. The method is especially effective when the\n        weight matrix columns are nearly orthonormal. It balances computational efficiency\n        with convergence speed through a user-defined `beta` parameter and iteration count.\n\n        Args:\n            weight_shape (tuple): The shape of the weight matrix to be orthogonalized.\n            beta (float): Coefficient controlling the convergence of the orthogonalization process.\n                Default is 0.5.\n            niters (int): Number of iterations for the orthogonalization algorithm. Default is 12.\n            pass_through (bool): If True, most iterations are performed without gradient computation,\n                which can improve efficiency.\n        \"\"\"\n        self.weight_shape = weight_shape\n        self.beta = beta\n        self.niters = niters\n        self.pass_through = pass_through\n        if weight_shape[-2] &lt; weight_shape[-1]:\n            self.wwtw_op = BatchedBjorckOrthogonalization.wwt_w_op\n        else:\n            self.wwtw_op = BatchedBjorckOrthogonalization.w_wtw_op\n        super(BatchedBjorckOrthogonalization, self).__init__()\n\n    @staticmethod\n    def w_wtw_op(w):\n        return w @ (w.transpose(-1, -2) @ w)\n\n    @staticmethod\n    def wwt_w_op(w):\n        return (w @ w.transpose(-1, -2)) @ w\n\n    def forward(self, w):\n        \"\"\"\n        Apply the Bj\u00f6rck orthogonalization process to the weight matrix.\n\n        The algorithm adjusts the input matrix to approximate the closest orthogonal matrix\n        by iteratively applying transformations based on the Bj\u00f6rck algorithm.\n\n        Args:\n            w (torch.Tensor): The weight matrix to be orthogonalized.\n\n        Returns:\n            torch.Tensor: The orthogonalized weight matrix.\n        \"\"\"\n        if self.pass_through:\n            with torch.no_grad():\n                for _ in range(self.niters):\n                    w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n            # Final iteration without no_grad, using parameters:\n            w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n        else:\n            for _ in range(self.niters):\n                w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n        return w\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedBjorckOrthogonalization.__init__","title":"<code>__init__(weight_shape, beta=0.5, niters=12, pass_through=False)</code>","text":"<p>Initialize the BatchedBjorckOrthogonalization module.</p> <p>This module implements the Bj\u00f6rck orthogonalization method, which iteratively refines a weight matrix towards orthogonality. The method is especially effective when the weight matrix columns are nearly orthonormal. It balances computational efficiency with convergence speed through a user-defined <code>beta</code> parameter and iteration count.</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <code>tuple</code> <p>The shape of the weight matrix to be orthogonalized.</p> required <code>beta</code> <code>float</code> <p>Coefficient controlling the convergence of the orthogonalization process. Default is 0.5.</p> <code>0.5</code> <code>niters</code> <code>int</code> <p>Number of iterations for the orthogonalization algorithm. Default is 12.</p> <code>12</code> <code>pass_through</code> <code>bool</code> <p>If True, most iterations are performed without gradient computation, which can improve efficiency.</p> <code>False</code> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape, beta=0.5, niters=12, pass_through=False):\n    \"\"\"\n    Initialize the BatchedBjorckOrthogonalization module.\n\n    This module implements the Bj\u00f6rck orthogonalization method, which iteratively refines\n    a weight matrix towards orthogonality. The method is especially effective when the\n    weight matrix columns are nearly orthonormal. It balances computational efficiency\n    with convergence speed through a user-defined `beta` parameter and iteration count.\n\n    Args:\n        weight_shape (tuple): The shape of the weight matrix to be orthogonalized.\n        beta (float): Coefficient controlling the convergence of the orthogonalization process.\n            Default is 0.5.\n        niters (int): Number of iterations for the orthogonalization algorithm. Default is 12.\n        pass_through (bool): If True, most iterations are performed without gradient computation,\n            which can improve efficiency.\n    \"\"\"\n    self.weight_shape = weight_shape\n    self.beta = beta\n    self.niters = niters\n    self.pass_through = pass_through\n    if weight_shape[-2] &lt; weight_shape[-1]:\n        self.wwtw_op = BatchedBjorckOrthogonalization.wwt_w_op\n    else:\n        self.wwtw_op = BatchedBjorckOrthogonalization.w_wtw_op\n    super(BatchedBjorckOrthogonalization, self).__init__()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedBjorckOrthogonalization.forward","title":"<code>forward(w)</code>","text":"<p>Apply the Bj\u00f6rck orthogonalization process to the weight matrix.</p> <p>The algorithm adjusts the input matrix to approximate the closest orthogonal matrix by iteratively applying transformations based on the Bj\u00f6rck algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>Tensor</code> <p>The weight matrix to be orthogonalized.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The orthogonalized weight matrix.</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def forward(self, w):\n    \"\"\"\n    Apply the Bj\u00f6rck orthogonalization process to the weight matrix.\n\n    The algorithm adjusts the input matrix to approximate the closest orthogonal matrix\n    by iteratively applying transformations based on the Bj\u00f6rck algorithm.\n\n    Args:\n        w (torch.Tensor): The weight matrix to be orthogonalized.\n\n    Returns:\n        torch.Tensor: The orthogonalized weight matrix.\n    \"\"\"\n    if self.pass_through:\n        with torch.no_grad():\n            for _ in range(self.niters):\n                w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n        # Final iteration without no_grad, using parameters:\n        w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n    else:\n        for _ in range(self.niters):\n            w = (1 + self.beta) * w - self.beta * self.wwtw_op(w)\n    return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedCholeskyOrthogonalization","title":"<code>BatchedCholeskyOrthogonalization</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedCholeskyOrthogonalization(nn.Module):\n    def __init__(self, weight_shape, stable=False):\n        \"\"\"\n        Initialize the BatchedCholeskyOrthogonalization module.\n\n        This module orthogonalizes a weight matrix using the Cholesky decomposition method.\n        It first computes the positive definite matrix \\( V V^T \\), then performs a Cholesky\n        decomposition to obtain a lower triangular matrix. Solving the resulting triangular\n        system yields an orthogonal matrix. This method is efficient and numerically stable,\n        making it suitable for a wide range of applications.\n\n        Args:\n            weight_shape (tuple): The shape of the weight matrix.\n            stable (bool): Whether to use the stable version of the Cholesky-based orthogonalization\n                function, which adds a small positive diagonal element to ensure numerical stability.\n                Default is False.\n        \"\"\"\n        self.weight_shape = weight_shape\n        super(BatchedCholeskyOrthogonalization, self).__init__()\n        if stable:\n            self.orth = BatchedCholeskyOrthogonalization.CholeskyOrthfn_stable.apply\n        else:\n            self.orth = BatchedCholeskyOrthogonalization.CholeskyOrthfn.apply\n\n    # @staticmethod\n    # def orth(X):\n    #     S = X @ X.mT\n    #     eps = S.diagonal(dim1=1, dim2=2).mean(1).mul(1e-3).detach()\n    #     eye = torch.eye(S.size(-1), dtype=S.dtype, device=S.device)\n    #     S = S + eps.view(-1, 1, 1) * eye.unsqueeze(0)\n    #     L = torch.linalg.cholesky(S)\n    #     W = torch.linalg.solve_triangular(L, X, upper=False)\n    #     return W\n\n    class CholeskyOrthfn(torch.autograd.Function):\n        @staticmethod\n        # def forward(ctx, X):\n        #     S = X @ X.mT\n        #     eps = S.diagonal(dim1=1, dim2=2).mean(1).mul(1e-3)\n        #     eye = torch.eye(S.size(-1), dtype=S.dtype, device=S.device)\n        #     S = S + eps.view(-1, 1, 1) * eye.unsqueeze(0)\n        #     L = torch.linalg.cholesky(S)\n        #     W = torch.linalg.solve_triangular(L, X, upper=False)\n        #     ctx.save_for_backward(W, L)\n        #     return W\n        def forward(ctx, X):\n            S = X @ X.mT\n            eps = 1e-5  # A common stable choice\n            S = S + eps * torch.eye(\n                S.size(-1), dtype=S.dtype, device=S.device\n            ).unsqueeze(0)\n            L = torch.linalg.cholesky(S)\n            W = torch.linalg.solve_triangular(L, X, upper=False)\n            ctx.save_for_backward(W, L)\n            return W\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            W, L = ctx.saved_tensors\n            LmT = L.mT.contiguous()\n            gB = torch.linalg.solve_triangular(LmT, grad_output, upper=True)\n            gA = (-gB @ W.mT).tril()\n            gS = (LmT @ gA).tril()\n            gS = gS + gS.tril(-1).mT\n            gS = torch.linalg.solve_triangular(LmT, gS, upper=True)\n            gX = gS @ W + gB\n            return gX\n\n    class CholeskyOrthfn_stable(torch.autograd.Function):\n        @staticmethod\n        def forward(ctx, X):\n            S = X @ X.mT\n            eps = 1e-5  # A common stable choice\n            S = S + eps * torch.eye(\n                S.size(-1), dtype=S.dtype, device=S.device\n            ).unsqueeze(0)\n            L = torch.linalg.cholesky(S)\n            W = torch.linalg.solve_triangular(L, X, upper=False)\n            ctx.save_for_backward(X, W, L)\n            return W\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            X, W, L = ctx.saved_tensors\n            gB = torch.linalg.solve_triangular(L.mT, grad_output, upper=True)\n            gA = (-gB @ W.mT).tril()\n            gS = (L.mT @ gA).tril()\n            gS = gS + gS.tril(-1).mT\n            gS = torch.linalg.solve_triangular(L.mT, gS, upper=True)\n            gS = torch.linalg.solve_triangular(L, gS, upper=False, left=False)\n            gX = gS @ X + gB\n            return gX\n\n    def forward(self, w):\n        \"\"\"\n        Apply Cholesky-based orthogonalization to the weight matrix.\n\n        This method constructs a symmetric positive definite matrix from the input weight\n        matrix, performs Cholesky decomposition, and solves the triangular system to produce\n        an orthogonal matrix. It mimics the results of the Gram-Schmidt process but with\n        improved numerical stability.\n\n        Args:\n            w (torch.Tensor): The weight matrix to be orthogonalized.\n\n        Returns:\n            torch.Tensor: The orthogonalized weight matrix.\n        \"\"\"\n        return self.orth(w).view(*self.weight_shape)\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedCholeskyOrthogonalization.__init__","title":"<code>__init__(weight_shape, stable=False)</code>","text":"<p>Initialize the BatchedCholeskyOrthogonalization module.</p> <p>This module orthogonalizes a weight matrix using the Cholesky decomposition method. It first computes the positive definite matrix \\( V V^T \\), then performs a Cholesky decomposition to obtain a lower triangular matrix. Solving the resulting triangular system yields an orthogonal matrix. This method is efficient and numerically stable, making it suitable for a wide range of applications.</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <code>tuple</code> <p>The shape of the weight matrix.</p> required <code>stable</code> <code>bool</code> <p>Whether to use the stable version of the Cholesky-based orthogonalization function, which adds a small positive diagonal element to ensure numerical stability. Default is False.</p> <code>False</code> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape, stable=False):\n    \"\"\"\n    Initialize the BatchedCholeskyOrthogonalization module.\n\n    This module orthogonalizes a weight matrix using the Cholesky decomposition method.\n    It first computes the positive definite matrix \\( V V^T \\), then performs a Cholesky\n    decomposition to obtain a lower triangular matrix. Solving the resulting triangular\n    system yields an orthogonal matrix. This method is efficient and numerically stable,\n    making it suitable for a wide range of applications.\n\n    Args:\n        weight_shape (tuple): The shape of the weight matrix.\n        stable (bool): Whether to use the stable version of the Cholesky-based orthogonalization\n            function, which adds a small positive diagonal element to ensure numerical stability.\n            Default is False.\n    \"\"\"\n    self.weight_shape = weight_shape\n    super(BatchedCholeskyOrthogonalization, self).__init__()\n    if stable:\n        self.orth = BatchedCholeskyOrthogonalization.CholeskyOrthfn_stable.apply\n    else:\n        self.orth = BatchedCholeskyOrthogonalization.CholeskyOrthfn.apply\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedCholeskyOrthogonalization.forward","title":"<code>forward(w)</code>","text":"<p>Apply Cholesky-based orthogonalization to the weight matrix.</p> <p>This method constructs a symmetric positive definite matrix from the input weight matrix, performs Cholesky decomposition, and solves the triangular system to produce an orthogonal matrix. It mimics the results of the Gram-Schmidt process but with improved numerical stability.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>Tensor</code> <p>The weight matrix to be orthogonalized.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The orthogonalized weight matrix.</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def forward(self, w):\n    \"\"\"\n    Apply Cholesky-based orthogonalization to the weight matrix.\n\n    This method constructs a symmetric positive definite matrix from the input weight\n    matrix, performs Cholesky decomposition, and solves the triangular system to produce\n    an orthogonal matrix. It mimics the results of the Gram-Schmidt process but with\n    improved numerical stability.\n\n    Args:\n        w (torch.Tensor): The weight matrix to be orthogonalized.\n\n    Returns:\n        torch.Tensor: The orthogonalized weight matrix.\n    \"\"\"\n    return self.orth(w).view(*self.weight_shape)\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedExponentialOrthogonalization","title":"<code>BatchedExponentialOrthogonalization</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedExponentialOrthogonalization(nn.Module):\n    def __init__(self, weight_shape, niters=7):\n        \"\"\"\n        Initialize the BatchedExponentialOrthogonalization module.\n\n        This module orthogonalizes a weight matrix using the exponential map of a skew-symmetric\n        matrix. By converting the matrix into a skew-symmetric form and applying the matrix\n        exponential, it produces an orthogonal matrix. This approach is particularly useful\n        in contexts where smooth transitions between matrices are required.\n\n        Non-square matrices are padded to the largest dimension to ensure that the matrix can\n        be converted to a skew-symmetric matrix. The resulting matrix is cropped to the original\n        dimension.\n\n        Args:\n            weight_shape (tuple): The shape of the weight matrix.\n            niters (int): Number of iterations for the series expansion approximation of the\n                matrix exponential. Default is 7.\n        \"\"\"\n        self.weight_shape = weight_shape\n        self.max_dim = max(weight_shape[-2:])\n        self.niters = niters\n        super(BatchedExponentialOrthogonalization, self).__init__()\n\n    def forward(self, w):\n        # fill w with zero to have a square matrix over the last two dimensions\n        # if ((self.max_dim - w.shape[-1]) != 0) and ((self.max_dim - w.shape[-2]) != 0):\n        w = torch.nn.functional.pad(\n            w, (0, self.max_dim - w.shape[-1], 0, self.max_dim - w.shape[-2])\n        )\n        # makes w skew symmetric\n        w = (w - w.transpose(-1, -2)) / 2\n        acc = w\n        res = torch.eye(acc.shape[-2], acc.shape[-1], device=w.device) + acc\n        for i in range(2, self.niters):\n            acc = torch.einsum(\"...ij,...jk-&gt;...ik\", acc, w) / i\n            res = res + acc\n        # if transpose:\n        #     res = res.transpose(-1, -2)\n        res = res[..., : self.weight_shape[-2], : self.weight_shape[-1]]\n        return res.contiguous()\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedExponentialOrthogonalization.__init__","title":"<code>__init__(weight_shape, niters=7)</code>","text":"<p>Initialize the BatchedExponentialOrthogonalization module.</p> <p>This module orthogonalizes a weight matrix using the exponential map of a skew-symmetric matrix. By converting the matrix into a skew-symmetric form and applying the matrix exponential, it produces an orthogonal matrix. This approach is particularly useful in contexts where smooth transitions between matrices are required.</p> <p>Non-square matrices are padded to the largest dimension to ensure that the matrix can be converted to a skew-symmetric matrix. The resulting matrix is cropped to the original dimension.</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <code>tuple</code> <p>The shape of the weight matrix.</p> required <code>niters</code> <code>int</code> <p>Number of iterations for the series expansion approximation of the matrix exponential. Default is 7.</p> <code>7</code> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape, niters=7):\n    \"\"\"\n    Initialize the BatchedExponentialOrthogonalization module.\n\n    This module orthogonalizes a weight matrix using the exponential map of a skew-symmetric\n    matrix. By converting the matrix into a skew-symmetric form and applying the matrix\n    exponential, it produces an orthogonal matrix. This approach is particularly useful\n    in contexts where smooth transitions between matrices are required.\n\n    Non-square matrices are padded to the largest dimension to ensure that the matrix can\n    be converted to a skew-symmetric matrix. The resulting matrix is cropped to the original\n    dimension.\n\n    Args:\n        weight_shape (tuple): The shape of the weight matrix.\n        niters (int): Number of iterations for the series expansion approximation of the\n            matrix exponential. Default is 7.\n    \"\"\"\n    self.weight_shape = weight_shape\n    self.max_dim = max(weight_shape[-2:])\n    self.niters = niters\n    super(BatchedExponentialOrthogonalization, self).__init__()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedIdentity","title":"<code>BatchedIdentity</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedIdentity(nn.Module):\n    def __init__(self, weight_shape):\n        \"\"\"\n        Class representing a batched identity matrix with a specific weight shape. The\n        matrix is initialized based on the provided shape of the weights. It is a\n        convenient utility for applications where identity-like operations are\n        required in a batched manner.\n\n        Attributes:\n            weight_shape (Tuple[int, int]): A tuple representing the shape of the\n            weight matrix for each batch. (unused)\n\n        Args:\n            weight_shape: A tuple specifying the shape of the individual weight matrix.\n        \"\"\"\n        super(BatchedIdentity, self).__init__()\n\n    def forward(self, w):\n        return w\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedIdentity.__init__","title":"<code>__init__(weight_shape)</code>","text":"<p>Class representing a batched identity matrix with a specific weight shape. The matrix is initialized based on the provided shape of the weights. It is a convenient utility for applications where identity-like operations are required in a batched manner.</p> <p>Attributes:</p> Name Type Description <code>weight_shape</code> <code>Tuple[int, int]</code> <p>A tuple representing the shape of the</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <p>A tuple specifying the shape of the individual weight matrix.</p> required Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape):\n    \"\"\"\n    Class representing a batched identity matrix with a specific weight shape. The\n    matrix is initialized based on the provided shape of the weights. It is a\n    convenient utility for applications where identity-like operations are\n    required in a batched manner.\n\n    Attributes:\n        weight_shape (Tuple[int, int]): A tuple representing the shape of the\n        weight matrix for each batch. (unused)\n\n    Args:\n        weight_shape: A tuple specifying the shape of the individual weight matrix.\n    \"\"\"\n    super(BatchedIdentity, self).__init__()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedPowerIteration","title":"<code>BatchedPowerIteration</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedPowerIteration(nn.Module):\n    def __init__(self, weight_shape, power_it_niter=3, eps=1e-12):\n        \"\"\"\n        BatchedPowerIteration is a class that performs spectral normalization on weights\n        using the power iteration method in a batched manner. It initializes singular\n        vectors 'u' and 'v', which are used to approximate the largest singular value\n        of the associated weight matrix during training. The L2 normalization is applied\n        to stabilize these singular vector parameters.\n\n        Attributes:\n            weight_shape: tuple\n                Shape of the weight tensor. Normalization is applied to the last two dimensions.\n            power_it_niter: int\n                Number of iterations to perform for the power iteration method.\n            eps: float\n                A small constant to ensure numerical stability during calculations. Used in the power iteration\n                method to avoid dividing by zero.\n        \"\"\"\n        super(BatchedPowerIteration, self).__init__()\n        self.weight_shape = weight_shape\n        self.power_it_niter = power_it_niter\n        self.eps = eps\n        # init u\n        # u will be weight_shape[:-2] + (weight_shape[:-2], 1)\n        # v will be weight_shape[:-2] + (weight_shape[:-1], 1,)\n        self.u = nn.Parameter(\n            torch.Tensor(torch.randn(*weight_shape[:-2], weight_shape[-2], 1)),\n            requires_grad=False,\n        )\n        self.v = nn.Parameter(\n            torch.Tensor(torch.randn(*weight_shape[:-2], weight_shape[-1], 1)),\n            requires_grad=False,\n        )\n        parametrize.register_parametrization(\n            self, \"u\", L2Normalize(dtype=self.u.dtype, dim=(-2))\n        )\n        parametrize.register_parametrization(\n            self, \"v\", L2Normalize(dtype=self.v.dtype, dim=(-2))\n        )\n\n    def forward(self, X, init_u=None, n_iters=3, return_uv=True):\n        for _ in range(n_iters):\n            self.v = X.transpose(-1, -2) @ self.u\n            self.u = X @ self.v\n        # stop gradient on u and v\n        u = self.u.detach()\n        v = self.v.detach()\n        # but keep gradient on s\n        s = u.transpose(-1, -2) @ X @ v\n        return X / (s + self.eps)\n\n    def right_inverse(self, normalized_kernel):\n        # we assume that the kernel is normalized\n        return normalized_kernel.to(self.u.dtype)\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedPowerIteration.__init__","title":"<code>__init__(weight_shape, power_it_niter=3, eps=1e-12)</code>","text":"<p>BatchedPowerIteration is a class that performs spectral normalization on weights using the power iteration method in a batched manner. It initializes singular vectors 'u' and 'v', which are used to approximate the largest singular value of the associated weight matrix during training. The L2 normalization is applied to stabilize these singular vector parameters.</p> <p>Attributes:</p> Name Type Description <code>weight_shape</code> <p>tuple Shape of the weight tensor. Normalization is applied to the last two dimensions.</p> <code>power_it_niter</code> <p>int Number of iterations to perform for the power iteration method.</p> <code>eps</code> <p>float A small constant to ensure numerical stability during calculations. Used in the power iteration method to avoid dividing by zero.</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape, power_it_niter=3, eps=1e-12):\n    \"\"\"\n    BatchedPowerIteration is a class that performs spectral normalization on weights\n    using the power iteration method in a batched manner. It initializes singular\n    vectors 'u' and 'v', which are used to approximate the largest singular value\n    of the associated weight matrix during training. The L2 normalization is applied\n    to stabilize these singular vector parameters.\n\n    Attributes:\n        weight_shape: tuple\n            Shape of the weight tensor. Normalization is applied to the last two dimensions.\n        power_it_niter: int\n            Number of iterations to perform for the power iteration method.\n        eps: float\n            A small constant to ensure numerical stability during calculations. Used in the power iteration\n            method to avoid dividing by zero.\n    \"\"\"\n    super(BatchedPowerIteration, self).__init__()\n    self.weight_shape = weight_shape\n    self.power_it_niter = power_it_niter\n    self.eps = eps\n    # init u\n    # u will be weight_shape[:-2] + (weight_shape[:-2], 1)\n    # v will be weight_shape[:-2] + (weight_shape[:-1], 1,)\n    self.u = nn.Parameter(\n        torch.Tensor(torch.randn(*weight_shape[:-2], weight_shape[-2], 1)),\n        requires_grad=False,\n    )\n    self.v = nn.Parameter(\n        torch.Tensor(torch.randn(*weight_shape[:-2], weight_shape[-1], 1)),\n        requires_grad=False,\n    )\n    parametrize.register_parametrization(\n        self, \"u\", L2Normalize(dtype=self.u.dtype, dim=(-2))\n    )\n    parametrize.register_parametrization(\n        self, \"v\", L2Normalize(dtype=self.v.dtype, dim=(-2))\n    )\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedQROrthogonalization","title":"<code>BatchedQROrthogonalization</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class BatchedQROrthogonalization(nn.Module):\n    def __init__(self, weight_shape):\n        \"\"\"\n        Initialize the BatchedQROrthogonalization module.\n\n        This module uses QR decomposition to orthogonalize a weight matrix in a batched manner.\n        It computes the orthogonal component (`Q`) from the decomposition, ensuring that the\n        output satisfies orthogonality constraints.\n\n        Args:\n            weight_shape (tuple): The shape of the weight matrix to be orthogonalized.\n        \"\"\"\n        super(BatchedQROrthogonalization, self).__init__()\n\n    def forward(self, w):\n        \"\"\"\n        Perform QR decomposition to compute the orthogonalized weight matrix.\n\n        The QR decomposition splits the input matrix into an orthogonal matrix (`Q`) and\n        an upper triangular matrix (`R`). This module returns the orthogonal component.\n\n        Args:\n            w (torch.Tensor): The weight matrix to be orthogonalized.\n\n        Returns:\n            torch.Tensor: The orthogonalized weight matrix (`Q` from the QR decomposition).\n        \"\"\"\n        transpose = w.shape[-2] &lt; w.shape[-1]\n        if transpose:\n            w = w.transpose(-1, -2)\n        q, r = torch.linalg.qr(w, mode=\"reduced\")\n        # compute the sign of the diagonal of d\n        diag_sign = torch.sign(torch.diagonal(r, dim1=-2, dim2=-1)).unsqueeze(-2)\n        # multiply the sign with the diagonal of r\n        q = q * diag_sign\n        if transpose:\n            q = q.transpose(-1, -2)\n        return q.contiguous()\n\n    def right_inverse(self, w):\n        return w\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedQROrthogonalization.__init__","title":"<code>__init__(weight_shape)</code>","text":"<p>Initialize the BatchedQROrthogonalization module.</p> <p>This module uses QR decomposition to orthogonalize a weight matrix in a batched manner. It computes the orthogonal component (<code>Q</code>) from the decomposition, ensuring that the output satisfies orthogonality constraints.</p> <p>Parameters:</p> Name Type Description Default <code>weight_shape</code> <code>tuple</code> <p>The shape of the weight matrix to be orthogonalized.</p> required Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, weight_shape):\n    \"\"\"\n    Initialize the BatchedQROrthogonalization module.\n\n    This module uses QR decomposition to orthogonalize a weight matrix in a batched manner.\n    It computes the orthogonal component (`Q`) from the decomposition, ensuring that the\n    output satisfies orthogonality constraints.\n\n    Args:\n        weight_shape (tuple): The shape of the weight matrix to be orthogonalized.\n    \"\"\"\n    super(BatchedQROrthogonalization, self).__init__()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.BatchedQROrthogonalization.forward","title":"<code>forward(w)</code>","text":"<p>Perform QR decomposition to compute the orthogonalized weight matrix.</p> <p>The QR decomposition splits the input matrix into an orthogonal matrix (<code>Q</code>) and an upper triangular matrix (<code>R</code>). This module returns the orthogonal component.</p> <p>Parameters:</p> Name Type Description Default <code>w</code> <code>Tensor</code> <p>The weight matrix to be orthogonalized.</p> required <p>Returns:</p> Type Description <p>torch.Tensor: The orthogonalized weight matrix (<code>Q</code> from the QR decomposition).</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def forward(self, w):\n    \"\"\"\n    Perform QR decomposition to compute the orthogonalized weight matrix.\n\n    The QR decomposition splits the input matrix into an orthogonal matrix (`Q`) and\n    an upper triangular matrix (`R`). This module returns the orthogonal component.\n\n    Args:\n        w (torch.Tensor): The weight matrix to be orthogonalized.\n\n    Returns:\n        torch.Tensor: The orthogonalized weight matrix (`Q` from the QR decomposition).\n    \"\"\"\n    transpose = w.shape[-2] &lt; w.shape[-1]\n    if transpose:\n        w = w.transpose(-1, -2)\n    q, r = torch.linalg.qr(w, mode=\"reduced\")\n    # compute the sign of the diagonal of d\n    diag_sign = torch.sign(torch.diagonal(r, dim1=-2, dim2=-1)).unsqueeze(-2)\n    # multiply the sign with the diagonal of r\n    q = q * diag_sign\n    if transpose:\n        q = q.transpose(-1, -2)\n    return q.contiguous()\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.L2Normalize","title":"<code>L2Normalize</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>class L2Normalize(nn.Module):\n    def __init__(self, dtype, dim=None):\n        \"\"\"\n        A class that performs L2 normalization for the given input tensor.\n\n        L2 normalization is a process that normalizes the input over a specified\n        dimension such that the sum of squares of the elements along that\n        dimension equals 1. It ensures that the resulting tensor has a unit norm.\n        This operation is widely used in machine learning and deep learning\n        applications to standardize feature representations.\n\n        Attributes:\n            dim (Optional[int]): The specific dimension along which normalization\n                is performed. If None, normalization is done over all dimensions.\n            dtype (Any): The data type of the tensor to be normalized.\n\n        Parameters:\n            dtype: The data type of the tensor to be normalized.\n            dim: An optional integer specifying the dimension along which to\n                normalize. If not provided, the input will be normalized globally\n                across all dimensions.\n        \"\"\"\n        super(L2Normalize, self).__init__()\n        self.dim = dim\n        self.dtype = dtype\n\n    def forward(self, x):\n        return x / (torch.norm(x, dim=self.dim, keepdim=True, dtype=self.dtype) + 1e-8)\n\n    def right_inverse(self, x):\n        return x / (torch.norm(x, dim=self.dim, keepdim=True, dtype=self.dtype) + 1e-8)\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.L2Normalize.__init__","title":"<code>__init__(dtype, dim=None)</code>","text":"<p>A class that performs L2 normalization for the given input tensor.</p> <p>L2 normalization is a process that normalizes the input over a specified dimension such that the sum of squares of the elements along that dimension equals 1. It ensures that the resulting tensor has a unit norm. This operation is widely used in machine learning and deep learning applications to standardize feature representations.</p> <p>Attributes:</p> Name Type Description <code>dim</code> <code>Optional[int]</code> <p>The specific dimension along which normalization is performed. If None, normalization is done over all dimensions.</p> <code>dtype</code> <code>Any</code> <p>The data type of the tensor to be normalized.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <p>The data type of the tensor to be normalized.</p> required <code>dim</code> <p>An optional integer specifying the dimension along which to normalize. If not provided, the input will be normalized globally across all dimensions.</p> <code>None</code> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>def __init__(self, dtype, dim=None):\n    \"\"\"\n    A class that performs L2 normalization for the given input tensor.\n\n    L2 normalization is a process that normalizes the input over a specified\n    dimension such that the sum of squares of the elements along that\n    dimension equals 1. It ensures that the resulting tensor has a unit norm.\n    This operation is widely used in machine learning and deep learning\n    applications to standardize feature representations.\n\n    Attributes:\n        dim (Optional[int]): The specific dimension along which normalization\n            is performed. If None, normalization is done over all dimensions.\n        dtype (Any): The data type of the tensor to be normalized.\n\n    Parameters:\n        dtype: The data type of the tensor to be normalized.\n        dim: An optional integer specifying the dimension along which to\n            normalize. If not provided, the input will be normalized globally\n            across all dimensions.\n    \"\"\"\n    super(L2Normalize, self).__init__()\n    self.dim = dim\n    self.dtype = dtype\n</code></pre>"},{"location":"api/reparametrizers/#orthogonium.reparametrizers.OrthoParams","title":"<code>OrthoParams</code>  <code>dataclass</code>","text":"<p>Represents the parameters and configurations used for orthogonalization and spectral normalization.</p> <p>This class encapsulates the necessary modules and settings required for performing spectral normalization and orthogonalization of tensors in a parameterized way. It accommodates various implementations of normalizers and orthogonalization techniques to provide flexibility in their application. This way we can easily switch between different normalization techniques inside our layer despite that each normalization have different parameters.</p> <p>Attributes:</p> Name Type Description <code>spectral_normalizer</code> <code>Callable[Tuple[int, ...], Module]</code> <p>A callable that produces a module for spectral normalization. Default is configured to use BatchedPowerIteration with specific parameters. This callable can be provided either as a <code>functool.partial</code> or as a <code>orthogonium.ClassParam</code>. It will recieve the shape of the weight tensor as its argument.</p> <code>orthogonalizer</code> <code>Callable[Tuple[int, ...], Module]</code> <p>A callable that produces a module for orthogonalization. Default is configured to use BatchedBjorckOrthogonalization with specific parameters. This callable can be provided either as a <code>functool.partial</code> or as a <code>orthogonium.ClassParam</code>. It will recieve the shape of the weight tensor as its argument.</p> Source code in <code>orthogonium\\reparametrizers.py</code> <pre><code>@dataclass\nclass OrthoParams:\n    \"\"\"\n    Represents the parameters and configurations used for orthogonalization\n    and spectral normalization.\n\n    This class encapsulates the necessary modules and settings required\n    for performing spectral normalization and orthogonalization of tensors\n    in a parameterized way. It accommodates various implementations of\n    normalizers and orthogonalization techniques to provide flexibility\n    in their application. This way we can easily switch between different\n    normalization techniques inside our layer despite that each normalization\n    have different parameters.\n\n    Attributes:\n        spectral_normalizer (Callable[Tuple[int, ...], nn.Module]): A callable\n            that produces a module for spectral normalization. Default is\n            configured to use BatchedPowerIteration with specific parameters.\n            This callable can be provided either as a `functool.partial` or as a\n            `orthogonium.ClassParam`. It will recieve the shape of the weight tensor as its\n            argument.\n        orthogonalizer (Callable[Tuple[int, ...], nn.Module]): A callable\n            that produces a module for orthogonalization. Default is\n            configured to use BatchedBjorckOrthogonalization with specific\n            parameters. This callable can be provided either as a `functool.partial` or as a\n            `orthogonium.ClassParam`. It will recieve the shape of the weight tensor as its argument.\n    \"\"\"\n\n    # spectral_normalizer: Callable[Tuple[int, ...], nn.Module] = BatchedIdentity\n    spectral_normalizer: Callable[Tuple[int, ...], nn.Module] = ClassParam(  # type: ignore\n        BatchedPowerIteration, power_it_niter=3, eps=1e-6\n    )\n    orthogonalizer: Callable[Tuple[int, ...], nn.Module] = ClassParam(  # type: ignore\n        BatchedBjorckOrthogonalization,\n        beta=0.5,\n        niters=12,\n        pass_through=False,\n        # ClassParam(BatchedExponentialOrthogonalization, niters=12)\n        # BatchedCholeskyOrthogonalization,\n        # BatchedQROrthogonalization,\n    )\n</code></pre>"},{"location":"api/singular_values/","title":"singular values","text":""},{"location":"api/singular_values/#orthogonium.layers.conv.singular_values.get_sv.get_conv_sv","title":"<code>get_conv_sv(layer, n_iter, agg_groups=True, return_stab_rank=True, device=None, detach=True, imsize=None)</code>","text":"<p>Computes Lipschitz constant (and optional 'stability rank') of a convolution layer. This use the layer paramaters to decide the correct function to call depending the the padding mode, the shape of the kernel and the stride. Under the hood it uses the methods described in [1] and [2].</p> <p>Parameters:</p> Name Type Description Default <code>layer</code> <code>Module</code> <p>Convolutional layer to compute the Lipschitz constant for. It must have a weight attribute. This function is expected to work only for layer in this library as striding is handled using the fact that our layers uses two subkernels in this situation.</p> required <code>n_iter</code> <code>int</code> <p>Number of iterations for the Delattre algorithm.</p> required <code>agg_groups</code> <code>bool</code> <p>If True, the Lipschitz constant is computed for the entire layer. If False, the Lipschitz constant is computed for each group separately. Default is True.</p> <code>True</code> <code>return_stab_rank</code> <code>bool</code> <p>If True, the function also returns the 'stability rank' of the layer (normalized to be a fraction of the full rank). When agg_groups = True the average stable rank over groups is returned. Default is True.</p> <code>True</code> <code>device</code> <code>device</code> <p>Device to use for the computation. Default is None.</p> <code>None</code> <code>detach</code> <code>bool</code> <p>If True, the result is detached from the computation graph. Default is True.</p> <code>True</code> <code>imsize</code> <code>int</code> <p>Size of the input image. Required for circular padding. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <p>float or tuple: Lipschitz constant of the layer. If return_stab_rank is True, the function returns a tuple</p> References <ul> <li>[1] Delattre, B., Barth\u00e9lemy, Q., Araujo, A., &amp; Allauzen, A. (2023, July). Efficient bound of Lipschitz constant for convolutional layers by gram iteration. In International Conference on Machine Learning (pp. 7513-7532). PMLR. https://arxiv.org/abs/2305.16173</li> <li>[2] Delattre, B., Barth\u00e9lemy, Q., &amp; Allauzen, A. (2024). Spectral Norm of Convolutional Layers with Circular and Zero Paddings. https://arxiv.org/abs/2402.00240</li> </ul> Source code in <code>orthogonium\\layers\\conv\\singular_values\\get_sv.py</code> <pre><code>def get_conv_sv(\n    layer,\n    n_iter,\n    agg_groups=True,\n    return_stab_rank=True,\n    device=None,\n    detach=True,\n    imsize=None,\n):\n    \"\"\"\n    Computes Lipschitz constant (and optional 'stability rank') of a convolution layer. This use the layer paramaters\n    to decide the correct function to call depending the the padding mode, the shape of the kernel and the stride.\n    Under the hood it uses the methods described in [1] and [2].\n\n    Parameters:\n        layer (torch.nn.Module): Convolutional layer to compute the Lipschitz constant for. It must have a weight\n            attribute. This function is expected to work only for layer in this library as striding is handled using\n            the fact that our layers uses two subkernels in this situation.\n        n_iter (int): Number of iterations for the Delattre algorithm.\n        agg_groups (bool, optional): If True, the Lipschitz constant is computed for the entire layer. If False, the\n            Lipschitz constant is computed for each group separately. Default is True.\n        return_stab_rank (bool, optional): If True, the function also returns the 'stability rank' of the layer\n            (normalized to be a fraction of the full rank). When agg_groups = True the average stable rank over groups\n            is returned. Default is True.\n        device (torch.device, optional): Device to use for the computation. Default is None.\n        detach (bool, optional): If True, the result is detached from the computation graph. Default is True.\n        imsize (int, optional): Size of the input image. Required for circular padding. Default is None.\n\n    Returns:\n        float or tuple: Lipschitz constant of the layer. If return_stab_rank is True, the function returns a tuple\n\n    Warnings:\n        There is currently an issue when estimating the lipschitz constant of a layer with circular padding and\n        asymmetric padding (ie. even kernel size and no stride). The function may return a lipschitz constant lower\n        than the actual value.\n\n    References:\n        - [1] Delattre, B., Barth\u00e9lemy, Q., Araujo, A., &amp; Allauzen, A. (2023, July).\n        Efficient bound of Lipschitz constant for convolutional layers by gram iteration.\n        In International Conference on Machine Learning (pp. 7513-7532). PMLR.\n        &lt;https://arxiv.org/abs/2305.16173&gt;\n        - [2] Delattre, B., Barth\u00e9lemy, Q., &amp; Allauzen, A. (2024).\n        Spectral Norm of Convolutional Layers with Circular and Zero Paddings.\n        &lt;https://arxiv.org/abs/2402.00240&gt;\n    \"\"\"\n\n    def _compute_grouped_lip(\n        weight, stride, padding_mode, return_stab_rank, imsize, n_iter\n    ):\n        \"\"\"\n        Internal helper: if groups&gt;1, reshapes the kernel to (g, co/g, ci/g, kw, kh)\n        and applies _compute_lip_subkernel to each 'slice' in the batch.\n\n        Returns either a list of per-group results or a single aggregated value\n        (e.g., multiplied across groups) based on agg_groups.\n        \"\"\"\n\n        g = layer.groups\n        # Original shape is (co, ci // g, kw, kh).\n        # Reshape to (g, co // g, ci // g, kw, kh).\n        co, ci_over_g, kw, kh = weight.shape\n        # sanity checks: co should be multiple of g, etc.\n        assert co % g == 0, \"Number of output channels must be divisible by groups.\"\n\n        weight_reshaped = weight.view(g, co // g, ci_over_g, kw, kh)\n\n        # Compute lip subkernel for each group slice\n        group_results = []\n        for i in range(g):\n            w_i = weight_reshaped[i]\n            lip_val = _compute_lip_subkernel(\n                w_i,\n                stride[0],\n                stride[1],\n                padding_mode,\n                return_stab_rank,\n                imsize,\n                n_iter,\n            )\n            group_results.append(lip_val)\n\n        # If we want to aggregate across all groups, define how to combine them\n        if agg_groups:\n            if return_stab_rank:\n                # In this scenario, each lip_val is a tuple: (lip_factor, stab_rank)\n                lip_prod = group_results[0][0]\n                stab_prod = group_results[0][1] / g\n                for i in range(1, g):\n                    lip_prod = lip_prod * group_results[i][0]\n                    stab_prod = stab_prod + group_results[i][1] / g\n                return (lip_prod, stab_prod)\n            else:\n                # If we only compute one scalar per group, multiply them\n                lip_prod = group_results[0]\n                for i in range(1, g):\n                    lip_prod = lip_prod * group_results[i]\n                return lip_prod\n        else:\n            # Return list of group-wise results without aggregation\n            return group_results\n\n    def _maybe_detach(res):\n        \"\"\"\n        Helper to detach results if required.\n        Return type is preserved (scalar vs tuple).\n        \"\"\"\n        if detach:\n            if return_stab_rank and isinstance(res, tuple):\n                return tuple([r.detach() for r in res])\n            elif isinstance(res, (list, tuple)):\n                return [r.detach() if hasattr(r, \"detach\") else r for r in res]\n            else:\n                return res.detach() if hasattr(res, \"detach\") else res\n        return res\n\n    #\n    # Main body of get_lipschitz_conv\n    #\n    # 1) Check whether layer has one weight or (weight_1, weight_2).\n    # 2) Check groups&gt;1 and handle accordingly.\n    #\n    if hasattr(layer, \"weight_1\"):\n        # The layer has two separate weight tensors\n        res_1 = _compute_grouped_lip(\n            layer.weight_1.to(device),\n            stride=(1, 1),  # presumably no stride for weight_1\n            padding_mode=layer.padding_mode,\n            return_stab_rank=return_stab_rank,\n            imsize=imsize,\n            n_iter=n_iter,\n        )\n        res_2 = _compute_grouped_lip(\n            layer.weight_2.to(device),\n            stride=(layer.stride[0], layer.stride[1]),\n            padding_mode=layer.padding_mode,\n            return_stab_rank=return_stab_rank,\n            imsize=imsize,\n            n_iter=n_iter,\n        )\n\n        # Combine the results from res_1 and res_2\n        if return_stab_rank:\n            # Each is a tuple: (lip_factor, stab_rank)\n            combined = (res_1[0] * res_2[0], 0.5 * res_1[1] + 0.5 * res_2[1])\n        else:\n            combined = res_1 * res_2\n\n        return _maybe_detach(combined)\n    else:\n        # The layer has a single weight\n        # If groups&gt;1, _compute_grouped_lip will handle the reshape/apply logic\n        res = _compute_grouped_lip(\n            layer.weight.to(device),\n            stride=(layer.stride[0], layer.stride[1]),\n            padding_mode=layer.padding_mode,\n            return_stab_rank=return_stab_rank,\n            imsize=imsize,\n            n_iter=n_iter,\n        )\n        return _maybe_detach(res)\n</code></pre>"},{"location":"notebooks/demo_cifar_classification/","title":"Demo cifar classification","text":"<pre><code># !pip install orthogonium lightning rich schedulefree\n</code></pre> <pre><code>import math\nimport os\n\nimport schedulefree\nimport torch\nimport torch.utils.data\nimport torchmetrics\nfrom lightning.pytorch import callbacks as pl_callbacks\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch import LightningModule, LightningDataModule\nfrom torchinfo import summary\n# from lightning.pytorch.loggers import WandbLogger  # Uncomment if using Wandb logging\nfrom torch.nn import AvgPool2d\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import Compose, Normalize, RandAugment, RandomHorizontalFlip, RandomResizedCrop, ToTensor\n\nfrom orthogonium.model_factory.classparam import ClassParam\nfrom orthogonium.layers.conv.AOC import AdaptiveOrthoConv2d\nfrom orthogonium.layers.linear import OrthoLinear\nfrom orthogonium.layers.custom_activations import MaxMin\nfrom orthogonium.losses import LossXent, CosineLoss\nfrom orthogonium.losses import VRA\nfrom orthogonium.model_factory.models_factory import StagedCNN, PatchBasedExapandedCNN\n\n# Enable benchmark mode and set matmul precision for performance tuning\ntorch.backends.cudnn.benchmark = True\ntorch.set_float32_matmul_precision(\"medium\")\n</code></pre> <pre><code>settings = {\n    \"non_robust\": {\n        \"loss\": CosineLoss,\n        \"epochs\": 60,\n    },\n    \"mildly_robust\": {\n        \"loss\": ClassParam(\n            LossXent,\n            n_classes=10,\n            offset=(math.sqrt(2) / 0.1983) * (36 / 255),  # aiming for 36/255 verified robust accuracy\n            temperature=0.125,\n        ),\n        \"epochs\": 150,\n    },\n    \"robust\": {\n        \"loss\": ClassParam(\n            LossXent,\n            n_classes=10,\n            offset=(math.sqrt(2) / 0.1983) * (72 / 255),  # aiming for 72/255 verified robust accuracy\n            temperature=0.25,\n        ),\n        \"epochs\": 150,\n    },\n}\n</code></pre> <pre><code>class Cifar10DataModule(LightningDataModule):\n    # Dataset configuration\n    _BATCH_SIZE = 256\n    _NUM_WORKERS = 8  # Number of parallel processes for data loading\n    _PREPROCESSING_PARAMS = {\n        \"img_mean\": (0.41757566, 0.26098573, 0.25888634),\n        \"img_std\": (0.21938758, 0.1983, 0.19342837),\n        \"crop_size\": 32,\n        \"horizontal_flip_prob\": 0.5,\n        \"random_resized_crop_params\": {\n            \"scale\": (0.5, 1.0),\n            \"ratio\": (3.0 / 4.0, 4.0 / 3.0),\n        },\n    }\n\n    def train_dataloader(self):\n        # Define the transformations for training data\n        transform = Compose(\n            [\n                RandomResizedCrop(\n                    self._PREPROCESSING_PARAMS[\"crop_size\"],\n                    **self._PREPROCESSING_PARAMS[\"random_resized_crop_params\"],\n                ),\n                RandomHorizontalFlip(self._PREPROCESSING_PARAMS[\"horizontal_flip_prob\"]),\n                # Uncomment the following line to use RandAugment\n                # RandAugment(**self._PREPROCESSING_PARAMS[\"randaug_params\"]),\n                ToTensor(),\n                Normalize(\n                    mean=self._PREPROCESSING_PARAMS[\"img_mean\"],\n                    std=self._PREPROCESSING_PARAMS[\"img_std\"],\n                ),\n            ]\n        )\n\n        train_dataset = CIFAR10(\n            root=\"./data\",\n            train=True,\n            download=True,\n            transform=transform,\n        )\n\n        return DataLoader(\n            train_dataset,\n            batch_size=self._BATCH_SIZE,\n            num_workers=self._NUM_WORKERS,\n            prefetch_factor=2,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        # Define the transformations for validation data\n        transform = Compose(\n            [\n                ToTensor(),\n                Normalize(\n                    mean=self._PREPROCESSING_PARAMS[\"img_mean\"],\n                    std=self._PREPROCESSING_PARAMS[\"img_std\"],\n                ),\n            ]\n        )\n\n        val_dataset = CIFAR10(\n            root=\"./data\",\n            train=False,\n            download=True,\n            transform=transform,\n        )\n\n        return DataLoader(\n            val_dataset,\n            batch_size=self._BATCH_SIZE * 4,\n            num_workers=self._NUM_WORKERS,\n            shuffle=False,\n        )\n</code></pre> <pre><code>class ClassificationLightningModule(LightningModule):\n    def __init__(self, num_classes=10, loss=None):\n        super().__init__()\n        self.num_classes = num_classes\n        self.model = PatchBasedExapandedCNN(\n            img_shape=(3, 32, 32),\n            dim=256,\n            depth=12,\n            kernel_size=3,\n            patch_size=2,\n            expand_factor=2,\n            groups=None,\n            n_classes=10,\n            skip=True,\n            conv=ClassParam(\n                AdaptiveOrthoConv2d,\n                bias=False,\n                padding=\"same\",\n                padding_mode=\"zeros\",\n            ),\n            act=ClassParam(MaxMin),\n            pool=ClassParam(\n                AdaptiveOrthoConv2d,\n                in_channels=256,\n                out_channels=256,\n                groups=128,\n                bias=False,\n                padding=0,\n                kernel_size=16,\n                stride=16,\n            ),\n            lin=ClassParam(OrthoLinear, bias=False),\n            norm=None,\n        )\n        self.criteria = loss() if loss is not None else torch.nn.CrossEntropyLoss()\n        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.train_vra = torchmetrics.MeanMetric()\n        self.val_vra = torchmetrics.MeanMetric()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        self.model.train()\n        img, label = batch\n        y_hat = self.model(img)\n        loss = self.criteria(y_hat, label)\n        self.train_acc(y_hat, label)\n        self.train_vra(\n            VRA(\n                y_hat,\n                label,\n                L=1 / min(Cifar10DataModule._PREPROCESSING_PARAMS[\"img_std\"]),\n                eps=36 / 255,\n                last_layer_type=\"global\",\n            )\n        )\n        self.log(\"loss\", loss, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n        self.log(\"accuracy\", self.train_acc, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n        self.log(\"vra\", self.train_vra, on_epoch=True, on_step=True, prog_bar=True, sync_dist=False)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        self.model.eval()\n        img, label = batch\n        y_hat = self.model(img)\n        loss = self.criteria(y_hat, label)\n        self.val_acc(y_hat, label)\n        self.val_vra(\n            VRA(\n                y_hat,\n                label,\n                L=1 / min(Cifar10DataModule._PREPROCESSING_PARAMS[\"img_std\"]),\n                eps=36 / 255,\n                last_layer_type=\"global\",\n            )\n        )\n        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n        self.log(\"val_accuracy\", self.val_acc, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n        self.log(\"val_vra\", self.val_vra, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n        return loss\n\n    def configure_optimizers(self):\n        # Setup the Adam optimizer with schedule-free updates.\n        optimizer = schedulefree.AdamWScheduleFree(self.parameters(), lr=5e-3, weight_decay=0)\n        optimizer.train()\n        self.hparams[\"lr\"] = optimizer.param_groups[0][\"lr\"]\n        return optimizer\n</code></pre> <pre>\n<code>Using bfloat16 Automatic Mixed Precision (AMP)\nGPU available: True (cuda), used: True\nTPU available: False, using: 0 TPU cores\nHPU available: False, using: 0 HPUs\n/mnt/deel/data/thibaut.boissin/envs/ortho/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/thibaut.boissin/projects/orthogonium/docs/notebooks/checkpoints exists and is not empty.\nLOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n\n  | Name      | Type               | Params | Mode \n---------------------------------------------------------\n0 | model     | Sequential         | 1.9 M  | train\n1 | criteria  | CosineLoss         | 0      | train\n2 | train_acc | MulticlassAccuracy | 0      | train\n3 | val_acc   | MulticlassAccuracy | 0      | train\n4 | train_vra | MeanMetric         | 0      | train\n5 | val_vra   | MeanMetric         | 0      | train\n---------------------------------------------------------\n1.8 M     Trainable params\n130 K     Non-trainable params\n1.9 M     Total params\n7.657     Total estimated model params size (MB)\n352       Modules in train mode\n0         Modules in eval mode\n</code>\n</pre> <pre>\n<code>==================================================================================================================================\nLayer (type:depth-idx)                                                           Output Shape              Param #\n==================================================================================================================================\nClassificationLightningModule                                                    [1, 10]                   --\n\u00e2\u201d\u0153\u00e2\u201d\u20acSequential: 1-1                                                                [1, 10]                   --\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acParametrizedRKOConv2d: 2-1                                                [1, 256, 16, 16]          --\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acModuleDict: 3-1                                                      --                        3,340\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-2                                                     [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-2                                                      [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-3                                                     [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-3                                                      [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-4                                                     [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-4                                                      [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-5                                                     [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-5                                                      [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-6                                                     [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-6                                                      [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-7                                                     [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-7                                                      [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-8                                                     [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-8                                                      [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-9                                                     [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-9                                                      [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-10                                                    [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-10                                                     [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-11                                                    [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-11                                                     [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-12                                                    [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-12                                                     [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acAdditiveResidual: 2-13                                                    [1, 256, 16, 16]          1\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acSequential: 3-13                                                     [1, 256, 16, 16]          142,592\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acParametrizedRKOConv2d: 2-14                                               [1, 256, 1, 1]            --\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acModuleDict: 3-14                                                     --                        196,864\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acFlatten: 2-15                                                             [1, 256]                  --\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acMaxMin: 2-16                                                              [1, 256]                  --\n\u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acParametrizedOrthoLinear: 2-17                                             [1, 10]                   --\n\u00e2\u201d\u201a    \u00e2\u201d\u201a    \u00e2\u201d\u201d\u00e2\u201d\u20acModuleDict: 3-15                                                     --                        2,826\n==================================================================================================================================\nTotal params: 1,914,146\nTrainable params: 1,783,308\nNon-trainable params: 130,838\nTotal mult-adds (Units.MEGABYTES): 0\n==================================================================================================================================\nInput size (MB): 0.01\nForward/backward pass size (MB): 0.00\nParams size (MB): 0.00\nEstimated Total Size (MB): 0.01\n==================================================================================================================================\n</code>\n</pre> <pre>\n<code>Sanity Checking: |                                                                                            \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Files already downloaded and verified\nFiles already downloaded and verified\n</code>\n</pre> <pre>\n<code>Training: |                                                                                                   \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>Validation: |                                                                                                 \u00e2\u20ac\u00a6</code>\n</pre> <pre>\n<code>`Trainer.fit` stopped: `max_epochs=60` reached.\n</code>\n</pre>"},{"location":"notebooks/demo_cifar_classification/#training-a-1-lipschitz-constrained-network-on-cifar10-with-orthogonium","title":"Training a 1-Lipschitz constrained network on CIFAR10 with Orthogonium","text":""},{"location":"notebooks/demo_cifar_classification/#lipschitz-constrained-networks-and-certifiable-robustness","title":"Lipschitz-Constrained Networks and Certifiable Robustness","text":"<p>What is a Lipschitz Network? A Lipschitz network is a neural network in which each layer is constrained to be a 1-Lipschitz function. This means that small changes in the input lead to only small changes in the output, ensuring controlled sensitivity throughout the network. The overall Lipschitz constant of the network is usually estimated as the product of the Lipschitz constants of its individual layers. However, this bound is often loose and difficult to compute exactly.</p> <p>How to Build Lipschitz Networks? To construct such networks: - Orthogonal Layers: Use layers that enforce orthogonality constraints (e.g., Adaptive OrthoConvolutions). These layers are designed to strictly represent 1-Lipschitz functions. - Special Activations: Incorporate activations like MaxMin which, when combined with orthogonal layers, help in obtaining a tight estimation of the network's Lipschitz constant. - Reparametrization Techniques: Methods such as AOC (Adaptive OrthoConvolutions) ensure that each layer adheres to the 1-Lipschitz constraint, making the overall bound much tighter compared to a simple product of individual bounds.</p> <p>Certifiable Robustness Certifiable robustness provides a guarantee on the minimal perturbation needed to alter the network's prediction, independent of any specific adversarial attack. For a 1-Lipschitz classification function $ f $ with $ f(x)l $ representing the logit for the true class and $ f(x)_i $ for any other class, a robustness certificate in the $ L_2 $ norm is given by: $$ \\epsilon \\geq \\frac{f(x)_l - \\max $$ This means that as long as the perturbation remains below $ \\epsilon $, the classification will not change. This certificate is: - } f(x)_i}{\\sqrt{2}Independent of Attacks: It does not rely on any particular adversarial attack method, ensuring that the guarantee remains valid even as new attack strategies emerge. - Computationally Efficient: The certificate can be computed cheaply and even integrated as a loss term during training, leading to models that are robust by design.</p> <p>Applications and Benefits Lipschitz-constrained networks are not only crucial for certifiable robustness but also have broader applications: - They are tightly linked with generative models like WGANs and concepts in optimal transport. - They enable scalable differential privacy and help avoid singularities in models such as diffusion networks. - They guarantee existence and uniqueness in classification tasks, making them appealing for reliable machine learning.</p> <p>In summary, by combining orthogonal layers with appropriate activations and reparametrization techniques, one can build Lipschitz networks that not only deliver competitive performance but also offer provable robustness guarantees.</p>"},{"location":"notebooks/demo_cifar_classification/#training-settings","title":"Training Settings","text":"<p>You can play with the training settings to explore different configurations and compare their performance. The settings include:</p> <p>Training settings include: - non_robust: Cosine Similarity loss training. - mildly_robust: Cross Entropy Loss includes a high margin targeting a VRA of 36/255, resulting in 42% VRA. - robust: Similar to mildly robust, but with settings that push towards 72/255 verified robust accuracy, resulting in 47% VRA.</p> <p>&gt; Note: The aim here is to show the training flow rather than reach state-of-the-art performance.</p>"},{"location":"notebooks/demo_cifar_classification/#training-settings-performance","title":"Training Settings Performance","text":"Setting Epochs Accuracy Verified Robust Accuracy (VRA) non_robust 60 88.5% 0% mildly_robust 150 75% 42% robust 150 71% 47% <p>These configurations are stored in the <code>settings</code> dictionary.</p>"},{"location":"notebooks/demo_cifar_classification/#data-module-cifar10","title":"Data Module: CIFAR10","text":"<p>We create a <code>LightningDataModule</code> to load and preprocess the CIFAR10 training and validation datasets.</p> <p>The training dataloader applies several transforms: - Random resized cropping - Random horizontal flip - Normalization using precomputed mean and standard deviation</p> <p>The validation dataloader only applies tensor conversion and normalization.</p>"},{"location":"notebooks/demo_cifar_classification/#classification-model-module","title":"Classification Model Module","text":"<p>We now define a <code>LightningModule</code> that wraps our CNN model. The network uses the <code>PatchBasedExapandedCNN</code> factory method from orthogonium.</p> <p>The architecture consists of 4 main parts: - The stem is a patch extractor: a convolution whose kernel size equals the stride. - A sequence of residual block: each residual features a learnable factor to ensure its Lipschitzness. In each residual, there is one depthwise convolution, A MaxMin activation, and a pointwise convolution. No pooling is performed in this part of the network. - A pooling layer: here, we use a depthwise convolution whose kernel size equals the image size. This allows for the localization of features without using a large amount of weight. (this is not mandatory for accurate training but seems to obtain a slightly better accuracy / robustness tradeoff in robust training). - a Fully connected layer for classification.</p> <p>All convolutional layers use AOC, allowing the construction of complex Lipschitz-constrained architectures.</p> <p>Key components include: - The custom CNN model architecture. - The loss function (set based on the selected training configuration). - Training and validation steps that compute and log both accuracy and verified robust accuracy (VRA). - The <code>configure_optimizers</code> method which sets up the Adam optimizer with schedule-free updates.</p>"},{"location":"notebooks/demo_cifar_classification/#training-routine","title":"Training Routine","text":"<p>For example, to run a non robust training setting, set:</p> <pre><code>train_setting = \"non_robust\"\n\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;div class=\"cell border-box-sizing code_cell rendered\" markdown=\"1\"&gt;\n&lt;div class=\"input\"&gt;\n\n```python\n# Select the training setting manually.\ntrain_setting = \"non_robust\"  # Options: \"non_robust\", \"mildly_robust\", or \"robust\"\n\n# Get the corresponding loss function and number of epochs from the settings.\ncurrent_setting = settings[train_setting]\n\n# Instantiate the classification model and data module.\nclassification_module = ClassificationLightningModule(num_classes=10, loss=current_setting[\"loss\"])\ndata_module = Cifar10DataModule()\n\n# Optionally, set up a logger or callbacks if needed.\n# For example, if using Wandb:\n# from lightning.pytorch.loggers import WandbLogger\n# wandb_logger = WandbLogger(project=\"lipschitz-robust-cifar10\", log_model=True)\n# checkpoint_callback = pl_callbacks.ModelCheckpoint(\n#     monitor=\"loss\",\n#     mode=\"min\",\n#     save_top_k=1,\n#     save_last=True,\n#     dirpath=f\"./checkpoints/{wandb_logger.experiment.dir}\",\n# )\n\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=[1],             # Use 1 GPU set to -1 for all GPUs\n    num_nodes=1,            # Number of nodes\n    # strategy=\"ddp_spawn\",         # Distributed strategy\n    precision=\"bf16-mixed\", # Mixed precision training\n    max_epochs=current_setting[\"epochs\"],\n    enable_model_summary=False,\n    # logger=[wandb_logger],  # Uncomment to enable Wandb logging\n    logger=False,\n    callbacks=[\n        # You can add callbacks here, e.g.:\n        # pl_callbacks.LearningRateFinder(max_lr=0.05),\n        # checkpoint_callback,\n    ],\n)\n\nprint(summary(classification_module, input_size=(1, 3, 32, 32)))\n# Start training\ntrainer.fit(classification_module, data_module)\n\n# Optionally, you can save the trained model afterwards:\n# torch.save(classification_module.model.state_dict(), \"single_stage.pth\")\n</code></pre>"},{"location":"notebooks/demo_cifar_classification/#next-steps","title":"Next Steps","text":"<ul> <li>Model Evaluation: You can add a new cell to perform model evaluation or predictions.</li> <li>Logging and Checkpoints: To enable model logging or checkpoint saving, uncomment the corresponding lines and configure as needed.</li> <li>Experiment with Settings: Change the <code>train_setting</code> variable to <code>\"mildly_robust\"</code> or <code>\"robust\"</code> to experiment with other training configurations.</li> </ul>"},{"location":"notebooks/demo_cifar_classification/","title":"Training a 1-Lipschitz constrained network on CIFAR10 with Orthogonium","text":""},{"location":"notebooks/demo_cifar_classification/#lipschitz-constrained-networks-and-certifiable-robustness","title":"Lipschitz-Constrained Networks and Certifiable Robustness","text":"<p>What is a Lipschitz Network? A Lipschitz network is a neural network in which each layer is constrained to be a 1-Lipschitz function. This means that small changes in the input lead to only small changes in the output, ensuring controlled sensitivity throughout the network. The overall Lipschitz constant of the network is usually estimated as the product of the Lipschitz constants of its individual layers. However, this bound is often loose and difficult to compute exactly.</p> <p>How to Build Lipschitz Networks? To construct such networks: - Orthogonal Layers: Use layers that enforce orthogonality constraints (e.g., Adaptive OrthoConvolutions). These layers are designed to strictly represent 1-Lipschitz functions. - Special Activations: Incorporate activations like MaxMin which, when combined with orthogonal layers, help in obtaining a tight estimation of the network's Lipschitz constant. - Reparametrization Techniques: Methods such as AOC (Adaptive OrthoConvolutions) ensure that each layer adheres to the 1-Lipschitz constraint, making the overall bound much tighter compared to a simple product of individual bounds.</p> <p>Certifiable Robustness Certifiable robustness provides a guarantee on the minimal perturbation needed to alter the network's prediction, independent of any specific adversarial attack. For a 1-Lipschitz classification function $ f $ with $ f(x)l $ representing the logit for the true class and $ f(x)_i $ for any other class, a robustness certificate in the $ L_2 $ norm is given by: $$ \\epsilon \\geq \\frac{f(x)_l - \\max $$ This means that as long as the perturbation remains below $ \\epsilon $, the classification will not change. This certificate is: - } f(x)_i}{\\sqrt{2}Independent of Attacks: It does not rely on any particular adversarial attack method, ensuring that the guarantee remains valid even as new attack strategies emerge. - Computationally Efficient: The certificate can be computed cheaply and even integrated as a loss term during training, leading to models that are robust by design.</p> <p>Applications and Benefits Lipschitz-constrained networks are not only crucial for certifiable robustness but also have broader applications: - They are tightly linked with generative models like WGANs and concepts in optimal transport. - They enable scalable differential privacy and help avoid singularities in models such as diffusion networks. - They guarantee existence and uniqueness in classification tasks, making them appealing for reliable machine learning.</p> <p>In summary, by combining orthogonal layers with appropriate activations and reparametrization techniques, one can build Lipschitz networks that not only deliver competitive performance but also offer provable robustness guarantees.</p> <pre><code># !pip install orthogonium lightning rich schedulefree\n</code></pre> <pre><code>import math\nimport os\n\nimport schedulefree\nimport torch\nimport torch.utils.data\nimport torchmetrics\nfrom lightning.pytorch import callbacks as pl_callbacks\nfrom lightning.pytorch import Trainer\nfrom lightning.pytorch import LightningModule, LightningDataModule\nfrom torchinfo import summary\n# from lightning.pytorch.loggers import WandbLogger  # Uncomment if using Wandb logging\nfrom torch.nn import AvgPool2d\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR10\nfrom torchvision.transforms import Compose, Normalize, RandAugment, RandomHorizontalFlip, RandomResizedCrop, ToTensor\n\nfrom orthogonium.model_factory.classparam import ClassParam\nfrom orthogonium.layers.conv.AOC import AdaptiveOrthoConv2d\nfrom orthogonium.layers.linear import OrthoLinear\nfrom orthogonium.layers.custom_activations import MaxMin\nfrom orthogonium.losses import LossXent, CosineLoss\nfrom orthogonium.losses import VRA\nfrom orthogonium.model_factory.models_factory import StagedCNN, PatchBasedExapandedCNN\n\n# Enable benchmark mode and set matmul precision for performance tuning\ntorch.backends.cudnn.benchmark = True\ntorch.set_float32_matmul_precision(\"medium\")\n</code></pre>"},{"location":"notebooks/demo_cifar_classification/#training-settings","title":"Training Settings","text":"<p>You can play with the training settings to explore different configurations and compare their performance. The settings include:</p> <p>Training settings include: - non_robust: Cosine Similarity loss training. - mildly_robust: Cross Entropy Loss includes a high margin targeting a VRA of 36/255, resulting in 42% VRA. - robust: Similar to mildly robust, but with settings that push towards 72/255 verified robust accuracy, resulting in 47% VRA.</p> <p>Note: The aim here is to show the training flow rather than reach state-of-the-art performance.</p>"},{"location":"notebooks/demo_cifar_classification/#training-settings-performance","title":"Training Settings Performance","text":"Setting Epochs Accuracy Verified Robust Accuracy (VRA) non_robust 60 88.5% 0% mildly_robust 150 75% 42% robust 150 71% 47% <p>These configurations are stored in the <code>settings</code> dictionary.</p> <pre><code>settings = {\n    \"non_robust\": {\n        \"loss\": CosineLoss,\n        \"epochs\": 60,\n    },\n    \"mildly_robust\": {\n        \"loss\": ClassParam(\n            LossXent,\n            n_classes=10,\n            offset=(math.sqrt(2) / 0.1983) * (36 / 255),  # aiming for 36/255 verified robust accuracy\n            temperature=0.125,\n        ),\n        \"epochs\": 150,\n    },\n    \"robust\": {\n        \"loss\": ClassParam(\n            LossXent,\n            n_classes=10,\n            offset=(math.sqrt(2) / 0.1983) * (72 / 255),  # aiming for 72/255 verified robust accuracy\n            temperature=0.25,\n        ),\n        \"epochs\": 150,\n    },\n}\n</code></pre>"},{"location":"notebooks/demo_cifar_classification/#data-module-cifar10","title":"Data Module: CIFAR10","text":"<p>We create a <code>LightningDataModule</code> to load and preprocess the CIFAR10 training and validation datasets.</p> <p>The training dataloader applies several transforms: - Random resized cropping - Random horizontal flip - Normalization using precomputed mean and standard deviation</p> <p>The validation dataloader only applies tensor conversion and normalization.</p> <pre><code>class Cifar10DataModule(LightningDataModule):\n    # Dataset configuration\n    _BATCH_SIZE = 256\n    _NUM_WORKERS = 8  # Number of parallel processes for data loading\n    _PREPROCESSING_PARAMS = {\n        \"img_mean\": (0.41757566, 0.26098573, 0.25888634),\n        \"img_std\": (0.21938758, 0.1983, 0.19342837),\n        \"crop_size\": 32,\n        \"horizontal_flip_prob\": 0.5,\n        \"random_resized_crop_params\": {\n            \"scale\": (0.25, 1.0),\n            \"ratio\": (3.0 / 4.0, 4.0 / 3.0),\n        },\n    }\n\n    def train_dataloader(self):\n        # Define the transformations for training data\n        transform = Compose(\n            [\n                RandomResizedCrop(\n                    self._PREPROCESSING_PARAMS[\"crop_size\"],\n                    **self._PREPROCESSING_PARAMS[\"random_resized_crop_params\"],\n                ),\n                RandomHorizontalFlip(self._PREPROCESSING_PARAMS[\"horizontal_flip_prob\"]),\n                # Uncomment the following line to use RandAugment\n                # RandAugment(**self._PREPROCESSING_PARAMS[\"randaug_params\"]),\n                ToTensor(),\n                Normalize(\n                    mean=self._PREPROCESSING_PARAMS[\"img_mean\"],\n                    std=self._PREPROCESSING_PARAMS[\"img_std\"],\n                ),\n            ]\n        )\n\n        train_dataset = CIFAR10(\n            root=\"./data\",\n            train=True,\n            download=True,\n            transform=transform,\n        )\n\n        return DataLoader(\n            train_dataset,\n            batch_size=self._BATCH_SIZE,\n            num_workers=self._NUM_WORKERS,\n            prefetch_factor=2,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        # Define the transformations for validation data\n        transform = Compose(\n            [\n                ToTensor(),\n                Normalize(\n                    mean=self._PREPROCESSING_PARAMS[\"img_mean\"],\n                    std=self._PREPROCESSING_PARAMS[\"img_std\"],\n                ),\n            ]\n        )\n\n        val_dataset = CIFAR10(\n            root=\"./data\",\n            train=False,\n            download=True,\n            transform=transform,\n        )\n\n        return DataLoader(\n            val_dataset,\n            batch_size=self._BATCH_SIZE * 4,\n            num_workers=self._NUM_WORKERS,\n            shuffle=False,\n        )\n</code></pre>"},{"location":"notebooks/demo_cifar_classification/#classification-model-module","title":"Classification Model Module","text":"<p>We now define a <code>LightningModule</code> that wraps our CNN model. The network uses the <code>PatchBasedExapandedCNN</code> factory method from orthogonium.</p> <p>The architecture consists of 4 main parts: - The stem is a patch extractor: a convolution whose kernel size equals the stride. - A sequence of residual block: each residual features a learnable factor to ensure its Lipschitzness. In each residual, there is one depthwise convolution, A MaxMin activation, and a pointwise convolution. No pooling is performed in this part of the network. - A pooling layer: here, we use a depthwise convolution whose kernel size equals the image size. This allows for the localization of features without using a large amount of weight. (this is not mandatory for accurate training but seems to obtain a slightly better accuracy / robustness tradeoff in robust training). - a Fully connected layer for classification.</p> <p>All convolutional layers use AOC, allowing the construction of complex Lipschitz-constrained architectures.</p> <p>Key components include: - The custom CNN model architecture. - The loss function (set based on the selected training configuration). - Training and validation steps that compute and log both accuracy and verified robust accuracy (VRA). - The <code>configure_optimizers</code> method which sets up the Adam optimizer with schedule-free updates.</p> <pre><code>class ClassificationLightningModule(LightningModule):\n    def __init__(self, num_classes=10, loss=None):\n        super().__init__()\n        self.num_classes = num_classes\n        self.model = PatchBasedExapandedCNN(\n            img_shape=(3, 32, 32),\n            dim=256,\n            depth=12,\n            kernel_size=3,\n            patch_size=2,\n            expand_factor=2,\n            groups=None,\n            n_classes=10,\n            skip=True,\n            conv=ClassParam(\n                AdaptiveOrthoConv2d,\n                bias=False,\n                padding=\"same\",\n                padding_mode=\"zeros\",\n            ),\n            act=ClassParam(MaxMin),\n            pool=ClassParam(\n                AdaptiveOrthoConv2d,\n                in_channels=256,\n                out_channels=256,\n                groups=128,\n                bias=False,\n                padding=0,\n                kernel_size=16,\n                stride=16,\n            ),\n            lin=ClassParam(OrthoLinear, bias=False),\n            norm=None,\n        )\n        self.criteria = loss() if loss is not None else torch.nn.CrossEntropyLoss()\n        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n        self.train_vra = torchmetrics.MeanMetric()\n        self.val_vra = torchmetrics.MeanMetric()\n\n    def forward(self, x):\n        return self.model(x)\n\n    def training_step(self, batch, batch_idx):\n        self.model.train()\n        img, label = batch\n        y_hat = self.model(img)\n        loss = self.criteria(y_hat, label)\n        self.train_acc(y_hat, label)\n        self.train_vra(\n            VRA(\n                y_hat,\n                label,\n                L=1 / min(Cifar10DataModule._PREPROCESSING_PARAMS[\"img_std\"]),\n                eps=36 / 255,\n                last_layer_type=\"global\",\n            )\n        )\n        self.log(\"loss\", loss, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n        self.log(\"accuracy\", self.train_acc, on_epoch=True, on_step=True, prog_bar=True, sync_dist=True)\n        self.log(\"vra\", self.train_vra, on_epoch=True, on_step=True, prog_bar=True, sync_dist=False)\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        self.model.eval()\n        img, label = batch\n        y_hat = self.model(img)\n        loss = self.criteria(y_hat, label)\n        self.val_acc(y_hat, label)\n        self.val_vra(\n            VRA(\n                y_hat,\n                label,\n                L=1 / min(Cifar10DataModule._PREPROCESSING_PARAMS[\"img_std\"]),\n                eps=36 / 255,\n                last_layer_type=\"global\",\n            )\n        )\n        self.log(\"val_loss\", loss, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n        self.log(\"val_accuracy\", self.val_acc, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n        self.log(\"val_vra\", self.val_vra, on_epoch=True, on_step=False, prog_bar=True, sync_dist=True)\n        return loss\n\n    def configure_optimizers(self):\n        # Setup the Adam optimizer with schedule-free updates.\n        optimizer = schedulefree.AdamWScheduleFree(self.parameters(), lr=5e-3, weight_decay=0)\n        optimizer.train()\n        self.hparams[\"lr\"] = optimizer.param_groups[0][\"lr\"]\n        return optimizer\n</code></pre>"},{"location":"notebooks/demo_cifar_classification/#training-routine","title":"Training Routine","text":"<p>For example, to run a non robust training setting, set:</p> <pre><code>train_setting = \"non_robust\"\n\n\n# Select the training setting manually.\ntrain_setting = \"non_robust\"  # Options: \"non_robust\", \"mildly_robust\", or \"robust\"\n\n# Get the corresponding loss function and number of epochs from the settings.\ncurrent_setting = settings[train_setting]\n\n# Instantiate the classification model and data module.\nclassification_module = ClassificationLightningModule(num_classes=10, loss=current_setting[\"loss\"])\ndata_module = Cifar10DataModule()\n\n# Optionally, set up a logger or callbacks if needed.\n# For example, if using Wandb:\n# from lightning.pytorch.loggers import WandbLogger\n# wandb_logger = WandbLogger(project=\"lipschitz-robust-cifar10\", log_model=True)\n# checkpoint_callback = pl_callbacks.ModelCheckpoint(\n#     monitor=\"loss\",\n#     mode=\"min\",\n#     save_top_k=1,\n#     save_last=True,\n#     dirpath=f\"./checkpoints/{wandb_logger.experiment.dir}\",\n# )\n\ntrainer = Trainer(\n    accelerator=\"gpu\",\n    devices=[1],  # Use 1 GPU set to -1 for all GPUs\n    num_nodes=1,  # Number of nodes\n    # strategy=\"ddp_spawn\",         # Distributed strategy\n    precision=\"bf16-mixed\",  # Mixed precision training\n    max_epochs=current_setting[\"epochs\"],\n    enable_model_summary=False,\n    # logger=[wandb_logger],  # Uncomment to enable Wandb logging\n    logger=False,\n    enable_progress_bar=False,\n    callbacks=[\n        # You can add callbacks here, e.g.:\n        # pl_callbacks.LearningRateFinder(max_lr=0.05),\n        # checkpoint_callback,\n    ],\n)\n\nprint(summary(classification_module, input_size=(1, 3, 32, 32)))\n# Start training\ntrainer.fit(classification_module, data_module)\n\nOptionally, you can save the trained model afterwards:\ntorch.save(classification_module.model.state_dict(), \"single_stage.pth\")\n</code></pre> <p><pre><code>    Using bfloat16 Automatic Mixed Precision (AMP)\n    GPU available: True (cuda), used: True\n    TPU available: False, using: 0 TPU cores\n    HPU available: False, using: 0 HPUs\n    /mnt/deel/data/thibaut.boissin/envs/ortho/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /home/thibaut.boissin/projects/orthogonium/scripts/pareto/checkpoints exists and is not empty.\n    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n\n\n    ==================================================================================================================================\n    Layer (type:depth-idx)                                                           Output Shape              Param #\n    ==================================================================================================================================\n    ClassificationLightningModule                                                    [1, 10]                   --\n    \u251c\u2500Sequential: 1-1                                                                [1, 10]                   --\n    \u2502    \u2514\u2500ParametrizedRKOConv2d: 2-1                                                [1, 256, 16, 16]          --\n    \u2502    \u2502    \u2514\u2500ModuleDict: 3-1                                                      --                        3,340\n    \u2502    \u2514\u2500AdditiveResidual: 2-2                                                     [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-2                                                      [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-3                                                     [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-3                                                      [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-4                                                     [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-4                                                      [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-5                                                     [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-5                                                      [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-6                                                     [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-6                                                      [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-7                                                     [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-7                                                      [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-8                                                     [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-8                                                      [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-9                                                     [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-9                                                      [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-10                                                    [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-10                                                     [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-11                                                    [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-11                                                     [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-12                                                    [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-12                                                     [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500AdditiveResidual: 2-13                                                    [1, 256, 16, 16]          1\n    \u2502    \u2502    \u2514\u2500Sequential: 3-13                                                     [1, 256, 16, 16]          142,592\n    \u2502    \u2514\u2500ParametrizedRKOConv2d: 2-14                                               [1, 256, 1, 1]            --\n    \u2502    \u2502    \u2514\u2500ModuleDict: 3-14                                                     --                        196,864\n    \u2502    \u2514\u2500Flatten: 2-15                                                             [1, 256]                  --\n    \u2502    \u2514\u2500MaxMin: 2-16                                                              [1, 256]                  --\n    \u2502    \u2514\u2500ParametrizedOrthoLinear: 2-17                                             [1, 10]                   --\n    \u2502    \u2502    \u2514\u2500ModuleDict: 3-15                                                     --                        2,826\n    ==================================================================================================================================\n    Total params: 1,914,146\n    Trainable params: 1,783,308\n    Non-trainable params: 130,838\n    Total mult-adds (Units.MEGABYTES): 0\n    ==================================================================================================================================\n    Input size (MB): 0.01\n    Forward/backward pass size (MB): 0.00\n    Params size (MB): 0.00\n    Estimated Total Size (MB): 0.01\n    ==================================================================================================================================\n    Files already downloaded and verified\n    Files already downloaded and verified\n\n\n    `Trainer.fit` stopped: `max_epochs=60` reached.\n</code></pre> trainer.validate(classification_module, datamodule=data_module) <pre><code>    LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n\n\n    Files already downloaded and verified\n\n\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503      Validate metric      \u2503       DataLoader 0        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502       val_accuracy        \u2502    0.8831999897956848     \u2502\n\u2502         val_loss          \u2502    -0.8909505605697632    \u2502\n\u2502          val_vra          \u2502            0.0            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\n\n\n\n\n    [{'val_loss': -0.8909505605697632,\n      'val_accuracy': 0.8831999897956848,\n      'val_vra': 0.0}]\n</code></pre></p>"},{"location":"notebooks/demo_cifar_classification/#next-steps","title":"Next Steps","text":"<ul> <li>Model Evaluation: You can add a new cell to perform model evaluation or predictions.</li> <li>Logging and Checkpoints: To enable model logging or checkpoint saving, uncomment the corresponding lines and configure as needed.</li> <li>Experiment with Settings: Change the <code>train_setting</code> variable to <code>\"mildly_robust\"</code> or <code>\"robust\"</code> to experiment with other training configurations.</li> </ul>"}]}