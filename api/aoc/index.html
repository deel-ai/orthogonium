
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../assets/banner.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.5.49">
    
    
      
        <title>Aoc - orthogonium</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6f8fc17f.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../css/custom.css">
    
      <link rel="stylesheet" href="../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="dark" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#orthogonium.layers.conv.AOC.ortho_conv" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="orthogonium" class="md-header__button md-logo" aria-label="orthogonium" data-md-component="logo">
      
  <img src="../../assets/banner.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            orthogonium
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Aoc
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="dark" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/thib-s/orthogonium" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    thib-s/orthogonium
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="orthogonium" class="md-nav__button md-logo" aria-label="orthogonium" data-md-component="logo">
      
  <img src="../../assets/banner.png" alt="logo">

    </a>
    orthogonium
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/thib-s/orthogonium" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    thib-s/orthogonium
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    API Reference
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../conv/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    convolutions
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../linear/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linear layers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reparametrizers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reparametrizers
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../activations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    activations
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../losses/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    losses
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CONTRIBUTING/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv" class="md-nav__link">
    <span class="md-ellipsis">
      ortho_conv
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveOrthoConv2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveOrthoConv2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveOrthoConvTranspose2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveOrthoConvTranspose2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv" class="md-nav__link">
    <span class="md-ellipsis">
      fast_block_ortho_conv
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer" class="md-nav__link">
    <span class="md-ellipsis">
      BCOPTrivializer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="BCOPTrivializer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d" class="md-nav__link">
    <span class="md-ellipsis">
      FastBlockConv2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FastBlockConv2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d.singular_values" class="md-nav__link">
    <span class="md-ellipsis">
      singular_values
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D" class="md-nav__link">
    <span class="md-ellipsis">
      FastBlockConvTranspose2D
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FastBlockConvTranspose2D">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.attach_bcop_weight" class="md-nav__link">
    <span class="md-ellipsis">
      attach_bcop_weight
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.block_orth" class="md-nav__link">
    <span class="md-ellipsis">
      block_orth
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.conv_singular_values_numpy" class="md-nav__link">
    <span class="md-ellipsis">
      conv_singular_values_numpy
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_batched_matrix_conv" class="md-nav__link">
    <span class="md-ellipsis">
      fast_batched_matrix_conv
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_matrix_conv" class="md-nav__link">
    <span class="md-ellipsis">
      fast_matrix_conv
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.transpose_kernel" class="md-nav__link">
    <span class="md-ellipsis">
      transpose_kernel
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.rko_conv" class="md-nav__link">
    <span class="md-ellipsis">
      rko_conv
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.rko_conv.RkoConvTranspose2d" class="md-nav__link">
    <span class="md-ellipsis">
      RkoConvTranspose2d
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Aoc</h1>

<p>The most scalable method to build orthogonal convolution. Allows control of kernel size, 
stride, groups dilation and transposed convolutions.</p>
<p>The classes <code>AdaptiveOrthoConv2d</code> and <code>AdaptiveOrthoConv2d</code> are not classes,
 but factory function to selecte bewteen 3 different parametrizations, as depicted
in the following figure:</p>
<p><img src="/assets/flowchart_v4.png" alt="drawing" style="width:300px;"/></p>


<div class="doc doc-object doc-module">



<a id="orthogonium.layers.conv.AOC.ortho_conv"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">AdaptiveOrthoConv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;circular&#39;</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel size and stride.</p>
<h4 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features">Key Features:<a class="headerlink" href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- Enforces orthogonality, preserving gradient norms.
- Supports native striding, dilation, grouped convolutions, and flexible padding.
</code></pre></div>
<h4 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior">Behavior:<a class="headerlink" href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- When kernel_size == stride, the layer is an `RKOConv2d`.
- When stride == 1, the layer is a `FastBlockConv2d`.
- Otherwise, the layer is a `BcopRkoConv2d`.
</code></pre></div>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of input channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of output channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the convolution kernel.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stride of the convolution. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td>
                  <code>str or <span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode or size. Default is "same".</p>
              </div>
            </td>
            <td>
                  <code>&#39;same&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dilation</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dilation rate. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of blocked connections from input to output channels. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include a learnable bias. Default is True.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mode</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode. Default is "circular".</p>
              </div>
            </td>
            <td>
                  <code>&#39;circular&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ortho_params</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p>
              </div>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a>()</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.Conv2d">Conv2d</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>`ValueError`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">AdaptiveOrthoConv2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;same&quot;</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;circular&quot;</span><span class="p">,</span>
    <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel size and stride.</span>

<span class="sd">    Key Features:</span>
<span class="sd">    -------------</span>
<span class="sd">        - Enforces orthogonality, preserving gradient norms.</span>
<span class="sd">        - Supports native striding, dilation, grouped convolutions, and flexible padding.</span>

<span class="sd">    Behavior:</span>
<span class="sd">    -------------</span>
<span class="sd">        - When kernel_size == stride, the layer is an `RKOConv2d`.</span>
<span class="sd">        - When stride == 1, the layer is a `FastBlockConv2d`.</span>
<span class="sd">        - Otherwise, the layer is a `BcopRkoConv2d`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        in_channels (int): Number of input channels.</span>
<span class="sd">        out_channels (int): Number of output channels.</span>
<span class="sd">        kernel_size (_size_2_t): Size of the convolution kernel.</span>
<span class="sd">        stride (_size_2_t, optional): Stride of the convolution. Default is 1.</span>
<span class="sd">        padding (str or _size_2_t, optional): Padding mode or size. Default is &quot;same&quot;.</span>
<span class="sd">        dilation (_size_2_t, optional): Dilation rate. Default is 1.</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.</span>
<span class="sd">        bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">        padding_mode (str, optional): Padding mode. Default is &quot;circular&quot;.</span>
<span class="sd">        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).</span>

<span class="sd">    Raises:</span>
<span class="sd">        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="n">stride</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">RKOConv2d</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">in_channels</span> <span class="o">&gt;=</span> <span class="n">out_channels</span><span class="p">):</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">FastBlockConv2d</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">BcopRkoConv2d</span>
    <span class="k">return</span> <span class="n">convclass</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">AdaptiveOrthoConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Factory function to create an orthogonal convolutional transpose layer, adapting based on kernel size and stride.</p>
<h4 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features">Key Features:<a class="headerlink" href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- Ensures orthogonality in transpose convolutions for stable gradient propagation.
- Supports dilation, grouped operations, and efficient kernel construction.
</code></pre></div>
<h4 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior">Behavior:<a class="headerlink" href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- When kernel_size == stride, the layer is an `RkoConvTranspose2d`.
- When stride == 1, the layer is a `FastBlockConvTranspose2D`.
- Otherwise, the layer is a `BcopRkoConvTranspose2d`.
</code></pre></div>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of input channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of output channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the convolution kernel.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stride of the transpose convolution. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding size. Default is 0.</p>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_padding</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional size for output. Default is 0.</p>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of groups. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include a learnable bias. Default is True.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dilation</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dilation rate. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mode</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode. Default is "zeros".</p>
              </div>
            </td>
            <td>
                  <code>&#39;zeros&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ortho_params</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p>
              </div>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a>()</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.ConvTranspose2d">ConvTranspose2d</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A configured instance of <code>nn.ConvTranspose2d</code> (one of <code>RkoConvTranspose2d</code>, <code>FastBlockConvTranspose2D</code>, or <code>BcopRkoConvTranspose2d</code>).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p><strong>Raises:</strong>
- <code>ValueError</code>: If kernel_size &lt; stride, as orthogonality cannot be enforced.</p>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">AdaptiveOrthoConvTranspose2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
    <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Factory function to create an orthogonal convolutional transpose layer, adapting based on kernel size and stride.</span>

<span class="sd">    Key Features:</span>
<span class="sd">    -------------</span>
<span class="sd">        - Ensures orthogonality in transpose convolutions for stable gradient propagation.</span>
<span class="sd">        - Supports dilation, grouped operations, and efficient kernel construction.</span>

<span class="sd">    Behavior:</span>
<span class="sd">    ---------</span>
<span class="sd">        - When kernel_size == stride, the layer is an `RkoConvTranspose2d`.</span>
<span class="sd">        - When stride == 1, the layer is a `FastBlockConvTranspose2D`.</span>
<span class="sd">        - Otherwise, the layer is a `BcopRkoConvTranspose2d`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        in_channels (int): Number of input channels.</span>
<span class="sd">        out_channels (int): Number of output channels.</span>
<span class="sd">        kernel_size (_size_2_t): Size of the convolution kernel.</span>
<span class="sd">        stride (_size_2_t, optional): Stride of the transpose convolution. Default is 1.</span>
<span class="sd">        padding (_size_2_t, optional): Padding size. Default is 0.</span>
<span class="sd">        output_padding (_size_2_t, optional): Additional size for output. Default is 0.</span>
<span class="sd">        groups (int, optional): Number of groups. Default is 1.</span>
<span class="sd">        bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">        dilation (_size_2_t, optional): Dilation rate. Default is 1.</span>
<span class="sd">        padding_mode (str, optional): Padding mode. Default is &quot;zeros&quot;.</span>
<span class="sd">        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A configured instance of `nn.ConvTranspose2d` (one of `RkoConvTranspose2d`, `FastBlockConvTranspose2D`, or `BcopRkoConvTranspose2d`).</span>

<span class="sd">    **Raises:**</span>
<span class="sd">    - `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="n">stride</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">RkoConvTranspose2d</span>
    <span class="k">elif</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">FastBlockConvTranspose2D</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">BcopRkoConvTranspose2d</span>
    <span class="k">return</span> <span class="n">convclass</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="o">=</span><span class="n">output_padding</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="orthogonium.layers.conv.AOC.fast_block_ortho_conv"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer" class="doc doc-heading">
            <code>BCOPTrivializer</code>


<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">BCOPTrivializer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;This module is used to generate orthogonal kernels for the BCOP layer. It takes</span>
<span class="sd">        as input a matrix PQ of shape (groups, 2*kernel_size, c, c//2) and returns a kernel</span>
<span class="sd">        of shape (c, c, kernel_size, kernel_size) that is orthogonal.</span>

<span class="sd">        Args:</span>
<span class="sd">            in_channels (int): number of input channels</span>
<span class="sd">            out_channels (int): number of output channels</span>
<span class="sd">            kernel_size (int): size of the kernel</span>
<span class="sd">            groups (int): number of groups</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">BCOPTrivializer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_channels</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_channels</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span> <span class="o">=</span> <span class="n">out_channels</span> <span class="o">&lt;</span> <span class="n">in_channels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">kernel_size</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">PQ</span><span class="p">):</span>
        <span class="n">ident</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">PQ</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="c1"># PQ contains 2*(kernel_size - 1) + 2 matrices of shape (c, c//2)</span>
        <span class="c1"># the 2 first matrices will be composed to build a (c, c) matrix</span>
        <span class="c1"># this (c, c) matrix will be used to build the first 1x1 conv</span>
        <span class="c1"># the remaining matrices will be used to build the 2x2 convs as</span>
        <span class="c1"># described in BCOP paper</span>
        <span class="c1">####</span>
        <span class="c1"># first we compute PQ@PQ.t (used by both 1x1 and 2x2 convs)</span>
        <span class="c1"># we can rewrite PQ@PQ.t as an einsum</span>
        <span class="n">PQ</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;gijl,gikl-&gt;gijk&quot;</span><span class="p">,</span> <span class="n">PQ</span><span class="p">,</span> <span class="n">PQ</span><span class="p">)</span>
        <span class="c1"># PQ = PQ @ PQ.transpose(-1, -2)</span>
        <span class="c1"># we build the 1x1 conv using the two first matrices</span>
        <span class="c1"># we construct the (c, c) matrix by computing (I - 2*PQ[0]) @ (I - 2*PQ[1])</span>
        <span class="c1"># this is an extension of Householder reflection to the matrix case where</span>
        <span class="c1"># instead of reflecting a vector and compose c matrices, we reflect 2</span>
        <span class="c1"># (c, c//2) matrices. This results in an orthogonal matrix, but the fact that</span>
        <span class="c1"># any orthogonal matrix can be decomposed this way is yet to be proven.</span>
        <span class="n">c11</span> <span class="o">=</span> <span class="n">ident</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">PQ</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="n">c11</span> <span class="o">=</span> <span class="n">c11</span> <span class="o">@</span> <span class="p">(</span><span class="n">ident</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">PQ</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
        <span class="c1"># reshape the matrix to build a 1x1 conv</span>
        <span class="n">c11</span> <span class="o">=</span> <span class="n">c11</span><span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_channels</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">,</span>
            <span class="mi">1</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># if the number of channels is different, we need to remove the extra channels</span>
        <span class="c1"># this results in a row/column othogonal matrix. It is still more efficient than</span>
        <span class="c1"># doing a separate orthogonalization (as shapes differs).</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span><span class="p">:</span>
            <span class="n">c11</span> <span class="o">=</span> <span class="n">c11</span><span class="p">[:,</span> <span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>

        <span class="c1"># build all 2x2 convs in parallel</span>
        <span class="c1"># half of the matrices will be used to create a 2x1 conv while the other half</span>
        <span class="c1"># will be used to create a 1x2 conv. The 2x1 and 1x2 convs will be composed</span>
        <span class="c1"># to build a 2x2 conv. c12 and c21 are notation abuse, since the tensors represent</span>
        <span class="c1"># 1x1 convs (it is the vertical/horizontal stacking of c12/c21 with (I-c12) and (I-c21)</span>
        <span class="c1"># that will result in a 1x2/2x1 conv)</span>
        <span class="n">c12</span> <span class="o">=</span> <span class="n">PQ</span><span class="p">[:,</span> <span class="mi">2</span> <span class="p">:</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="n">c21</span> <span class="o">=</span> <span class="n">PQ</span><span class="p">[:,</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="p">:,</span> <span class="p">:,</span> <span class="p">:]</span>
        <span class="n">c22</span> <span class="o">=</span> <span class="n">block_orth</span><span class="p">(</span>
            <span class="n">c12</span><span class="p">,</span> <span class="n">c21</span>
        <span class="p">)</span>  <span class="c1"># this is an efficient and parallel way to compute the 2x2 convs</span>
        <span class="c1"># i used to belive that transposing half of the matrices would alleviate the expressiveness</span>
        <span class="c1"># issue, but it is not notable.</span>
        <span class="c1"># c22[1::2] = -c22[1::2].flip(-1, -2)</span>

        <span class="c1"># we now need to compose the 2x2 convs to build the k*k kernel</span>
        <span class="c1"># by using the associativity of the block conv operator we can</span>
        <span class="c1"># run the steps of the BCOP algorithm in parallel: we groups the</span>
        <span class="c1"># 2x2 convs in pairs and apply the block conv operator to each pair</span>
        <span class="c1"># until we have a single conv. If k-1 is a power of two this algorithm</span>
        <span class="c1"># run in log(k-1) steps. (naive associative scan algorithm)</span>
        <span class="k">while</span> <span class="n">c22</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">mid</span> <span class="o">=</span> <span class="n">c22</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="mi">2</span>
            <span class="n">c22</span> <span class="o">=</span> <span class="n">fast_batched_matrix_conv</span><span class="p">(</span><span class="n">c22</span><span class="p">[:</span><span class="n">mid</span><span class="p">],</span> <span class="n">c22</span><span class="p">[</span><span class="n">mid</span><span class="p">:],</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="c1"># we finally compose the 1x1 conv with the kxk conv</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">c11</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">c22</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>  <span class="c1"># c22.shape[0] == 1 if k-1 is a power of two</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">fast_matrix_conv</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">c22</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="c1"># since it is less expensive to compute the transposed kernel when co &lt; ci</span>
        <span class="c1"># we transpose the kernel if needed</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">transpose_kernel</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="n">flip</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># it seems more efficient to make the kernel contiguous since it will be used</span>
        <span class="c1"># in a convolution</span>
        <span class="k">return</span> <span class="n">res</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">groups</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.BCOPTrivializer.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>This module is used to generate orthogonal kernels for the BCOP layer. It takes
as input a matrix PQ of shape (groups, 2*kernel_size, c, c//2) and returns a kernel
of shape (c, c, kernel_size, kernel_size) that is orthogonal.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of input channels</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of output channels</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>size of the kernel</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of groups</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This module is used to generate orthogonal kernels for the BCOP layer. It takes</span>
<span class="sd">    as input a matrix PQ of shape (groups, 2*kernel_size, c, c//2) and returns a kernel</span>
<span class="sd">    of shape (c, c, kernel_size, kernel_size) that is orthogonal.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): number of input channels</span>
<span class="sd">        out_channels (int): number of output channels</span>
<span class="sd">        kernel_size (int): size of the kernel</span>
<span class="sd">        groups (int): number of groups</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BCOPTrivializer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">=</span> <span class="n">out_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">=</span> <span class="n">in_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_channels</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_channels</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transpose</span> <span class="o">=</span> <span class="n">out_channels</span> <span class="o">&lt;</span> <span class="n">in_channels</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_kernels</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">kernel_size</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d" class="doc doc-heading">
            <code>FastBlockConv2d</code>


<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Conv2d">Conv2d</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FastBlockConv2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;circular&quot;</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Fast implementation of the Block Circulant Orthogonal Parametrization (BCOP) for convolutional layers.</span>
<span class="sd">        This approach changes the original BCOP algorithm to make it more scalable and efficient. This implementation</span>
<span class="sd">        rewrite efficiently the block convolution operator as a single convolution operation. Also the iterative algorithm</span>
<span class="sd">        is parallelized in the associative scan fashion.</span>

<span class="sd">        This layer is a drop-in replacement for the nn.Conv2d layer. It is orthogonal and Lipschitz continuous while maintaining</span>
<span class="sd">        the same interface as the Con2d. Also this method has an explicit kernel, whihc allows to compute the singular values of</span>
<span class="sd">        the convolutional layer.</span>

<span class="sd">        Striding is not supported when out_channels &gt; in_channels. Real striding is supported in BcopRkoConv2d. The use of</span>
<span class="sd">        OrthogonalConv2d is recommended.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FastBlockConv2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># raise runtime error if kernel size &gt;= stride</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">out_channels</span> <span class="o">&gt;</span> <span class="n">in_channels</span><span class="p">)</span>
        <span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;stride &gt; 1 is not supported when out_channels &gt; in_channels, &quot;</span>
                <span class="s2">&quot;use TODO layer instead&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
            <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.&quot;</span>
            <span class="p">)</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">attach_bcop_weight</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="n">out_channels</span><span class="p">,</span>
                <span class="n">in_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="p">),</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">singular_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Compute the singular values of the convolutional layer using the FFT+SVD method.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple[float, float, float]: min singular value, max singular value and</span>
<span class="sd">            normalized stable rank (1 means orthogonal matrix)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># use the fft+svd method to compute the singular values</span>
        <span class="c1"># assuming circular padding, if &quot;zero&quot; padding is used the value</span>
        <span class="c1"># will be overestimated (ie. the singular values will be larger than</span>
        <span class="c1"># the real ones)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s2">&quot;circular&quot;</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;padding </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="si">}</span><span class="s2"> not supported, return min and max&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;singular values as if it was &#39;circular&#39; padding &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(overestimate the values).&quot;</span>
            <span class="p">)</span>
        <span class="n">sv_min</span><span class="p">,</span> <span class="n">sv_max</span><span class="p">,</span> <span class="n">stable_rank</span> <span class="o">=</span> <span class="n">conv_singular_values_numpy</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">sv_min</span><span class="p">,</span> <span class="n">sv_max</span><span class="p">,</span> <span class="n">stable_rank</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">FastBlockConv2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;circular&#39;</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Fast implementation of the Block Circulant Orthogonal Parametrization (BCOP) for convolutional layers.
This approach changes the original BCOP algorithm to make it more scalable and efficient. This implementation
rewrite efficiently the block convolution operator as a single convolution operation. Also the iterative algorithm
is parallelized in the associative scan fashion.</p>
<p>This layer is a drop-in replacement for the nn.Conv2d layer. It is orthogonal and Lipschitz continuous while maintaining
the same interface as the Con2d. Also this method has an explicit kernel, whihc allows to compute the singular values of
the convolutional layer.</p>
<p>Striding is not supported when out_channels &gt; in_channels. Real striding is supported in BcopRkoConv2d. The use of
OrthogonalConv2d is recommended.</p>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;same&quot;</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;circular&quot;</span><span class="p">,</span>
    <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Fast implementation of the Block Circulant Orthogonal Parametrization (BCOP) for convolutional layers.</span>
<span class="sd">    This approach changes the original BCOP algorithm to make it more scalable and efficient. This implementation</span>
<span class="sd">    rewrite efficiently the block convolution operator as a single convolution operation. Also the iterative algorithm</span>
<span class="sd">    is parallelized in the associative scan fashion.</span>

<span class="sd">    This layer is a drop-in replacement for the nn.Conv2d layer. It is orthogonal and Lipschitz continuous while maintaining</span>
<span class="sd">    the same interface as the Con2d. Also this method has an explicit kernel, whihc allows to compute the singular values of</span>
<span class="sd">    the convolutional layer.</span>

<span class="sd">    Striding is not supported when out_channels &gt; in_channels. Real striding is supported in BcopRkoConv2d. The use of</span>
<span class="sd">    OrthogonalConv2d is recommended.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FastBlockConv2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># raise runtime error if kernel size &gt;= stride</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">out_channels</span> <span class="o">&gt;</span> <span class="n">in_channels</span><span class="p">)</span>
    <span class="p">)</span> <span class="ow">or</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;stride &gt; 1 is not supported when out_channels &gt; in_channels, &quot;</span>
            <span class="s2">&quot;use TODO layer instead&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">)</span>
        <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
        <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">attach_bcop_weight</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
        <span class="p">(</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">in_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">),</span>
        <span class="n">groups</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d.singular_values" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">singular_values</span><span class="p">()</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConv2d.singular_values" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Compute the singular values of the convolutional layer using the FFT+SVD method.</p>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Tuple[float, float, float]: min singular value, max singular value and</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>normalized stable rank (1 means orthogonal matrix)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">singular_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the singular values of the convolutional layer using the FFT+SVD method.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[float, float, float]: min singular value, max singular value and</span>
<span class="sd">        normalized stable rank (1 means orthogonal matrix)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># use the fft+svd method to compute the singular values</span>
    <span class="c1"># assuming circular padding, if &quot;zero&quot; padding is used the value</span>
    <span class="c1"># will be overestimated (ie. the singular values will be larger than</span>
    <span class="c1"># the real ones)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s2">&quot;circular&quot;</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;padding </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="si">}</span><span class="s2"> not supported, return min and max&quot;</span>
            <span class="sa">f</span><span class="s2">&quot;singular values as if it was &#39;circular&#39; padding &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;(overestimate the values).&quot;</span>
        <span class="p">)</span>
    <span class="n">sv_min</span><span class="p">,</span> <span class="n">sv_max</span><span class="p">,</span> <span class="n">stable_rank</span> <span class="o">=</span> <span class="n">conv_singular_values_numpy</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
        <span class="o">.</span><span class="n">view</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">sv_min</span><span class="p">,</span> <span class="n">sv_max</span><span class="p">,</span> <span class="n">stable_rank</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D" class="doc doc-heading">
            <code>FastBlockConvTranspose2D</code>


<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.ConvTranspose2d">ConvTranspose2d</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">FastBlockConvTranspose2D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extention of the BCOP algorithm to transposed convolutions. This implementation</span>
<span class="sd">        uses the same algorithm as the FlashBCOP layer, but the layer acts as a transposed</span>
<span class="sd">        convolutional layer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">FastBlockConvTranspose2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span> <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s2">&quot;zeros&quot;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
        <span class="k">if</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_same_padding</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_standardize_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
            <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">((</span><span class="nb">max</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span> <span class="o">//</span> <span class="n">groups</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;inner conv must have at least 2 channels&quot;</span><span class="p">)</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">attach_bcop_weight</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
            <span class="p">(</span>
                <span class="n">in_channels</span><span class="p">,</span>
                <span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="p">),</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_calculate_same_padding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate padding for &#39;same&#39; mode.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="nb">int</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="o">/</span> <span class="mi">2</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="nb">int</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="o">/</span> <span class="mi">2</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="nb">int</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                    <span class="o">/</span> <span class="mi">2</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="nb">int</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                    <span class="o">/</span> <span class="mi">2</span>
                <span class="p">)</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_standardize_padding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Ensure padding is always a tuple.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">padding</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;padding must be int or tuple, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">singular_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">!=</span> <span class="s2">&quot;circular&quot;</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;padding </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="si">}</span><span class="s2"> not supported, return min and max&quot;</span>
                <span class="sa">f</span><span class="s2">&quot;singular values as if it was &#39;circular&#39; padding &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;(overestimate the values).&quot;</span>
            <span class="p">)</span>
        <span class="n">sv_min</span><span class="p">,</span> <span class="n">sv_max</span><span class="p">,</span> <span class="n">stable_rank</span> <span class="o">=</span> <span class="n">conv_singular_values_numpy</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">sv_min</span><span class="p">,</span> <span class="n">sv_max</span><span class="p">,</span> <span class="n">stable_rank</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_padding_mode</span> <span class="o">!=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_padding_mode</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="p">(</span>
                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="o">+</span> <span class="mi">1</span>
                    <span class="p">),</span>
                    <span class="p">(</span>
                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="o">+</span> <span class="mi">1</span>
                    <span class="p">),</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">y</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">FastBlockConvTranspose2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.FastBlockConvTranspose2D.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Extention of the BCOP algorithm to transposed convolutions. This implementation
uses the same algorithm as the FlashBCOP layer, but the layer acts as a transposed
convolutional layer.</p>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
    <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extention of the BCOP algorithm to transposed convolutions. This implementation</span>
<span class="sd">    uses the same algorithm as the FlashBCOP layer, but the layer acts as a transposed</span>
<span class="sd">    convolutional layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FastBlockConvTranspose2D</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span> <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s2">&quot;zeros&quot;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">,</span>
        <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">real_padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
    <span class="k">if</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_same_padding</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">real_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_standardize_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">)</span>
        <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
        <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="p">(</span>
        <span class="p">((</span><span class="nb">max</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span> <span class="o">//</span> <span class="n">groups</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">)</span>
        <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;inner conv must have at least 2 channels&quot;</span><span class="p">)</span>
    <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
    <span class="n">attach_bcop_weight</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
        <span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="p">),</span>
        <span class="n">groups</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>


<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.attach_bcop_weight" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">attach_bcop_weight</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">weight_name</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.attach_bcop_weight" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Attach a weight to a layer and parametrize it with the BCOPTrivializer module.
The attached weight will be the kernel of an orthogonal convolutional layer.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>layer</code>
            </td>
            <td>
                  <code><span title="torch.nn.Module">Module</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>layer to which the weight will be attached</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>weight_name</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>name of the weight</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_shape</code>
            </td>
            <td>
                  <code>tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>shape of the kernel (out_channels, in_channels/groups, kernel_size, kernel_size)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of groups</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bjorck_params</code>
            </td>
            <td>
                  <code>BjorckParams</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>parameters of the Bjorck orthogonalization. Defaults to BjorckParams().</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>torch.Tensor: a handle to the attached weight</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">attach_bcop_weight</span><span class="p">(</span>
    <span class="n">layer</span><span class="p">,</span> <span class="n">weight_name</span><span class="p">,</span> <span class="n">kernel_shape</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">()</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Attach a weight to a layer and parametrize it with the BCOPTrivializer module.</span>
<span class="sd">    The attached weight will be the kernel of an orthogonal convolutional layer.</span>

<span class="sd">    Args:</span>
<span class="sd">        layer (torch.nn.Module): layer to which the weight will be attached</span>
<span class="sd">        weight_name (str): name of the weight</span>
<span class="sd">        kernel_shape (tuple): shape of the kernel (out_channels, in_channels/groups, kernel_size, kernel_size)</span>
<span class="sd">        groups (int): number of groups</span>
<span class="sd">        bjorck_params (BjorckParams, optional): parameters of the Bjorck orthogonalization. Defaults to BjorckParams().</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: a handle to the attached weight</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">k2</span> <span class="o">=</span> <span class="n">kernel_shape</span>
    <span class="n">in_channels</span> <span class="o">*=</span> <span class="n">groups</span>  <span class="c1"># compute the real number of input channels</span>
    <span class="k">assert</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="n">k2</span><span class="p">,</span> <span class="s2">&quot;only square kernels are supported for the moment&quot;</span>
    <span class="n">max_channels</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">)</span>
    <span class="n">num_kernels</span> <span class="o">=</span> <span class="p">(</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="n">kernel_size</span>
    <span class="p">)</span>  <span class="c1"># the number of projectors needed to create the kernel</span>
    <span class="c1"># register projectors matrices</span>
    <span class="n">layer</span><span class="o">.</span><span class="n">register_parameter</span><span class="p">(</span>
        <span class="n">weight_name</span><span class="p">,</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
                <span class="n">groups</span><span class="p">,</span>
                <span class="n">num_kernels</span><span class="p">,</span>
                <span class="p">(</span><span class="n">max_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">),</span>
                <span class="p">(</span><span class="n">max_channels</span> <span class="o">//</span> <span class="p">(</span><span class="n">groups</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)),</span>
            <span class="p">),</span>
            <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">layer</span><span class="p">,</span> <span class="n">weight_name</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">orthogonal_</span><span class="p">(</span><span class="n">weight</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># if max_channels//groups == 1, we can use L2 normalization</span>
        <span class="c1"># instead of Bjorck orthogonalization which is significantly faster</span>
        <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
            <span class="n">layer</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="p">,</span>
            <span class="n">L2Normalize</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)),</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># register power iteration and Bjorck orthogonalization</span>
        <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
            <span class="n">layer</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="p">,</span>
            <span class="n">ortho_params</span><span class="o">.</span><span class="n">spectral_normalizer</span><span class="p">(</span><span class="n">weight_shape</span><span class="o">=</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
            <span class="n">layer</span><span class="p">,</span>
            <span class="n">weight_name</span><span class="p">,</span>
            <span class="n">ortho_params</span><span class="o">.</span><span class="n">orthogonalizer</span><span class="p">(</span>
                <span class="n">weight_shape</span><span class="o">=</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
    <span class="c1"># now we have orthogonal projectors, we can build the orthogonal kernel</span>
    <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
        <span class="n">layer</span><span class="p">,</span>
        <span class="n">weight_name</span><span class="p">,</span>
        <span class="n">BCOPTrivializer</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">unsafe</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">weight</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.block_orth" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">block_orth</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.block_orth" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Construct a 2x2 orthogonal matrix from two orthogonal orthogonal projectors.
Each projector can be seen as a 1x1 convolution, hence the stacking spatial stacking
of [pi, I-pi] can be seen as a 2x1 or 1x2 orthogonal convolution. By using the block
convolution operator, we can compute the 2x2 orthogonal conv. In this specific case,
we can write the whole operation as a single einsum.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>p1</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>orthogonal projector of shape (g, x, c, c) where g is the number
of groups, x is a batch dimension (allowing to compute the operation in parallel)
and c is the number of channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>p2</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>orthogonal projector of shape (g, x, c, c) also.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>torch.Tensor: orthogonal 2x2 conv of shape (x, g*c, c, 2, 2)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">block_orth</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Construct a 2x2 orthogonal matrix from two orthogonal orthogonal projectors.</span>
<span class="sd">    Each projector can be seen as a 1x1 convolution, hence the stacking spatial stacking</span>
<span class="sd">    of [pi, I-pi] can be seen as a 2x1 or 1x2 orthogonal convolution. By using the block</span>
<span class="sd">    convolution operator, we can compute the 2x2 orthogonal conv. In this specific case,</span>
<span class="sd">    we can write the whole operation as a single einsum.</span>

<span class="sd">    Args:</span>
<span class="sd">        p1 (torch.Tensor): orthogonal projector of shape (g, x, c, c) where g is the number</span>
<span class="sd">            of groups, x is a batch dimension (allowing to compute the operation in parallel)</span>
<span class="sd">            and c is the number of channels.</span>
<span class="sd">        p2 (torch.Tensor): orthogonal projector of shape (g, x, c, c) also.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: orthogonal 2x2 conv of shape (x, g*c, c, 2, 2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="n">p1</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">p2</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">g</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n2</span> <span class="o">=</span> <span class="n">p1</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">eye</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">p1</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">p1</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="c1"># sorry for using x as a batch dimension, but this einsum was hard to write (thank you unit tests!)</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span>
        <span class="s2">&quot;bgxij,cgxjk-&gt;xgikbc&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">p1</span><span class="p">,</span> <span class="n">eye</span> <span class="o">-</span> <span class="n">p1</span><span class="p">]),</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">p2</span><span class="p">,</span> <span class="n">eye</span> <span class="o">-</span> <span class="n">p2</span><span class="p">])</span>
    <span class="p">)</span>
    <span class="c1"># we reshape the result to get a 2x2 conv kernel</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">g</span> <span class="o">*</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.conv_singular_values_numpy" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">conv_singular_values_numpy</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.conv_singular_values_numpy" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers.
In International Conference on Learning Representations, 2019.</p>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">conv_singular_values_numpy</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Hanie Sedghi, Vineet Gupta, and Philip M. Long. The singular values of convolutional layers.</span>
<span class="sd">    In International Conference on Learning Representations, 2019.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>  <span class="c1"># g, k1, k2, ci, co</span>
    <span class="n">transforms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft2</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>  <span class="c1"># g, k1, k2, ci, co</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">svs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span>
            <span class="n">transforms</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>  <span class="c1"># g, k1, k2, min(ci, co)</span>
        <span class="n">stable_rank</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">svs</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">svs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">svs</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">svs</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">stable_rank</span>
    <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;numerical error in svd, returning only largest singular value&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">None</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">transforms</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">),</span> <span class="kc">None</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_batched_matrix_conv" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">fast_batched_matrix_conv</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_batched_matrix_conv" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Compute the convolution of two matrices using the block convolution operator.
This is exactly the same as fast_matrix_conv but with an additional batch dimension.
This is useful when we want to compute the convolution of multiple matrices in parallel.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>m1</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>matrix of shape (b, c2, c1/g, k1, k2)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>m2</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>matrix of shape (b, c3, c2/g, l1, l2)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of groups. Defaults to 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(b, c3, c1, k+l-1, k+l-1)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fast_batched_matrix_conv</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the convolution of two matrices using the block convolution operator.</span>
<span class="sd">    This is exactly the same as fast_matrix_conv but with an additional batch dimension.</span>
<span class="sd">    This is useful when we want to compute the convolution of multiple matrices in parallel.</span>

<span class="sd">    Args:</span>
<span class="sd">        m1 (torch.Tensor): matrix of shape (b, c2, c1/g, k1, k2)</span>
<span class="sd">        m2 (torch.Tensor): matrix of shape (b, c3, c2/g, l1, l2)</span>
<span class="sd">        groups (int, optional): number of groups. Defaults to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape</span>
<span class="sd">        (b, c3, c1, k+l-1, k+l-1)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">k2</span> <span class="o">=</span> <span class="n">m1</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">b2</span><span class="p">,</span> <span class="n">nb</span><span class="p">,</span> <span class="n">mb</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span> <span class="o">=</span> <span class="n">m2</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">m</span> <span class="o">==</span> <span class="n">mb</span> <span class="o">*</span> <span class="n">groups</span>
    <span class="k">assert</span> <span class="n">b</span> <span class="o">==</span> <span class="n">b2</span>
    <span class="n">m1</span> <span class="o">=</span> <span class="n">m1</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">)</span>
    <span class="n">m2</span> <span class="o">=</span> <span class="n">m2</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">nb</span><span class="p">,</span> <span class="n">mb</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span><span class="p">)</span>
    <span class="c1"># Rearrange m1 for conv</span>
    <span class="n">m1</span> <span class="o">=</span> <span class="n">m1</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># n*m*k1*k2</span>
    <span class="c1"># Rearrange m2 for conv</span>
    <span class="n">m2</span> <span class="o">=</span> <span class="n">m2</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span> <span class="o">*</span> <span class="n">b</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">l1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">l2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
    <span class="c1"># Rearrange result</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">r2</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">nb</span><span class="p">,</span> <span class="n">k1</span> <span class="o">+</span> <span class="n">l1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k2</span> <span class="o">+</span> <span class="n">l2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">r2</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">r2</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_matrix_conv" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">fast_matrix_conv</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.fast_matrix_conv" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Compute the convolution of two matrices using the block convolution operator.
The original algorithm can be written as a single convolution operation, which
unlock the massive parallelism of the convolution operator. This implementation
is also more memory efficient than the original algorithm.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>m1</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>matrix of shape (c2, c1/g, k1, k2)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>m2</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>matrix of shape (c3, c2/g, l1, l2)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of groups. Defaults to 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape</p>
              </div>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>(c3, c1, k+l-1, k+l-1)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">fast_matrix_conv</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the convolution of two matrices using the block convolution operator.</span>
<span class="sd">    The original algorithm can be written as a single convolution operation, which</span>
<span class="sd">    unlock the massive parallelism of the convolution operator. This implementation</span>
<span class="sd">    is also more memory efficient than the original algorithm.</span>

<span class="sd">    Args:</span>
<span class="sd">        m1 (torch.Tensor): matrix of shape (c2, c1/g, k1, k2)</span>
<span class="sd">        m2 (torch.Tensor): matrix of shape (c3, c2/g, l1, l2)</span>
<span class="sd">        groups (int, optional): number of groups. Defaults to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: result of the convolution of m1 and m2, this is a kernel of shape</span>
<span class="sd">        (c3, c1, k+l-1, k+l-1)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># m1 is m*n*k1*k2</span>
    <span class="c1"># m2 is nb*m*l1*l2</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">k2</span> <span class="o">=</span> <span class="n">m1</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">nb</span><span class="p">,</span> <span class="n">mb</span><span class="p">,</span> <span class="n">l1</span><span class="p">,</span> <span class="n">l2</span> <span class="o">=</span> <span class="n">m2</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span> <span class="n">m</span> <span class="o">==</span> <span class="n">mb</span> <span class="o">*</span> <span class="n">groups</span>

    <span class="c1"># Rearrange m1 for conv</span>
    <span class="n">m1</span> <span class="o">=</span> <span class="n">m1</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># n*m*k1*k2</span>

    <span class="c1"># Rearrange m2 for conv</span>
    <span class="n">m2</span> <span class="o">=</span> <span class="n">m2</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Run conv, output shape nb*n*(k+l-1)*(k+l-1)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">m1</span><span class="p">,</span> <span class="n">m2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">l1</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">l2</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>

    <span class="c1"># Rearrange result</span>
    <span class="k">return</span> <span class="n">r2</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># n*nb*(k+l-1)*(k+l-1)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.fast_block_ortho_conv.transpose_kernel" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">transpose_kernel</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">flip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.AOC.fast_block_ortho_conv.transpose_kernel" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Compute the transpose of a kernel. This is done by transposing the kernel and
flipping it along the last two dimensions. This operation is equivalent to the
transpose of the convolution operator (when the stride is 1)</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>p</code>
            </td>
            <td>
                  <code><span title="torch.Tensor">Tensor</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>kernel of shape (cig, cog, k1, k2)</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>number of groups</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>flip</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>if True, the kernel will be flipped. Defaults to True.
False can be used when the is no need to flip the kernel.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
            </td>
            <td>
              <div class="doc-md-description">
                <p>torch.Tensor: transposed kernel of shape (cog, cig, k1, k2)</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\fast_block_ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">transpose_kernel</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">flip</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute the transpose of a kernel. This is done by transposing the kernel and</span>
<span class="sd">    flipping it along the last two dimensions. This operation is equivalent to the</span>
<span class="sd">    transpose of the convolution operator (when the stride is 1)</span>

<span class="sd">    Args:</span>
<span class="sd">        p (torch.Tensor): kernel of shape (cig, cog, k1, k2)</span>
<span class="sd">        groups (int): number of groups</span>
<span class="sd">        flip (bool, optional): if True, the kernel will be flipped. Defaults to True.</span>
<span class="sd">            False can be used when the is no need to flip the kernel.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: transposed kernel of shape (cog, cig, k1, k2)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cig</span><span class="p">,</span> <span class="n">cog</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">k2</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">cig</span> <span class="o">=</span> <span class="n">cig</span> <span class="o">//</span> <span class="n">groups</span>
    <span class="c1"># we do not perform flip since it does not affect orthogonality</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">groups</span><span class="p">,</span> <span class="n">cig</span><span class="p">,</span> <span class="n">cog</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">flip</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">flip</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="c1"># merge groups to get the final kernel</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">cog</span> <span class="o">*</span> <span class="n">groups</span><span class="p">,</span> <span class="n">cig</span><span class="p">,</span> <span class="n">k1</span><span class="p">,</span> <span class="n">k2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="orthogonium.layers.conv.AOC.rko_conv"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.AOC.rko_conv.RkoConvTranspose2d" class="doc doc-heading">
            <code>RkoConvTranspose2d</code>


<a href="#orthogonium.layers.conv.AOC.rko_conv.RkoConvTranspose2d" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.ConvTranspose2d">ConvTranspose2d</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\AOC\rko_conv.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">RkoConvTranspose2d</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RkoConvTranspose2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span> <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s2">&quot;zeros&quot;</span> <span class="k">else</span> <span class="mi">0</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="p">,</span>
            <span class="n">dilation</span><span class="p">,</span>
            <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
        <span class="k">if</span> <span class="n">padding</span> <span class="o">==</span> <span class="s2">&quot;same&quot;</span><span class="p">:</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_same_padding</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">real_padding</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_standardize_padding</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;kernel size must be smaller than stride. The set of orthogonal convolutions is empty in this setting.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
            <span class="ow">and</span> <span class="p">(((</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">))</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;dilation must be 1 when stride is not 1. The set of orthonal convolutions is empty in this setting.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span>
        <span class="n">attach_rko_weight</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
            <span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
            <span class="n">groups</span><span class="p">,</span>
            <span class="n">scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scale</span><span class="p">,</span>
            <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_calculate_same_padding</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Calculate padding for &#39;same&#39; mode.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">(</span>
            <span class="nb">int</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="o">/</span> <span class="mi">2</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="nb">int</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="o">/</span> <span class="mi">2</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="nb">int</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                    <span class="o">/</span> <span class="mi">2</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="nb">int</span><span class="p">(</span>
                <span class="n">np</span><span class="o">.</span><span class="n">floor</span><span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
                    <span class="o">/</span> <span class="mi">2</span>
                <span class="p">)</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">_standardize_padding</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Ensure padding is always a tuple.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">int</span><span class="p">):</span>
            <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="n">padding</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">padding</span> <span class="o">=</span> <span class="p">(</span><span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">padding</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">padding</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;padding must be int or tuple, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">padding</span><span class="p">)</span><span class="si">}</span><span class="s2"> instead&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">singular_values</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="ow">and</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="p">):</span>
            <span class="n">svs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
                    <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span>
                <span class="p">)</span>
                <span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
                <span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
                <span class="n">compute_uv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">sv_min</span> <span class="o">=</span> <span class="n">svs</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
            <span class="n">sv_max</span> <span class="o">=</span> <span class="n">svs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
            <span class="n">stable_rank</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">svs</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">svs</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">sv_min</span><span class="p">,</span> <span class="n">sv_max</span><span class="p">,</span> <span class="n">stable_rank</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                <span class="s2">&quot;Not able to compute singular values for this configuration&quot;</span>
            <span class="p">)</span>
        <span class="c1"># Implements interface required by LipschitzModuleL2</span>
        <span class="n">sv_min</span><span class="p">,</span> <span class="n">sv_max</span><span class="p">,</span> <span class="n">stable_rank</span> <span class="o">=</span> <span class="n">conv_singular_values_numpy</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
            <span class="o">.</span><span class="n">view</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
            <span class="p">)</span>
            <span class="o">.</span><span class="n">numpy</span><span class="p">(),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">sv_min</span><span class="p">,</span> <span class="n">sv_max</span><span class="p">,</span> <span class="n">stable_rank</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_input_shape</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_padding_mode</span> <span class="o">!=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_padding</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">real_padding_mode</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span>
                <span class="n">X</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="p">(</span>
                    <span class="p">(</span>
                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="o">+</span> <span class="mi">1</span>
                    <span class="p">),</span>
                    <span class="p">(</span>
                        <span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                        <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="o">+</span> <span class="mi">1</span>
                    <span class="p">),</span>
                <span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">output_padding</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
                <span class="n">dilation</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dilation</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">y</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">(</span><span class="n">RkoConvTranspose2d</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">











  </div>

    </div>

</div>




  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.88dd0f4e.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../../js/custom.js"></script>
      
    
  </body>
</html>