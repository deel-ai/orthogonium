
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../..">
      
      
        <link rel="next" href="../linear/">
      
      
      <link rel="icon" href="../../assets/banner.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.3">
    
    
      
        <title>convolutions - orthogonium</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.d7758b05.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../assets/_mkdocstrings.css">
    
      <link rel="stylesheet" href="../../css/custom.css">
    
      <link rel="stylesheet" href="../../css/ansi-colours.css">
    
      <link rel="stylesheet" href="../../css/jupyter-cells.css">
    
      <link rel="stylesheet" href="../../css/pandas-dataframe.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="dark" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#orthogonium.layers.conv.AOC.ortho_conv" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="orthogonium" class="md-header__button md-logo" aria-label="orthogonium" data-md-component="logo">
      
  <img src="../../assets/banner.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            orthogonium
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              convolutions
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="dark" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6m0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4M7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/thib-s/orthogonium" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    thib-s/orthogonium
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="orthogonium" class="md-nav__button md-logo" aria-label="orthogonium" data-md-component="logo">
      
  <img src="../../assets/banner.png" alt="logo">

    </a>
    orthogonium
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/thib-s/orthogonium" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.7.2 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8M97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    thib-s/orthogonium
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    API Reference
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            API Reference
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    convolutions
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    convolutions
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv" class="md-nav__link">
    <span class="md-ellipsis">
      ortho_conv
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveOrthoConv2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveOrthoConv2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveOrthoConvTranspose2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveOrthoConvTranspose2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer" class="md-nav__link">
    <span class="md-ellipsis">
      sll_layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer--ssl-derived-1-lipschitz-layers" class="md-nav__link">
    <span class="md-ellipsis">
      SSL derived 1-Lipschitz Layers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SSL derived 1-Lipschitz Layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer--references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer--notes-on-the-sll-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Notes on the SLL approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock" class="md-nav__link">
    <span class="md-ellipsis">
      AOCLipschitzResBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AOCLipschitzResBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense" class="md-nav__link">
    <span class="md-ellipsis">
      SDPBasedLipschitzDense
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SDPBasedLipschitzDense">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock" class="md-nav__link">
    <span class="md-ellipsis">
      SDPBasedLipschitzResBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SDPBasedLipschitzResBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock" class="md-nav__link">
    <span class="md-ellipsis">
      SLLxAOCLipschitzResBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SLLxAOCLipschitzResBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol" class="md-nav__link">
    <span class="md-ellipsis">
      aol
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConv2D" class="md-nav__link">
    <span class="md-ellipsis">
      AOLConv2D
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AOLConv2D">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConv2D.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConv2D.reset_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      reset_parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D" class="md-nav__link">
    <span class="md-ellipsis">
      AOLConvTranspose2D
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AOLConvTranspose2D">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer" class="md-nav__link">
    <span class="md-ellipsis">
      MultiStepAOLReparametrizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MultiStepAOLReparametrizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer.reset_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      reset_parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv" class="md-nav__link">
    <span class="md-ellipsis">
      ortho_conv
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveSOCConv2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveSOCConv2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveSOCConvTranspose2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveSOCConvTranspose2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../linear/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    linear layers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../reparametrizers/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    reparametrizers
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../activations/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    activations
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../losses/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    losses
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../singular_values/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    singular values
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Tutorials
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Tutorials
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/demo_cifar_classification/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Demo 1: Certifiable robustness with 1-Lipschitz networks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CONTRIBUTING/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Contributing
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv" class="md-nav__link">
    <span class="md-ellipsis">
      ortho_conv
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveOrthoConv2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveOrthoConv2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveOrthoConvTranspose2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveOrthoConvTranspose2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer" class="md-nav__link">
    <span class="md-ellipsis">
      sll_layer
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer--ssl-derived-1-lipschitz-layers" class="md-nav__link">
    <span class="md-ellipsis">
      SSL derived 1-Lipschitz Layers
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SSL derived 1-Lipschitz Layers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer--references" class="md-nav__link">
    <span class="md-ellipsis">
      References
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer--notes-on-the-sll-approach" class="md-nav__link">
    <span class="md-ellipsis">
      Notes on the SLL approach
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock" class="md-nav__link">
    <span class="md-ellipsis">
      AOCLipschitzResBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AOCLipschitzResBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense" class="md-nav__link">
    <span class="md-ellipsis">
      SDPBasedLipschitzDense
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SDPBasedLipschitzDense">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock" class="md-nav__link">
    <span class="md-ellipsis">
      SDPBasedLipschitzResBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SDPBasedLipschitzResBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock" class="md-nav__link">
    <span class="md-ellipsis">
      SLLxAOCLipschitzResBlock
    </span>
  </a>
  
    <nav class="md-nav" aria-label="SLLxAOCLipschitzResBlock">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol" class="md-nav__link">
    <span class="md-ellipsis">
      aol
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConv2D" class="md-nav__link">
    <span class="md-ellipsis">
      AOLConv2D
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AOLConv2D">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConv2D.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConv2D.reset_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      reset_parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D" class="md-nav__link">
    <span class="md-ellipsis">
      AOLConvTranspose2D
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AOLConvTranspose2D">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D.__init__" class="md-nav__link">
    <span class="md-ellipsis">
      __init__
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer" class="md-nav__link">
    <span class="md-ellipsis">
      MultiStepAOLReparametrizer
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MultiStepAOLReparametrizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer.reset_parameters" class="md-nav__link">
    <span class="md-ellipsis">
      reset_parameters
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv" class="md-nav__link">
    <span class="md-ellipsis">
      ortho_conv
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveSOCConv2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveSOCConv2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d" class="md-nav__link">
    <span class="md-ellipsis">
      AdaptiveSOCConvTranspose2d
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AdaptiveSOCConvTranspose2d">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--key-features" class="md-nav__link">
    <span class="md-ellipsis">
      Key Features:
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--behavior" class="md-nav__link">
    <span class="md-ellipsis">
      Behavior:
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


  
  


  <h1>convolutions</h1>

<div class="doc doc-object doc-module">



<a id="orthogonium.layers.conv.AOC.ortho_conv"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">AdaptiveOrthoConv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;circular&#39;</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel
size and stride. This is the implementation for the <code>Adaptive Orthogonal Convolution</code> scheme [1]. It aims to be
scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers.
This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class (e.g., striding, dilation,
grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a
standard convolutional layer, but the weight are constrained to be orthogonal.</p>
<h4 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features">Key Features:<a class="headerlink" href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--key-features" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- Enforces orthogonality, preserving gradient norms.
- Supports native striding, dilation, grouped convolutions, and flexible padding.
</code></pre></div>
<h4 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior">Behavior:<a class="headerlink" href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConv2d--behavior" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- When kernel_size == stride, the layer is an `RKOConv2d`.
- When stride == 1, the layer is a `FastBlockConv2d`.
- Otherwise, the layer is a `BcopRkoConv2d`.
</code></pre></div>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>This implementation also work under zero padding, it lipschitz constant is still tight, but it looses
    orthogonality.orthogonality on the image border.</li>
<li>the unit tesing validated for a tolerance of 1e-4 under various orthogonalization schemes (see
    reparametrizers). Only Cholesky based methods were validated for a lower tolerance of 5e-2.</li>
</ul>
</details>

<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of input channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of output channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the convolution kernel.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stride of the convolution. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td>
                  <code>str or <span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode or size. Default is "same".</p>
              </div>
            </td>
            <td>
                  <code>&#39;same&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dilation</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dilation rate. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of blocked connections from input to output channels. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include a learnable bias. Default is True.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mode</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode. Default is "circular".</p>
              </div>
            </td>
            <td>
                  <code>&#39;circular&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ortho_params</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p>
              </div>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a>()</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.Conv2d">Conv2d</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>`ValueError`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="references" open>
  <summary>References</summary>
  <ul>
<li>[1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).
An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.
<a href="https://arxiv.org/abs/2501.07930">https://arxiv.org/abs/2501.07930</a></li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span>
<span class="normal">85</span>
<span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">AdaptiveOrthoConv2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;same&quot;</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;circular&quot;</span><span class="p">,</span>
    <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel</span>
<span class="sd">    size and stride. This is the implementation for the `Adaptive Orthogonal Convolution` scheme [1]. It aims to be</span>
<span class="sd">    scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers.</span>
<span class="sd">    This layer also intend to be compatible with all the feature of the `nn.Conv2d` class (e.g., striding, dilation,</span>
<span class="sd">    grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a</span>
<span class="sd">    standard convolutional layer, but the weight are constrained to be orthogonal.</span>

<span class="sd">    Key Features:</span>
<span class="sd">    -------------</span>
<span class="sd">        - Enforces orthogonality, preserving gradient norms.</span>
<span class="sd">        - Supports native striding, dilation, grouped convolutions, and flexible padding.</span>

<span class="sd">    Behavior:</span>
<span class="sd">    ---------</span>
<span class="sd">        - When kernel_size == stride, the layer is an `RKOConv2d`.</span>
<span class="sd">        - When stride == 1, the layer is a `FastBlockConv2d`.</span>
<span class="sd">        - Otherwise, the layer is a `BcopRkoConv2d`.</span>

<span class="sd">    Note:</span>
<span class="sd">        - This implementation also work under zero padding, it lipschitz constant is still tight, but it looses</span>
<span class="sd">            orthogonality.orthogonality on the image border.</span>
<span class="sd">        - the unit tesing validated for a tolerance of 1e-4 under various orthogonalization schemes (see</span>
<span class="sd">            reparametrizers). Only Cholesky based methods were validated for a lower tolerance of 5e-2.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        in_channels (int): Number of input channels.</span>
<span class="sd">        out_channels (int): Number of output channels.</span>
<span class="sd">        kernel_size (_size_2_t): Size of the convolution kernel.</span>
<span class="sd">        stride (_size_2_t, optional): Stride of the convolution. Default is 1.</span>
<span class="sd">        padding (str or _size_2_t, optional): Padding mode or size. Default is &quot;same&quot;.</span>
<span class="sd">        dilation (_size_2_t, optional): Dilation rate. Default is 1.</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.</span>
<span class="sd">        bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">        padding_mode (str, optional): Padding mode. Default is &quot;circular&quot;.</span>
<span class="sd">        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).</span>

<span class="sd">    Raises:</span>
<span class="sd">        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.</span>


<span class="sd">    References:</span>
<span class="sd">        - [1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).</span>
<span class="sd">        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.</span>
<span class="sd">        &lt;https://arxiv.org/abs/2501.07930&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="n">stride</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">RKOConv2d</span>
    <span class="k">elif</span> <span class="p">(</span><span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">or</span> <span class="p">((</span><span class="n">in_channels</span> <span class="o">&gt;=</span> <span class="n">out_channels</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">dilation</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)):</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">FastBlockConv2d</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">BcopRkoConv2d</span>
    <span class="k">return</span> <span class="n">convclass</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">AdaptiveOrthoConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on kernel
size and stride. This is the implementation for the <code>Adaptive Orthogonal Convolution</code> scheme [1]. It aims to be
scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers.
This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class (e.g., striding, dilation,
grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a
standard convolutional layer, but the weight are constrained to be orthogonal.</p>
<h4 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features">Key Features:<a class="headerlink" href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--key-features" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- Ensures orthogonality in transpose convolutions for stable gradient propagation.
- Supports dilation, grouped operations, and efficient kernel construction.
</code></pre></div>
<h4 id="orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior">Behavior:<a class="headerlink" href="#orthogonium.layers.conv.AOC.ortho_conv.AdaptiveOrthoConvTranspose2d--behavior" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- When kernel_size == stride, the layer is an `RkoConvTranspose2d`.
- When stride == 1, the layer is a `FastBlockConvTranspose2D`.
- Otherwise, the layer is a `BcopRkoConvTranspose2d`.
</code></pre></div>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>This implementation also work under zero padding, it lipschitz constant is still tight, but it looses
    orthogonality.orthogonality on the image border.</li>
<li>The current implementation of the torch.nn.ConvTranspose2d does not support circular padding. One can
    implement padding manually by add a padding layer before and setting padding = (0,0).</li>
</ul>
</details>

<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of input channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of output channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the convolution kernel.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stride of the transpose convolution. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding size. Default is 0.</p>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_padding</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional size for output. Default is 0.</p>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of groups. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include a learnable bias. Default is True.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dilation</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dilation rate. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mode</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode. Default is "zeros".</p>
              </div>
            </td>
            <td>
                  <code>&#39;zeros&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ortho_params</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p>
              </div>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a>()</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.ConvTranspose2d">ConvTranspose2d</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A configured instance of <code>nn.ConvTranspose2d</code> (one of <code>RkoConvTranspose2d</code>, <code>FastBlockConvTranspose2D</code>, or <code>BcopRkoConvTranspose2d</code>).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>
        <p><strong>Raises:</strong>
- <code>ValueError</code>: If kernel_size &lt; stride, as orthogonality cannot be enforced.</p>


<details class="references" open>
  <summary>References</summary>
  <ul>
<li>[1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).
An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.
<a href="https://arxiv.org/abs/2501.07930">https://arxiv.org/abs/2501.07930</a></li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOC\ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">AdaptiveOrthoConvTranspose2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
    <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on kernel</span>
<span class="sd">    size and stride. This is the implementation for the `Adaptive Orthogonal Convolution` scheme [1]. It aims to be</span>
<span class="sd">    scalable to large networks and large image sizes, while enforcing orthogonality in the convolutional layers.</span>
<span class="sd">    This layer also intend to be compatible with all the feature of the `nn.Conv2d` class (e.g., striding, dilation,</span>
<span class="sd">    grouping, etc.). This method has an explicit kernel, which means that the forward operation is equivalent to a</span>
<span class="sd">    standard convolutional layer, but the weight are constrained to be orthogonal.</span>

<span class="sd">    Key Features:</span>
<span class="sd">    -------------</span>
<span class="sd">        - Ensures orthogonality in transpose convolutions for stable gradient propagation.</span>
<span class="sd">        - Supports dilation, grouped operations, and efficient kernel construction.</span>

<span class="sd">    Behavior:</span>
<span class="sd">    ---------</span>
<span class="sd">        - When kernel_size == stride, the layer is an `RkoConvTranspose2d`.</span>
<span class="sd">        - When stride == 1, the layer is a `FastBlockConvTranspose2D`.</span>
<span class="sd">        - Otherwise, the layer is a `BcopRkoConvTranspose2d`.</span>


<span class="sd">    Note:</span>
<span class="sd">        - This implementation also work under zero padding, it lipschitz constant is still tight, but it looses</span>
<span class="sd">            orthogonality.orthogonality on the image border.</span>
<span class="sd">        - The current implementation of the torch.nn.ConvTranspose2d does not support circular padding. One can</span>
<span class="sd">            implement padding manually by add a padding layer before and setting padding = (0,0).</span>

<span class="sd">    Arguments:</span>
<span class="sd">        in_channels (int): Number of input channels.</span>
<span class="sd">        out_channels (int): Number of output channels.</span>
<span class="sd">        kernel_size (_size_2_t): Size of the convolution kernel.</span>
<span class="sd">        stride (_size_2_t, optional): Stride of the transpose convolution. Default is 1.</span>
<span class="sd">        padding (_size_2_t, optional): Padding size. Default is 0.</span>
<span class="sd">        output_padding (_size_2_t, optional): Additional size for output. Default is 0.</span>
<span class="sd">        groups (int, optional): Number of groups. Default is 1.</span>
<span class="sd">        bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">        dilation (_size_2_t, optional): Dilation rate. Default is 1.</span>
<span class="sd">        padding_mode (str, optional): Padding mode. Default is &quot;zeros&quot;.</span>
<span class="sd">        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A configured instance of `nn.ConvTranspose2d` (one of `RkoConvTranspose2d`, `FastBlockConvTranspose2D`, or `BcopRkoConvTranspose2d`).</span>

<span class="sd">    **Raises:**</span>
<span class="sd">    - `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.</span>


<span class="sd">    References:</span>
<span class="sd">        - [1] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).</span>
<span class="sd">        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.</span>
<span class="sd">        &lt;https://arxiv.org/abs/2501.07930&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="n">stride</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">RkoConvTranspose2d</span>
    <span class="k">elif</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">FastBlockConvTranspose2D</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">BcopRkoConvTranspose2d</span>
    <span class="k">return</span> <span class="n">convclass</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="o">=</span><span class="n">output_padding</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="orthogonium.layers.conv.SLL.sll_layer"></a>
    <div class="doc doc-contents first">

        <h2 id="orthogonium.layers.conv.SLL.sll_layer--ssl-derived-1-lipschitz-layers">SSL derived 1-Lipschitz Layers<a class="headerlink" href="#orthogonium.layers.conv.SLL.sll_layer--ssl-derived-1-lipschitz-layers" title="Permanent link">&para;</a></h2>
<p>This module implements several 1-Lipschitz residual blocks, inspired by and extending
the SDP-based Lipschitz Layers (SLL) from [1]. Specifically:</p>
<ul>
<li>
<p><strong><code>SDPBasedLipschitzResBlock</code></strong><br />
  The original version of the 1-Lipschitz convolutional residual block. It enforces Lipschitz
  constraints by rescaling activation outputs according to an estimate of the operator norm.</p>
</li>
<li>
<p><strong><code>SLLxAOCLipschitzResBlock</code></strong><br />
  An extended version of the SLL approach described in [1], combined with additional orthogonal
  convolutions to handle stride, kernel-size, or channel-dimension changes. It fuses multiple
  convolutions via the block convolution, thereby preserving the 1-Lipschitz property while enabling
  strided downsampling or modifying input/output channels.</p>
</li>
<li>
<p><strong><code>AOCLipschitzResBlock</code></strong><br />
  A variant of the original Lipschitz block where the core convolution is replaced by an
  <code>AdaptiveOrthoConv2d</code>. It maintains the 1-Lipschitz property with orthogonal weight
  parameterization while providing efficient convolution implementations.</p>
</li>
</ul>
<h3 id="orthogonium.layers.conv.SLL.sll_layer--references">References<a class="headerlink" href="#orthogonium.layers.conv.SLL.sll_layer--references" title="Permanent link">&para;</a></h3>
<p>[1] Alexandre Araujo, Aaron J Havens, Blaise Delattre, Alexandre Allauzen, and Bin Hu. A unified alge-
braic perspective on lipschitz neural networks. In The Eleventh International Conference on Learning
Representations, 2023
[2] Thibaut Boissin, Franck Mamalet, Thomas Fel, Agustin Martin Picard, Thomas Massena, Mathieu Serrurier,
An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures</p>
<h3 id="orthogonium.layers.conv.SLL.sll_layer--notes-on-the-sll-approach">Notes on the SLL approach<a class="headerlink" href="#orthogonium.layers.conv.SLL.sll_layer--notes-on-the-sll-approach" title="Permanent link">&para;</a></h3>
<p>In [1], the SLL layer for convolutions is a 1-Lipschitz residual operation defined approximately as:</p>
<div class="arithmatex">\[
y = x - \mathbf{K}^T \star (t \times  \sigma(\mathbf{K} \star x + b)),
\]</div>
<p>where <span class="arithmatex">\(\mathbf{K}\)</span> represents a toeplitz (convolution) matrix that represent a 1-Lipschitz operator.
This is done in practice by computing a normalization vector <span class="arithmatex">\(\mathbf{t}\)</span> and rescaling the
activation outputs by <span class="arithmatex">\(\mathbf{t}\)</span>.</p>
<p>By default, the SLL formulation does <strong>not</strong> allow strides or changes in the number of channels.<br />
To address these issues, <code>SLLxAOCLipschitzResBlock</code> adds extra orthogonal convolutions before and/or
after the main SLL operation. These additional convolutions can be merged via block convolution
(Proposition 1 in [2]) to maintain 1-Lipschitz behavior while enabling stride and/or channel changes.</p>
<p>When <span class="arithmatex">\(\mathbf{K}\)</span>, <span class="arithmatex">\(\mathbf{K}_{pre}\)</span>, and <span class="arithmatex">\(\mathbf{K}_{post}\)</span> each correspond to 2×2 convolutions,
the resulting block effectively contains two 3×3 convolutions in one branch and a single 4×4 stride-2
convolution in the skip branch—quite similar to typical ResNet blocks.</p>








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock" class="doc doc-heading">
            <code>AOCLipschitzResBlock</code>


<a href="#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\SLL\sll_layer.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AOCLipschitzResBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">inner_dim_factor</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;circular&quot;</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A Lipschitz residual block in which the main convolution is replaced by</span>
<span class="sd">        `AdaptiveOrthoConv2d` (AOC). This preserves 1-Lipschitz (or lower) behavior through</span>
<span class="sd">        an orthogonal parameterization, without explicitly computing a scaling factor `t`.</span>

<span class="sd">        $$</span>
<span class="sd">        y = x - \mathbf{K}^T \\star (\sigma(\\mathbf{K} \\star x + b)),</span>
<span class="sd">        $$</span>

<span class="sd">        **Args**:</span>
<span class="sd">          - `in_channels` (int): Number of input channels.</span>
<span class="sd">          - `inner_dim_factor` (int): Multiplier for internal representation size.</span>
<span class="sd">          - `kernel_size` (_size_2_t): Convolution kernel size.</span>
<span class="sd">          - `dilation` (_size_2_t, optional): Default is 1.</span>
<span class="sd">          - `groups` (int, optional): Default is 1.</span>
<span class="sd">          - `bias` (bool, optional): If True, adds a learnable bias. Default is True.</span>
<span class="sd">          - `padding_mode` (str, optional): `&#39;circular&#39;` or `&#39;zeros&#39;`. Default is `&#39;circular&#39;`.</span>
<span class="sd">          - `ortho_params` (OrthoParams, optional): Orthogonal parameterization settings. Default is `OrthoParams()`.</span>


<span class="sd">        References:</span>
<span class="sd">            - [1] Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.</span>
<span class="sd">            A Unified Algebraic Perspective on Lipschitz Neural Networks.</span>
<span class="sd">            In The Eleventh International Conference on Learning Representations.</span>
<span class="sd">            &lt;https://arxiv.org/abs/2303.03169&gt;</span>
<span class="sd">            - [2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).</span>
<span class="sd">            An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.</span>
<span class="sd">            &lt;https://arxiv.org/abs/2501.07930&gt;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">inner_dim_factor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">padding_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;circular&quot;</span><span class="p">,</span> <span class="s2">&quot;zeros&quot;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;padding_mode must be either &#39;circular&#39; or &#39;zeros&#39;&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s2">&quot;circular&quot;</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># will be handled by the padding function</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">in_conv</span> <span class="o">=</span> <span class="n">AdaptiveOrthoConv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="p">,</span>
            <span class="n">inner_dim</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
            <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_conv</span><span class="o">.</span><span class="n">weight</span>
        <span class="c1"># conv</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">x</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">==</span> <span class="s2">&quot;circular&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">res</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;circular&quot;</span><span class="p">,</span>
                <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
            <span class="n">res</span><span class="p">,</span>
            <span class="n">kernel</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">in_conv</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># activation</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="c1"># conv transpose</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">==</span> <span class="s2">&quot;circular&quot;</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span>
                <span class="n">res</span><span class="p">,</span>
                <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,)</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span>
                <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;circular&quot;</span><span class="p">,</span>
                <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="c1"># residual</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">res</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">inner_dim_factor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;circular&#39;</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.SLL.sll_layer.AOCLipschitzResBlock.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>A Lipschitz residual block in which the main convolution is replaced by
<code>AdaptiveOrthoConv2d</code> (AOC). This preserves 1-Lipschitz (or lower) behavior through
an orthogonal parameterization, without explicitly computing a scaling factor <code>t</code>.</p>
<div class="arithmatex">\[
y = x - \mathbf{K}^T \star (\sigma(\mathbf{K} \star x + b)),
\]</div>
<p><strong>Args</strong>:
  - <code>in_channels</code> (int): Number of input channels.
  - <code>inner_dim_factor</code> (int): Multiplier for internal representation size.
  - <code>kernel_size</code> (_size_2_t): Convolution kernel size.
  - <code>dilation</code> (_size_2_t, optional): Default is 1.
  - <code>groups</code> (int, optional): Default is 1.
  - <code>bias</code> (bool, optional): If True, adds a learnable bias. Default is True.
  - <code>padding_mode</code> (str, optional): <code>'circular'</code> or <code>'zeros'</code>. Default is <code>'circular'</code>.
  - <code>ortho_params</code> (OrthoParams, optional): Orthogonal parameterization settings. Default is <code>OrthoParams()</code>.</p>


<details class="references" open>
  <summary>References</summary>
  <ul>
<li>[1] Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.
A Unified Algebraic Perspective on Lipschitz Neural Networks.
In The Eleventh International Conference on Learning Representations.
<a href="https://arxiv.org/abs/2303.03169">https://arxiv.org/abs/2303.03169</a></li>
<li>[2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).
An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.
<a href="https://arxiv.org/abs/2501.07930">https://arxiv.org/abs/2501.07930</a></li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\SLL\sll_layer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">inner_dim_factor</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;circular&quot;</span><span class="p">,</span>
    <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A Lipschitz residual block in which the main convolution is replaced by</span>
<span class="sd">    `AdaptiveOrthoConv2d` (AOC). This preserves 1-Lipschitz (or lower) behavior through</span>
<span class="sd">    an orthogonal parameterization, without explicitly computing a scaling factor `t`.</span>

<span class="sd">    $$</span>
<span class="sd">    y = x - \mathbf{K}^T \\star (\sigma(\\mathbf{K} \\star x + b)),</span>
<span class="sd">    $$</span>

<span class="sd">    **Args**:</span>
<span class="sd">      - `in_channels` (int): Number of input channels.</span>
<span class="sd">      - `inner_dim_factor` (int): Multiplier for internal representation size.</span>
<span class="sd">      - `kernel_size` (_size_2_t): Convolution kernel size.</span>
<span class="sd">      - `dilation` (_size_2_t, optional): Default is 1.</span>
<span class="sd">      - `groups` (int, optional): Default is 1.</span>
<span class="sd">      - `bias` (bool, optional): If True, adds a learnable bias. Default is True.</span>
<span class="sd">      - `padding_mode` (str, optional): `&#39;circular&#39;` or `&#39;zeros&#39;`. Default is `&#39;circular&#39;`.</span>
<span class="sd">      - `ortho_params` (OrthoParams, optional): Orthogonal parameterization settings. Default is `OrthoParams()`.</span>


<span class="sd">    References:</span>
<span class="sd">        - [1] Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.</span>
<span class="sd">        A Unified Algebraic Perspective on Lipschitz Neural Networks.</span>
<span class="sd">        In The Eleventh International Conference on Learning Representations.</span>
<span class="sd">        &lt;https://arxiv.org/abs/2303.03169&gt;</span>
<span class="sd">        - [2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).</span>
<span class="sd">        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.</span>
<span class="sd">        &lt;https://arxiv.org/abs/2501.07930&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="n">inner_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">inner_dim_factor</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">padding_mode</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;circular&quot;</span><span class="p">,</span> <span class="s2">&quot;zeros&quot;</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;padding_mode must be either &#39;circular&#39; or &#39;zeros&#39;&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">padding_mode</span> <span class="o">==</span> <span class="s2">&quot;circular&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># will be handled by the padding function</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">in_conv</span> <span class="o">=</span> <span class="n">AdaptiveOrthoConv2d</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">inner_dim</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="s2">&quot;same&quot;</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
        <span class="n">ortho_params</span><span class="o">=</span><span class="n">ortho_params</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dilation</span> <span class="o">=</span> <span class="n">dilation</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">bias</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding_mode</span> <span class="o">=</span> <span class="n">padding_mode</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense" class="doc doc-heading">
            <code>SDPBasedLipschitzDense</code>


<a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\SLL\sll_layer.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SDPBasedLipschitzDense</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        A 1-Lipschitz fully-connected layer (dense version). Similar to the convolutional</span>
<span class="sd">        SLL approach, but operates on vectors:</span>

<span class="sd">        $$</span>
<span class="sd">        y = x - K^T \\times (t \\times \sigma(K \\times x + b)),</span>
<span class="sd">        $$</span>

<span class="sd">        **Args**:</span>
<span class="sd">          - `in_features` (int): Input size.</span>
<span class="sd">          - `out_features` (int): Output size (must match `in_features` to remain 1-Lipschitz).</span>
<span class="sd">          - `inner_dim` (int): The internal dimension used for the transform.</span>


<span class="sd">        References:</span>
<span class="sd">            - Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.</span>
<span class="sd">            A Unified Algebraic Perspective on Lipschitz Neural Networks.</span>
<span class="sd">            In The Eleventh International Conference on Learning Representations.</span>
<span class="sd">            &lt;https://arxiv.org/abs/2303.03169&gt;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="k">if</span> <span class="n">inner_dim</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">in_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">in_features</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">))</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>  <span class="c1"># bias init</span>

    <span class="k">def</span> <span class="nf">compute_t</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">q_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;i,ik,kj,j -&gt; ij&quot;</span><span class="p">,</span> <span class="n">q_inv</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">q</span><span class="p">)</span>
        <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">safe_inv</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">t</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_t</span><span class="p">()</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">res</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzDense.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>A 1-Lipschitz fully-connected layer (dense version). Similar to the convolutional
SLL approach, but operates on vectors:</p>
<div class="arithmatex">\[
y = x - K^T \times (t \times \sigma(K \times x + b)),
\]</div>
<p><strong>Args</strong>:
  - <code>in_features</code> (int): Input size.
  - <code>out_features</code> (int): Output size (must match <code>in_features</code> to remain 1-Lipschitz).
  - <code>inner_dim</code> (int): The internal dimension used for the transform.</p>


<details class="references" open>
  <summary>References</summary>
  <ul>
<li>Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.
A Unified Algebraic Perspective on Lipschitz Neural Networks.
In The Eleventh International Conference on Learning Representations.
<a href="https://arxiv.org/abs/2303.03169">https://arxiv.org/abs/2303.03169</a></li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\SLL\sll_layer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A 1-Lipschitz fully-connected layer (dense version). Similar to the convolutional</span>
<span class="sd">    SLL approach, but operates on vectors:</span>

<span class="sd">    $$</span>
<span class="sd">    y = x - K^T \\times (t \\times \sigma(K \\times x + b)),</span>
<span class="sd">    $$</span>

<span class="sd">    **Args**:</span>
<span class="sd">      - `in_features` (int): Input size.</span>
<span class="sd">      - `out_features` (int): Output size (must match `in_features` to remain 1-Lipschitz).</span>
<span class="sd">      - `inner_dim` (int): The internal dimension used for the transform.</span>


<span class="sd">    References:</span>
<span class="sd">        - Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.</span>
<span class="sd">        A Unified Algebraic Perspective on Lipschitz Neural Networks.</span>
<span class="sd">        In The Eleventh International Conference on Learning Representations.</span>
<span class="sd">        &lt;https://arxiv.org/abs/2303.03169&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">inner_dim</span> <span class="k">if</span> <span class="n">inner_dim</span> <span class="o">!=</span> <span class="o">-</span><span class="mi">1</span> <span class="k">else</span> <span class="n">in_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">in_features</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">))</span>

    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>  <span class="c1"># bias init</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock" class="doc doc-heading">
            <code>SDPBasedLipschitzResBlock</code>


<a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\SLL\sll_layer.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SDPBasedLipschitzResBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cin</span><span class="p">,</span> <span class="n">inner_dim_factor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">         Original 1-Lipschitz convolutional residual block, based on the SDP-based Lipschitz</span>
<span class="sd">        layer (SLL) approach [1]. It has a structure akin to:</span>

<span class="sd">        out = x - 2 * ConvTranspose( t * ReLU(Conv(x) + bias) )</span>

<span class="sd">        where `t` is a channel-wise scaling factor ensuring a Lipschitz constant ≤ 1.</span>

<span class="sd">        !!! note</span>
<span class="sd">            By default, `SDPBasedLipschitzResBlock` assumes `cin == cout` and does **not** handle</span>
<span class="sd">            stride changes outside the skip connection (i.e., typically used when stride=1 or 2</span>
<span class="sd">            for downsampling in a standard residual architecture).</span>

<span class="sd">        **Args**:</span>
<span class="sd">          - `cin` (int): Number of input channels.</span>
<span class="sd">          - `cout` (int): Number of output channels.</span>
<span class="sd">          - `inner_dim_factor` (float): Multiplier for the intermediate dimensionality.</span>
<span class="sd">          - `kernel_size` (int, optional): Size of the convolution kernel. Default is 3.</span>
<span class="sd">          - `groups` (int, optional): Number of groups for the convolution. Default is 1.</span>
<span class="sd">          - `**kwargs`: Additional keyword arguments (unused).</span>


<span class="sd">        References:</span>
<span class="sd">            - Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.</span>
<span class="sd">            A Unified Algebraic Perspective on Lipschitz Neural Networks.</span>
<span class="sd">            In The Eleventh International Conference on Learning Representations.</span>
<span class="sd">            &lt;https://arxiv.org/abs/2303.03169&gt;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="n">inner_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cin</span> <span class="o">*</span> <span class="n">inner_dim_factor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">cin</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="s2">&quot;kernel&quot;</span><span class="p">,</span>
            <span class="n">AOLReparametrizer</span><span class="p">(</span>
                <span class="n">inner_dim</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>  <span class="c1"># bias init</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
        <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">parametrize</span><span class="o">.</span><span class="n">cached</span><span class="p">():</span>
            <span class="n">res</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">conv_transpose2d</span><span class="p">(</span>
                <span class="n">res</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
            <span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">res</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">inner_dim_factor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.SLL.sll_layer.SDPBasedLipschitzResBlock.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Original 1-Lipschitz convolutional residual block, based on the SDP-based Lipschitz
layer (SLL) approach [1]. It has a structure akin to:</p>
<p>out = x - 2 * ConvTranspose( t * ReLU(Conv(x) + bias) )</p>
<p>where <code>t</code> is a channel-wise scaling factor ensuring a Lipschitz constant ≤ 1.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, <code>SDPBasedLipschitzResBlock</code> assumes <code>cin == cout</code> and does <strong>not</strong> handle
stride changes outside the skip connection (i.e., typically used when stride=1 or 2
for downsampling in a standard residual architecture).</p>
</div>
<p><strong>Args</strong>:
  - <code>cin</code> (int): Number of input channels.
  - <code>cout</code> (int): Number of output channels.
  - <code>inner_dim_factor</code> (float): Multiplier for the intermediate dimensionality.
  - <code>kernel_size</code> (int, optional): Size of the convolution kernel. Default is 3.
  - <code>groups</code> (int, optional): Number of groups for the convolution. Default is 1.
  - <code>**kwargs</code>: Additional keyword arguments (unused).</p>


<details class="references" open>
  <summary>References</summary>
  <ul>
<li>Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.
A Unified Algebraic Perspective on Lipschitz Neural Networks.
In The Eleventh International Conference on Learning Representations.
<a href="https://arxiv.org/abs/2303.03169">https://arxiv.org/abs/2303.03169</a></li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\SLL\sll_layer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cin</span><span class="p">,</span> <span class="n">inner_dim_factor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">     Original 1-Lipschitz convolutional residual block, based on the SDP-based Lipschitz</span>
<span class="sd">    layer (SLL) approach [1]. It has a structure akin to:</span>

<span class="sd">    out = x - 2 * ConvTranspose( t * ReLU(Conv(x) + bias) )</span>

<span class="sd">    where `t` is a channel-wise scaling factor ensuring a Lipschitz constant ≤ 1.</span>

<span class="sd">    !!! note</span>
<span class="sd">        By default, `SDPBasedLipschitzResBlock` assumes `cin == cout` and does **not** handle</span>
<span class="sd">        stride changes outside the skip connection (i.e., typically used when stride=1 or 2</span>
<span class="sd">        for downsampling in a standard residual architecture).</span>

<span class="sd">    **Args**:</span>
<span class="sd">      - `cin` (int): Number of input channels.</span>
<span class="sd">      - `cout` (int): Number of output channels.</span>
<span class="sd">      - `inner_dim_factor` (float): Multiplier for the intermediate dimensionality.</span>
<span class="sd">      - `kernel_size` (int, optional): Size of the convolution kernel. Default is 3.</span>
<span class="sd">      - `groups` (int, optional): Number of groups for the convolution. Default is 1.</span>
<span class="sd">      - `**kwargs`: Additional keyword arguments (unused).</span>


<span class="sd">    References:</span>
<span class="sd">        - Araujo, A., Havens, A. J., Delattre, B., Allauzen, A., &amp; Hu, B.</span>
<span class="sd">        A Unified Algebraic Perspective on Lipschitz Neural Networks.</span>
<span class="sd">        In The Eleventh International Conference on Learning Representations.</span>
<span class="sd">        &lt;https://arxiv.org/abs/2303.03169&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="n">inner_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cin</span> <span class="o">*</span> <span class="n">inner_dim_factor</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="n">cin</span> <span class="o">//</span> <span class="n">groups</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="s2">&quot;kernel&quot;</span><span class="p">,</span>
        <span class="n">AOLReparametrizer</span><span class="p">(</span>
            <span class="n">inner_dim</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>  <span class="c1"># bias init</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock" class="doc doc-heading">
            <code>SLLxAOCLipschitzResBlock</code>


<a href="#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\SLL\sll_layer.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">SLLxAOCLipschitzResBlock</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">cin</span><span class="p">,</span> <span class="n">cout</span><span class="p">,</span> <span class="n">inner_dim_factor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Extended SLL-based convolutional residual block. Supports arbitrary kernel sizes,</span>
<span class="sd">        strides, and changes in the number of channels by integrating additional</span>
<span class="sd">        orthogonal convolutions *and* fusing them via `\mathbconv` [1].</span>

<span class="sd">        The forward pass follows:</span>

<span class="sd">        $$</span>
<span class="sd">        y = (\mathbf{K}_{post} \circledast \mathbf{K}_{pre}) \\star x - (\mathbf{K}_{post} \circledast \mathbf{K}^T) \\star (t \\times  \sigma(( \mathbf{K} \circledast \mathbf{K}_{pre}) \\star x + b)),</span>
<span class="sd">        $$</span>

<span class="sd">        where $\mathbf{K}_{pre}$ and $\mathbf{K}_{post}$ are obtained with AOC.</span>


<span class="sd">        &lt;img src=&quot;../../assets/SLL_3.png&quot; alt=&quot;illustration of SLL x AOC&quot; width=&quot;600&quot;&gt;</span>



<span class="sd">        where the kernel `\kernel{K}` may effectively be expanded by pre/post AOC layers to</span>
<span class="sd">        handle stride and channel changes. This approach is described in &quot;Improving</span>
<span class="sd">        SDP-based Lipschitz Layers&quot; of [1].</span>

<span class="sd">        **Args**:</span>
<span class="sd">          - `cin` (int): Number of input channels.</span>
<span class="sd">          - `inner_dim_factor` (float): Multiplier for the internal channel dimension.</span>
<span class="sd">          - `kernel_size` (int, optional): Base kernel size for the SLL portion. Default is 3.</span>
<span class="sd">          - `stride` (int, optional): Stride for the skip connection. Default is 2.</span>
<span class="sd">          - `groups` (int, optional): Number of groups for the convolution. Default is 1.</span>
<span class="sd">          - `**kwargs`: Additional options (unused).</span>



<span class="sd">        References:</span>
<span class="sd">            - Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).</span>
<span class="sd">            An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.</span>
<span class="sd">            &lt;https://arxiv.org/abs/2501.07930&gt;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">inner_kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="p">(</span><span class="n">stride</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">skip_kernel_size</span> <span class="o">=</span> <span class="n">stride</span> <span class="o">+</span> <span class="p">(</span><span class="n">stride</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">inner_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cout</span> <span class="o">*</span> <span class="n">inner_dim_factor</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
                <span class="n">inner_dim</span><span class="p">,</span> <span class="n">cin</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="n">inner_kernel_size</span><span class="p">,</span> <span class="n">inner_kernel_size</span>
            <span class="p">)</span>
        <span class="p">)</span>
        <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="s2">&quot;kernel&quot;</span><span class="p">,</span>
            <span class="n">AOLReparametrizer</span><span class="p">(</span>
                <span class="n">inner_dim</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
        <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
        <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>  <span class="c1"># bias init</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">pre_conv</span> <span class="o">=</span> <span class="n">AdaptiveOrthoConv2d</span><span class="p">(</span>
            <span class="n">cin</span><span class="p">,</span> <span class="n">cin</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_conv</span> <span class="o">=</span> <span class="n">AdaptiveOrthoConv2d</span><span class="p">(</span>
            <span class="n">cin</span><span class="p">,</span>
            <span class="n">cout</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># compute t</span>
        <span class="c1"># print(self.pre_conv.weight.shape, self.kernel.shape, self.post_conv.weight.shape)</span>
        <span class="n">kernel_1a</span> <span class="o">=</span> <span class="n">fast_matrix_conv</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pre_conv</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
        <span class="p">)</span>
        <span class="k">with</span> <span class="n">parametrize</span><span class="o">.</span><span class="n">cached</span><span class="p">():</span>
            <span class="n">kernel_1b</span> <span class="o">=</span> <span class="n">fast_matrix_conv</span><span class="p">(</span>
                <span class="n">transpose_kernel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">),</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">post_conv</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">kernel_2</span> <span class="o">=</span> <span class="n">fast_matrix_conv</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pre_conv</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_conv</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
            <span class="p">)</span>
            <span class="c1"># first branch</span>
            <span class="c1"># fuse pre conv with kernel</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">kernel_1a</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">res</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
            <span class="n">res</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
            <span class="n">res</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
                <span class="n">res</span><span class="p">,</span>
                <span class="n">kernel_1b</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="c1"># residual branch</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span><span class="p">(</span>
                <span class="n">x</span><span class="p">,</span>
                <span class="n">kernel_2</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">skip_kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="c1"># skip connection</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">res</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">cout</span><span class="p">,</span> <span class="n">inner_dim_factor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.SLL.sll_layer.SLLxAOCLipschitzResBlock.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Extended SLL-based convolutional residual block. Supports arbitrary kernel sizes,
strides, and changes in the number of channels by integrating additional
orthogonal convolutions <em>and</em> fusing them via <code>\mathbconv</code> [1].</p>
<p>The forward pass follows:</p>
<div class="arithmatex">\[
y = (\mathbf{K}_{post} \circledast \mathbf{K}_{pre}) \star x - (\mathbf{K}_{post} \circledast \mathbf{K}^T) \star (t \times  \sigma(( \mathbf{K} \circledast \mathbf{K}_{pre}) \star x + b)),
\]</div>
<p>where <span class="arithmatex">\(\mathbf{K}_{pre}\)</span> and <span class="arithmatex">\(\mathbf{K}_{post}\)</span> are obtained with AOC.</p>
<p><img src="../../assets/SLL_3.png" alt="illustration of SLL x AOC" width="600"></p>
<p>where the kernel <code>\kernel{K}</code> may effectively be expanded by pre/post AOC layers to
handle stride and channel changes. This approach is described in "Improving
SDP-based Lipschitz Layers" of [1].</p>
<p><strong>Args</strong>:
  - <code>cin</code> (int): Number of input channels.
  - <code>inner_dim_factor</code> (float): Multiplier for the internal channel dimension.
  - <code>kernel_size</code> (int, optional): Base kernel size for the SLL portion. Default is 3.
  - <code>stride</code> (int, optional): Stride for the skip connection. Default is 2.
  - <code>groups</code> (int, optional): Number of groups for the convolution. Default is 1.
  - <code>**kwargs</code>: Additional options (unused).</p>


<details class="references" open>
  <summary>References</summary>
  <ul>
<li>Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).
An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.
<a href="https://arxiv.org/abs/2501.07930">https://arxiv.org/abs/2501.07930</a></li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\SLL\sll_layer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">cin</span><span class="p">,</span> <span class="n">cout</span><span class="p">,</span> <span class="n">inner_dim_factor</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Extended SLL-based convolutional residual block. Supports arbitrary kernel sizes,</span>
<span class="sd">    strides, and changes in the number of channels by integrating additional</span>
<span class="sd">    orthogonal convolutions *and* fusing them via `\mathbconv` [1].</span>

<span class="sd">    The forward pass follows:</span>

<span class="sd">    $$</span>
<span class="sd">    y = (\mathbf{K}_{post} \circledast \mathbf{K}_{pre}) \\star x - (\mathbf{K}_{post} \circledast \mathbf{K}^T) \\star (t \\times  \sigma(( \mathbf{K} \circledast \mathbf{K}_{pre}) \\star x + b)),</span>
<span class="sd">    $$</span>

<span class="sd">    where $\mathbf{K}_{pre}$ and $\mathbf{K}_{post}$ are obtained with AOC.</span>


<span class="sd">    &lt;img src=&quot;../../assets/SLL_3.png&quot; alt=&quot;illustration of SLL x AOC&quot; width=&quot;600&quot;&gt;</span>



<span class="sd">    where the kernel `\kernel{K}` may effectively be expanded by pre/post AOC layers to</span>
<span class="sd">    handle stride and channel changes. This approach is described in &quot;Improving</span>
<span class="sd">    SDP-based Lipschitz Layers&quot; of [1].</span>

<span class="sd">    **Args**:</span>
<span class="sd">      - `cin` (int): Number of input channels.</span>
<span class="sd">      - `inner_dim_factor` (float): Multiplier for the internal channel dimension.</span>
<span class="sd">      - `kernel_size` (int, optional): Base kernel size for the SLL portion. Default is 3.</span>
<span class="sd">      - `stride` (int, optional): Stride for the skip connection. Default is 2.</span>
<span class="sd">      - `groups` (int, optional): Number of groups for the convolution. Default is 1.</span>
<span class="sd">      - `**kwargs`: Additional options (unused).</span>



<span class="sd">    References:</span>
<span class="sd">        - Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).</span>
<span class="sd">        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.</span>
<span class="sd">        &lt;https://arxiv.org/abs/2501.07930&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">inner_kernel_size</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">-</span> <span class="p">(</span><span class="n">stride</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">skip_kernel_size</span> <span class="o">=</span> <span class="n">stride</span> <span class="o">+</span> <span class="p">(</span><span class="n">stride</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">inner_dim</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">cout</span> <span class="o">*</span> <span class="n">inner_dim_factor</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stride</span> <span class="o">=</span> <span class="n">stride</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">padding</span> <span class="o">=</span> <span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span>
            <span class="n">inner_dim</span><span class="p">,</span> <span class="n">cin</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="n">inner_kernel_size</span><span class="p">,</span> <span class="n">inner_kernel_size</span>
        <span class="p">)</span>
    <span class="p">)</span>
    <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="s2">&quot;kernel&quot;</span><span class="p">,</span>
        <span class="n">AOLReparametrizer</span><span class="p">(</span>
            <span class="n">inner_dim</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">inner_dim</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
    <span class="n">fan_in</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">_calculate_fan_in_and_fan_out</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">)</span>
    <span class="n">bound</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">fan_in</span><span class="p">)</span>
    <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">,</span> <span class="o">-</span><span class="n">bound</span><span class="p">,</span> <span class="n">bound</span><span class="p">)</span>  <span class="c1"># bias init</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pre_conv</span> <span class="o">=</span> <span class="n">AdaptiveOrthoConv2d</span><span class="p">(</span>
        <span class="n">cin</span><span class="p">,</span> <span class="n">cin</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_conv</span> <span class="o">=</span> <span class="n">AdaptiveOrthoConv2d</span><span class="p">(</span>
        <span class="n">cin</span><span class="p">,</span>
        <span class="n">cout</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="orthogonium.layers.conv.AOL.aol"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.AOL.aol.AOLConv2D" class="doc doc-heading">
            <code>AOLConv2D</code>


<a href="#orthogonium.layers.conv.AOL.aol.AOLConv2D" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Conv2d">Conv2d</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\AOL\aol.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AOLConv2D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">niter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce</span>
<span class="sd">        almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer</span>
<span class="sd">        is guaranteed to be less than 1.</span>

<span class="sd">        Args:</span>
<span class="sd">            in_channels (int): Number of input channels.</span>
<span class="sd">            out_channels (int): Number of output channels.</span>
<span class="sd">            kernel_size (int or tuple): Size of the convolution kernel.</span>
<span class="sd">            stride (int or tuple, optional): Stride of the convolution. Default is 1.</span>
<span class="sd">            padding (int or tuple, optional): Padding size. Default is 0.</span>
<span class="sd">            dilation (int or tuple, optional): Dilation rate. Default is 1.</span>
<span class="sd">            groups (int, optional): Number of groups. Default is 1.</span>
<span class="sd">            bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">            padding_mode (str, optional): Padding mode. Default is &quot;zeros&quot;.</span>
<span class="sd">            device (torch.device, optional): Device to store the layer parameters. Default is None.</span>
<span class="sd">            dtype (torch.dtype, optional): Data type to store the layer parameters. Default is None.</span>


<span class="sd">        References:</span>
<span class="sd">            `[1] Prach, B., &amp; Lampert, C. H. (2022).</span>
<span class="sd">                   &quot;Almost-orthogonal layers for efficient general-purpose lipschitz networks.&quot;</span>
<span class="sd">                   ECCV.`&lt;https://arxiv.org/abs/2208.03160&gt;`_</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">niter</span> <span class="o">=</span> <span class="n">niter</span>

        <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
            <span class="n">MultiStepAOLReparametrizer</span><span class="p">(</span>
                <span class="nb">min</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">),</span>
                <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
                <span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span>
            <span class="p">),</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets parameters of the module. This includes the weight and bias</span>
<span class="sd">        parameters, if they are used.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
        <span class="c1"># # Reset the parametrization</span>
        <span class="c1"># init kernel using the orthogonal kernel</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span>
            <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">conv_orthogonal_</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
                <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
                <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
            <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.AOL.aol.AOLConv2D.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.AOL.aol.AOLConv2D.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce
almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer
is guaranteed to be less than 1.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of input channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of output channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td>
                  <code>int or tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the convolution kernel.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td>
                  <code>int or tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stride of the convolution. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td>
                  <code>int or tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding size. Default is 0.</p>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dilation</code>
            </td>
            <td>
                  <code>int or tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dilation rate. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of groups. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include a learnable bias. Default is True.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mode</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode. Default is "zeros".</p>
              </div>
            </td>
            <td>
                  <code>&#39;zeros&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="torch.device">device</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Device to store the layer parameters. Default is None.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dtype</code>
            </td>
            <td>
                  <code><span title="torch.dtype">dtype</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Data type to store the layer parameters. Default is None.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<details class="references" open>
  <summary>References</summary>
  <p><code>[1] Prach, B., &amp; Lampert, C. H. (2022).
       "Almost-orthogonal layers for efficient general-purpose lipschitz networks."
       ECCV.</code><a href="https://arxiv.org/abs/2208.03160">https://arxiv.org/abs/2208.03160</a>`_</p>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOL\aol.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">niter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce</span>
<span class="sd">    almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer</span>
<span class="sd">    is guaranteed to be less than 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of input channels.</span>
<span class="sd">        out_channels (int): Number of output channels.</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolution kernel.</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default is 1.</span>
<span class="sd">        padding (int or tuple, optional): Padding size. Default is 0.</span>
<span class="sd">        dilation (int or tuple, optional): Dilation rate. Default is 1.</span>
<span class="sd">        groups (int, optional): Number of groups. Default is 1.</span>
<span class="sd">        bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">        padding_mode (str, optional): Padding mode. Default is &quot;zeros&quot;.</span>
<span class="sd">        device (torch.device, optional): Device to store the layer parameters. Default is None.</span>
<span class="sd">        dtype (torch.dtype, optional): Data type to store the layer parameters. Default is None.</span>


<span class="sd">    References:</span>
<span class="sd">        `[1] Prach, B., &amp; Lampert, C. H. (2022).</span>
<span class="sd">               &quot;Almost-orthogonal layers for efficient general-purpose lipschitz networks.&quot;</span>
<span class="sd">               ECCV.`&lt;https://arxiv.org/abs/2208.03160&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">niter</span> <span class="o">=</span> <span class="n">niter</span>

    <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
        <span class="n">MultiStepAOLReparametrizer</span><span class="p">(</span>
            <span class="nb">min</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">),</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">niter</span><span class="o">=</span><span class="n">niter</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.AOL.aol.AOLConv2D.reset_parameters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>

<a href="#orthogonium.layers.conv.AOL.aol.AOLConv2D.reset_parameters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Resets parameters of the module. This includes the weight and bias
parameters, if they are used.</p>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOL\aol.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Resets parameters of the module. This includes the weight and bias</span>
<span class="sd">    parameters, if they are used.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">reset_parameters</span><span class="p">()</span>
    <span class="c1"># # Reset the parametrization</span>
    <span class="c1"># init kernel using the orthogonal kernel</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">in_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_channels</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">conv_orthogonal_</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">weight</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D" class="doc doc-heading">
            <code>AOLConvTranspose2D</code>


<a href="#orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.ConvTranspose2d">ConvTranspose2d</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\AOL\aol.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">AOLConvTranspose2D</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">niter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce</span>
<span class="sd">        almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer</span>
<span class="sd">        is guaranteed to be less than 1.</span>

<span class="sd">        Args:</span>
<span class="sd">            in_channels (int): Number of input channels.</span>
<span class="sd">            out_channels (int): Number of output channels.</span>
<span class="sd">            kernel_size (int or tuple): Size of the convolution kernel.</span>
<span class="sd">            stride (int or tuple, optional): Stride of the convolution. Default is 1.</span>
<span class="sd">            padding (int or tuple, optional): Padding size. Default is 0.</span>
<span class="sd">            output_padding (int or tuple, optional): Additional size added to the output shape. Default is 0.</span>
<span class="sd">            groups (int, optional): Number of groups. Default is 1.</span>
<span class="sd">            bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">            dilation (int or tuple, optional): Dilation rate. Default is 1.</span>
<span class="sd">            padding_mode (str, optional): Padding mode. Default is &quot;zeros&quot;.</span>
<span class="sd">            device (torch.device, optional): Device to store the layer parameters. Default is None.</span>
<span class="sd">            dtype (torch.dtype, optional): Data type to store the layer parameters. Default is None.</span>


<span class="sd">        References:</span>
<span class="sd">            `[1] Prach, B., &amp; Lampert, C. H. (2022).</span>
<span class="sd">                   &quot;Almost-orthogonal layers for efficient general-purpose lipschitz networks.&quot;</span>
<span class="sd">                   ECCV.`&lt;https://arxiv.org/abs/2208.03160&gt;`_</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">output_padding</span><span class="o">=</span><span class="n">output_padding</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
            <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
            <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">niter</span> <span class="o">=</span> <span class="n">niter</span>

        <span class="c1"># Register the same AOLReparametrizer</span>
        <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
            <span class="n">MultiStepAOLReparametrizer</span><span class="p">(</span>
                <span class="nb">min</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="n">niter</span>
            <span class="p">),</span>
        <span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D.__init__" class="doc doc-heading">
            <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>

<a href="#orthogonium.layers.conv.AOL.aol.AOLConvTranspose2D.__init__" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce
almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer
is guaranteed to be less than 1.</p>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of input channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of output channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td>
                  <code>int or tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the convolution kernel.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td>
                  <code>int or tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stride of the convolution. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td>
                  <code>int or tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding size. Default is 0.</p>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>output_padding</code>
            </td>
            <td>
                  <code>int or tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Additional size added to the output shape. Default is 0.</p>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of groups. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include a learnable bias. Default is True.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dilation</code>
            </td>
            <td>
                  <code>int or tuple</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dilation rate. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mode</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode. Default is "zeros".</p>
              </div>
            </td>
            <td>
                  <code>&#39;zeros&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>device</code>
            </td>
            <td>
                  <code><span title="torch.device">device</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Device to store the layer parameters. Default is None.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dtype</code>
            </td>
            <td>
                  <code><span title="torch.dtype">dtype</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Data type to store the layer parameters. Default is None.</p>
              </div>
            </td>
            <td>
                  <code>None</code>
            </td>
          </tr>
      </tbody>
    </table>


<details class="references" open>
  <summary>References</summary>
  <p><code>[1] Prach, B., &amp; Lampert, C. H. (2022).
       "Almost-orthogonal layers for efficient general-purpose lipschitz networks."
       ECCV.</code><a href="https://arxiv.org/abs/2208.03160">https://arxiv.org/abs/2208.03160</a>`_</p>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOL\aol.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">in_channels</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">,</span>
    <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="o">=</span><span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">niter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Almost-Orthogonal Convolution layer. This layer implements the method proposed in [1] to enforce</span>
<span class="sd">    almost-orthogonality. While orthogonality is not enforced, the lipschitz constant of the layer</span>
<span class="sd">    is guaranteed to be less than 1.</span>

<span class="sd">    Args:</span>
<span class="sd">        in_channels (int): Number of input channels.</span>
<span class="sd">        out_channels (int): Number of output channels.</span>
<span class="sd">        kernel_size (int or tuple): Size of the convolution kernel.</span>
<span class="sd">        stride (int or tuple, optional): Stride of the convolution. Default is 1.</span>
<span class="sd">        padding (int or tuple, optional): Padding size. Default is 0.</span>
<span class="sd">        output_padding (int or tuple, optional): Additional size added to the output shape. Default is 0.</span>
<span class="sd">        groups (int, optional): Number of groups. Default is 1.</span>
<span class="sd">        bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">        dilation (int or tuple, optional): Dilation rate. Default is 1.</span>
<span class="sd">        padding_mode (str, optional): Padding mode. Default is &quot;zeros&quot;.</span>
<span class="sd">        device (torch.device, optional): Device to store the layer parameters. Default is None.</span>
<span class="sd">        dtype (torch.dtype, optional): Data type to store the layer parameters. Default is None.</span>


<span class="sd">    References:</span>
<span class="sd">        `[1] Prach, B., &amp; Lampert, C. H. (2022).</span>
<span class="sd">               &quot;Almost-orthogonal layers for efficient general-purpose lipschitz networks.&quot;</span>
<span class="sd">               ECCV.`&lt;https://arxiv.org/abs/2208.03160&gt;`_</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="o">=</span><span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="o">=</span><span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="o">=</span><span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="o">=</span><span class="n">output_padding</span><span class="p">,</span>
        <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="n">dilation</span><span class="o">=</span><span class="n">dilation</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="o">=</span><span class="n">padding_mode</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">niter</span> <span class="o">=</span> <span class="n">niter</span>

    <span class="c1"># Register the same AOLReparametrizer</span>
    <span class="n">parametrize</span><span class="o">.</span><span class="n">register_parametrization</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="s2">&quot;weight&quot;</span><span class="p">,</span>
        <span class="n">MultiStepAOLReparametrizer</span><span class="p">(</span>
            <span class="nb">min</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">),</span> <span class="n">groups</span><span class="o">=</span><span class="n">groups</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="n">niter</span>
        <span class="p">),</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>

<div class="doc doc-object doc-class">



<h2 id="orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer" class="doc doc-heading">
            <code>MultiStepAOLReparametrizer</code>


<a href="#orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">
            <p class="doc doc-class-bases">
              Bases: <code><span title="torch.nn.Module">Module</span></code></p>







              <details class="quote">
                <summary>Source code in <code>orthogonium\layers\conv\AOL\aol.py</code></summary>
                <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">MultiStepAOLReparametrizer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">nb_features</span><span class="p">,</span> <span class="n">groups</span><span class="p">,</span> <span class="n">niter</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiStepAOLReparametrizer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">nb_features</span> <span class="o">=</span> <span class="n">nb_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">niter</span> <span class="o">=</span> <span class="n">niter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">nb_features</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel</span><span class="p">):</span>
        <span class="n">co</span><span class="p">,</span> <span class="n">cig</span><span class="p">,</span> <span class="n">ks</span><span class="p">,</span> <span class="n">ks2</span> <span class="o">=</span> <span class="n">kernel</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="n">co</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">&gt;=</span> <span class="n">cig</span><span class="p">:</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="n">transpose_kernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="n">flip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">kkt</span> <span class="o">=</span> <span class="n">kernel</span>
        <span class="n">log_curr_norm</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">niter</span><span class="p">):</span>
            <span class="n">kkt_norm</span> <span class="o">=</span> <span class="n">kkt</span><span class="o">.</span><span class="n">norm</span><span class="p">()</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">kkt</span> <span class="o">=</span> <span class="n">kkt</span> <span class="o">/</span> <span class="n">kkt_norm</span>
            <span class="n">log_curr_norm</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">log_curr_norm</span> <span class="o">+</span> <span class="n">kkt_norm</span><span class="o">.</span><span class="n">log</span><span class="p">())</span>
            <span class="n">kkt</span> <span class="o">=</span> <span class="n">fast_matrix_conv</span><span class="p">(</span>
                <span class="n">transpose_kernel</span><span class="p">(</span><span class="n">kkt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="n">flip</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">kkt</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span>
            <span class="p">)</span>

        <span class="n">inverse_power</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">**</span> <span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">niter</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">kkt</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">q_inv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">q_inv</span> <span class="o">*</span> <span class="n">t</span> <span class="o">*</span> <span class="n">q</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">inverse_power</span><span class="p">)</span>
        <span class="n">norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">log_curr_norm</span> <span class="o">*</span> <span class="n">inverse_power</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span> <span class="o">*</span> <span class="n">norm</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span> <span class="o">/</span> <span class="n">t</span>
        <span class="k">if</span> <span class="n">co</span> <span class="o">//</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">&gt;=</span> <span class="n">cig</span><span class="p">:</span>
            <span class="n">kernel</span> <span class="o">=</span> <span class="n">transpose_kernel</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">,</span> <span class="n">flip</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">kernel</span>

    <span class="k">def</span> <span class="nf">right_inverse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">kernel</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">kernel</span>

    <span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Resets the parameters of the reparametrizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Reset the q parameter to its initial value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
              </details>



  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h3 id="orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer.reset_parameters" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">()</span></code>

<a href="#orthogonium.layers.conv.AOL.aol.MultiStepAOLReparametrizer.reset_parameters" class="headerlink" title="Permanent link">&para;</a></h3>


    <div class="doc doc-contents ">

        <p>Resets the parameters of the reparametrizer.</p>

            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\AOL\aol.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Resets the parameters of the reparametrizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Reset the q parameter to its initial value</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>




  </div>

    </div>

</div>

<div class="doc doc-object doc-module">



<a id="orthogonium.layers.conv.adaptiveSOC.ortho_conv"></a>
    <div class="doc doc-contents first">








  <div class="doc doc-children">









<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">AdaptiveSOCConv2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;circular&#39;</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel
size and stride. This is a modified implementation of the <code>Skew orthogonal convolution</code> [1], with significant
modification from the original paper:</p>
<ul>
<li>This implementation provide an explicit kernel (which is larger the original kernel size) so the forward is done
    in a single iteration. As described in [2].</li>
<li>This implementation avoid the use of channels padding to handle case where cin != cout. Similarly, stride is
    handled natively using the ad adaptive scheme.</li>
<li>the fantastic four method is replaced by AOL which allows to reduce the number of iterations required to
    converge.</li>
</ul>
<p>It aims to be more scalable to large networks and large image sizes, while enforcing orthogonality in the
convolutional layers. This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class
(e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward
operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</p>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>this implementation changes the size of the kernel, which also change the padding semantics. Please adjust
    the padding according to the kernel size and the number of iterations.</li>
<li>current unit testing use a tolerance of 8e-2 sor this layer can be expected to be 1.08 lipschitz continuous.
    Similarly, the stable rank is evaluated loosely (must be greater than 0.5).</li>
</ul>
</details>        <h4 id="orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--key-features">Key Features:<a class="headerlink" href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--key-features" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- Enforces orthogonality, preserving gradient norms.
- Supports native striding, dilation, grouped convolutions, and flexible padding.
</code></pre></div>
<h4 id="orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--behavior">Behavior:<a class="headerlink" href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConv2d--behavior" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- When kernel_size == stride, the layer is an `RKOConv2d`.
- When stride == 1, the layer is a `FastBlockConv2d`.
- Otherwise, the layer is a `BcopRkoConv2d`.
</code></pre></div>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of input channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of output channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the convolution kernel.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stride of the convolution. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td>
                  <code>str or <span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode or size. Default is "same".</p>
              </div>
            </td>
            <td>
                  <code>&#39;same&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dilation</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dilation rate. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of blocked connections from input to output channels. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include a learnable bias. Default is True.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mode</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode. Default is "circular".</p>
              </div>
            </td>
            <td>
                  <code>&#39;circular&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ortho_params</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p>
              </div>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a>()</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.Conv2d">Conv2d</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>`ValueError`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="references" open>
  <summary>References</summary>
  <ul>
<li>[1] Singla, S., &amp; Feizi, S. (2021, July). Skew orthogonal convolutions. In International Conference
on Machine Learning (pp. 9756-9766). PMLR.<a href="https://arxiv.org/abs/2105.11417">https://arxiv.org/abs/2105.11417</a></li>
<li>[2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).
An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.
<a href="https://arxiv.org/abs/2501.07930">https://arxiv.org/abs/2501.07930</a></li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\adaptiveSOC\ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">AdaptiveSOCConv2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">_size_2_t</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;same&quot;</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;circular&quot;</span><span class="p">,</span>
    <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Factory function to create an orthogonal convolutional layer, selecting the appropriate class based on kernel</span>
<span class="sd">    size and stride. This is a modified implementation of the `Skew orthogonal convolution` [1], with significant</span>
<span class="sd">    modification from the original paper:</span>


<span class="sd">    - This implementation provide an explicit kernel (which is larger the original kernel size) so the forward is done</span>
<span class="sd">        in a single iteration. As described in [2].</span>
<span class="sd">    - This implementation avoid the use of channels padding to handle case where cin != cout. Similarly, stride is</span>
<span class="sd">        handled natively using the ad adaptive scheme.</span>
<span class="sd">    - the fantastic four method is replaced by AOL which allows to reduce the number of iterations required to</span>
<span class="sd">        converge.</span>

<span class="sd">    It aims to be more scalable to large networks and large image sizes, while enforcing orthogonality in the</span>
<span class="sd">    convolutional layers. This layer also intend to be compatible with all the feature of the `nn.Conv2d` class</span>
<span class="sd">    (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward</span>
<span class="sd">    operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</span>

<span class="sd">    Note:</span>
<span class="sd">        - this implementation changes the size of the kernel, which also change the padding semantics. Please adjust</span>
<span class="sd">            the padding according to the kernel size and the number of iterations.</span>
<span class="sd">        - current unit testing use a tolerance of 8e-2 sor this layer can be expected to be 1.08 lipschitz continuous.</span>
<span class="sd">            Similarly, the stable rank is evaluated loosely (must be greater than 0.5).</span>

<span class="sd">    Key Features:</span>
<span class="sd">    -------------</span>
<span class="sd">        - Enforces orthogonality, preserving gradient norms.</span>
<span class="sd">        - Supports native striding, dilation, grouped convolutions, and flexible padding.</span>

<span class="sd">    Behavior:</span>
<span class="sd">    -------------</span>
<span class="sd">        - When kernel_size == stride, the layer is an `RKOConv2d`.</span>
<span class="sd">        - When stride == 1, the layer is a `FastBlockConv2d`.</span>
<span class="sd">        - Otherwise, the layer is a `BcopRkoConv2d`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        in_channels (int): Number of input channels.</span>
<span class="sd">        out_channels (int): Number of output channels.</span>
<span class="sd">        kernel_size (_size_2_t): Size of the convolution kernel.</span>
<span class="sd">        stride (_size_2_t, optional): Stride of the convolution. Default is 1.</span>
<span class="sd">        padding (str or _size_2_t, optional): Padding mode or size. Default is &quot;same&quot;.</span>
<span class="sd">        dilation (_size_2_t, optional): Dilation rate. Default is 1.</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.</span>
<span class="sd">        bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">        padding_mode (str, optional): Padding mode. Default is &quot;circular&quot;.</span>
<span class="sd">        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).</span>

<span class="sd">    Raises:</span>
<span class="sd">        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.</span>


<span class="sd">    References:</span>
<span class="sd">        - [1] Singla, S., &amp; Feizi, S. (2021, July). Skew orthogonal convolutions. In International Conference</span>
<span class="sd">        on Machine Learning (pp. 9756-9766). PMLR.&lt;https://arxiv.org/abs/2105.11417&gt;</span>
<span class="sd">        - [2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).</span>
<span class="sd">        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.</span>
<span class="sd">        &lt;https://arxiv.org/abs/2501.07930&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="n">stride</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">RKOConv2d</span>
    <span class="k">elif</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">FastSOC</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">SOCRkoConv2d</span>
    <span class="k">return</span> <span class="n">convclass</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">,</span>
        <span class="c1"># ortho_params=ortho_params,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>

<div class="doc doc-object doc-function">


<h2 id="orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d" class="doc doc-heading">
            <code class="highlight language-python"><span class="n">AdaptiveSOCConvTranspose2d</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">,</span> <span class="n">kernel_size</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">output_padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dilation</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding_mode</span><span class="o">=</span><span class="s1">&#39;zeros&#39;</span><span class="p">,</span> <span class="n">ortho_params</span><span class="o">=</span><span class="n">OrthoParams</span><span class="p">())</span></code>

<a href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d" class="headerlink" title="Permanent link">&para;</a></h2>


    <div class="doc doc-contents ">

        <p>Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on
kernel size and stride. This is a modified implementation of the <code>Skew orthogonal convolution</code> [1], with significant
modification from the original paper:</p>
<ul>
<li>This implementation provide an explicit kernel (which is larger the original kernel size) so the forward is done
    in a single iteration. As described in [2].</li>
<li>This implementation avoid the use of channels padding to handle case where cin != cout. Similarly, stride is
    handled natively using the ad adaptive scheme.</li>
<li>the fantastic four method is replaced by AOL which allows to reduce the number of iterations required to
    converge.</li>
</ul>
<p>It aims to be more scalable to large networks and large image sizes, while enforcing orthogonality in the
convolutional layers. This layer also intend to be compatible with all the feature of the <code>nn.Conv2d</code> class
(e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward
operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</p>


<details class="note" open>
  <summary>Note</summary>
  <ul>
<li>this implementation changes the size of the kernel, which also change the padding semantics. Please adjust
    the padding according to the kernel size and the number of iterations.</li>
<li>current unit testing use a tolerance of 8e-2 sor this layer can be expected to be 1.08 lipschitz continuous.
    Similarly, the stable rank is evaluated loosely (must be greater than 0.5).</li>
</ul>
</details>        <h4 id="orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--key-features">Key Features:<a class="headerlink" href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--key-features" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- Enforces orthogonality, preserving gradient norms.
- Supports native striding, dilation, grouped convolutions, and flexible padding.
</code></pre></div>
<h4 id="orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--behavior">Behavior:<a class="headerlink" href="#orthogonium.layers.conv.adaptiveSOC.ortho_conv.AdaptiveSOCConvTranspose2d--behavior" title="Permanent link">&para;</a></h4>
<div class="highlight"><pre><span></span><code>- When kernel_size == stride, the layer is an `RKOConv2d`.
- When stride == 1, the layer is a `FastBlockConv2d`.
- Otherwise, the layer is a `BcopRkoConv2d`.
</code></pre></div>


<p><span class="doc-section-title">Parameters:</span></p>
    <table>
      <thead>
        <tr>
          <th>Name</th>
          <th>Type</th>
          <th>Description</th>
          <th>Default</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                <code>in_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of input channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>out_channels</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of output channels.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>kernel_size</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Size of the convolution kernel.</p>
              </div>
            </td>
            <td>
                <em>required</em>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>stride</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Stride of the convolution. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding</code>
            </td>
            <td>
                  <code>str or <span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode or size. Default is "same".</p>
              </div>
            </td>
            <td>
                  <code>0</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>dilation</code>
            </td>
            <td>
                  <code><span title="torch.nn.common_types._size_2_t">_size_2_t</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Dilation rate. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>groups</code>
            </td>
            <td>
                  <code>int</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Number of blocked connections from input to output channels. Default is 1.</p>
              </div>
            </td>
            <td>
                  <code>1</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>bias</code>
            </td>
            <td>
                  <code>bool</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Whether to include a learnable bias. Default is True.</p>
              </div>
            </td>
            <td>
                  <code>True</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>padding_mode</code>
            </td>
            <td>
                  <code>str</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Padding mode. Default is "circular".</p>
              </div>
            </td>
            <td>
                  <code>&#39;zeros&#39;</code>
            </td>
          </tr>
          <tr class="doc-section-item">
            <td>
                <code>ortho_params</code>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>Parameters to control orthogonality. Default is <code>OrthoParams()</code>.</p>
              </div>
            </td>
            <td>
                  <code><a class="autorefs autorefs-internal" title="orthogonium.reparametrizers.OrthoParams" href="../reparametrizers/#orthogonium.reparametrizers.OrthoParams">OrthoParams</a>()</code>
            </td>
          </tr>
      </tbody>
    </table>


    <p><span class="doc-section-title">Returns:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code><span title="torch.nn.ConvTranspose2d">ConvTranspose2d</span></code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>A configured instance of <code>nn.Conv2d</code> (one of <code>RKOConv2d</code>, <code>FastBlockConv2d</code>, or <code>BcopRkoConv2d</code>).</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<p><span class="doc-section-title">Raises:</span></p>
    <table>
      <thead>
        <tr>
          <th>Type</th>
          <th>Description</th>
        </tr>
      </thead>
      <tbody>
          <tr class="doc-section-item">
            <td>
                  <code>`ValueError`</code>
            </td>
            <td>
              <div class="doc-md-description">
                <p>If kernel_size &lt; stride, as orthogonality cannot be enforced.</p>
              </div>
            </td>
          </tr>
      </tbody>
    </table>


<details class="references" open>
  <summary>References</summary>
  <ul>
<li>[1] Singla, S., &amp; Feizi, S. (2021, July). Skew orthogonal convolutions. In International Conference
on Machine Learning (pp. 9756-9766). PMLR.<a href="https://arxiv.org/abs/2105.11417">https://arxiv.org/abs/2105.11417</a></li>
<li>[2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).
An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.
<a href="https://arxiv.org/abs/2501.07930">https://arxiv.org/abs/2501.07930</a></li>
</ul>
</details>
            <details class="quote">
              <summary>Source code in <code>orthogonium\layers\conv\adaptiveSOC\ortho_conv.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">AdaptiveSOCConvTranspose2d</span><span class="p">(</span>
    <span class="n">in_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">out_channels</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">_size_2_t</span><span class="p">,</span>
    <span class="n">stride</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">output_padding</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">groups</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">dilation</span><span class="p">:</span> <span class="n">_size_2_t</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
    <span class="n">ortho_params</span><span class="p">:</span> <span class="n">OrthoParams</span> <span class="o">=</span> <span class="n">OrthoParams</span><span class="p">(),</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">ConvTranspose2d</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Factory function to create an orthogonal transposed convolutional layer, selecting the appropriate class based on</span>
<span class="sd">    kernel size and stride. This is a modified implementation of the `Skew orthogonal convolution` [1], with significant</span>
<span class="sd">    modification from the original paper:</span>

<span class="sd">    - This implementation provide an explicit kernel (which is larger the original kernel size) so the forward is done</span>
<span class="sd">        in a single iteration. As described in [2].</span>
<span class="sd">    - This implementation avoid the use of channels padding to handle case where cin != cout. Similarly, stride is</span>
<span class="sd">        handled natively using the ad adaptive scheme.</span>
<span class="sd">    - the fantastic four method is replaced by AOL which allows to reduce the number of iterations required to</span>
<span class="sd">        converge.</span>

<span class="sd">    It aims to be more scalable to large networks and large image sizes, while enforcing orthogonality in the</span>
<span class="sd">    convolutional layers. This layer also intend to be compatible with all the feature of the `nn.Conv2d` class</span>
<span class="sd">    (e.g., striding, dilation, grouping, etc.). This method has an explicit kernel, which means that the forward</span>
<span class="sd">    operation is equivalent to a standard convolutional layer, but the weight are constrained to be orthogonal.</span>

<span class="sd">    Note:</span>
<span class="sd">        - this implementation changes the size of the kernel, which also change the padding semantics. Please adjust</span>
<span class="sd">            the padding according to the kernel size and the number of iterations.</span>
<span class="sd">        - current unit testing use a tolerance of 8e-2 sor this layer can be expected to be 1.08 lipschitz continuous.</span>
<span class="sd">            Similarly, the stable rank is evaluated loosely (must be greater than 0.5).</span>

<span class="sd">    Key Features:</span>
<span class="sd">    -------------</span>
<span class="sd">        - Enforces orthogonality, preserving gradient norms.</span>
<span class="sd">        - Supports native striding, dilation, grouped convolutions, and flexible padding.</span>

<span class="sd">    Behavior:</span>
<span class="sd">    -------------</span>
<span class="sd">        - When kernel_size == stride, the layer is an `RKOConv2d`.</span>
<span class="sd">        - When stride == 1, the layer is a `FastBlockConv2d`.</span>
<span class="sd">        - Otherwise, the layer is a `BcopRkoConv2d`.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        in_channels (int): Number of input channels.</span>
<span class="sd">        out_channels (int): Number of output channels.</span>
<span class="sd">        kernel_size (_size_2_t): Size of the convolution kernel.</span>
<span class="sd">        stride (_size_2_t, optional): Stride of the convolution. Default is 1.</span>
<span class="sd">        padding (str or _size_2_t, optional): Padding mode or size. Default is &quot;same&quot;.</span>
<span class="sd">        dilation (_size_2_t, optional): Dilation rate. Default is 1.</span>
<span class="sd">        groups (int, optional): Number of blocked connections from input to output channels. Default is 1.</span>
<span class="sd">        bias (bool, optional): Whether to include a learnable bias. Default is True.</span>
<span class="sd">        padding_mode (str, optional): Padding mode. Default is &quot;circular&quot;.</span>
<span class="sd">        ortho_params (OrthoParams, optional): Parameters to control orthogonality. Default is `OrthoParams()`.</span>

<span class="sd">    Returns:</span>
<span class="sd">        A configured instance of `nn.Conv2d` (one of `RKOConv2d`, `FastBlockConv2d`, or `BcopRkoConv2d`).</span>

<span class="sd">    Raises:</span>
<span class="sd">        `ValueError`: If kernel_size &lt; stride, as orthogonality cannot be enforced.</span>


<span class="sd">    References:</span>
<span class="sd">        - [1] Singla, S., &amp; Feizi, S. (2021, July). Skew orthogonal convolutions. In International Conference</span>
<span class="sd">        on Machine Learning (pp. 9756-9766). PMLR.&lt;https://arxiv.org/abs/2105.11417&gt;</span>
<span class="sd">        - [2] Boissin, T., Mamalet, F., Fel, T., Picard, A. M., Massena, T., &amp; Serrurier, M. (2025).</span>
<span class="sd">        An Adaptive Orthogonal Convolution Scheme for Efficient and Flexible CNN Architectures.</span>
<span class="sd">        &lt;https://arxiv.org/abs/2501.07930&gt;</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">&lt;</span> <span class="n">stride</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;kernel size must be smaller than stride. The set of orthonal convolutions is empty in this setting.&quot;</span>
        <span class="p">)</span>
    <span class="k">if</span> <span class="n">kernel_size</span> <span class="o">==</span> <span class="n">stride</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">RkoConvTranspose2d</span>
    <span class="k">elif</span> <span class="n">stride</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">SOCTranspose</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">convclass</span> <span class="o">=</span> <span class="n">SOCRkoConvTranspose2d</span>
    <span class="k">return</span> <span class="n">convclass</span><span class="p">(</span>
        <span class="n">in_channels</span><span class="p">,</span>
        <span class="n">out_channels</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">stride</span><span class="p">,</span>
        <span class="n">padding</span><span class="p">,</span>
        <span class="n">output_padding</span><span class="p">,</span>
        <span class="n">groups</span><span class="p">,</span>
        <span class="n">bias</span><span class="p">,</span>
        <span class="n">dilation</span><span class="p">,</span>
        <span class="n">padding_mode</span><span class="p">,</span>
        <span class="c1"># ortho_params=ortho_params,</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
            </details>
    </div>

</div>



  </div>

    </div>

</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.f8cc74c7.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.f1b6f286.min.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
        <script src="../../js/custom.js"></script>
      
    
  </body>
</html>